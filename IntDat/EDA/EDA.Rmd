---
title: "Exploratory Data Analysis"
author: "Ignacio Vellido"
date: "11/6/2020"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
library(ggplot2)
library(tidyverse)
library(car)
library(ISLR)
library(dlookr)
library(corrplot)
library(HSAUR2)
library(vcdExtra)
library(MASS)
library(cepp)
library("dslabs")
library(reshape2)
library(moments)
```

## Ejemplo 1, hip dataset

__Descargate el  dataset hip con el siguiente commando__
```{r}
hip <- read.table("http://astrostatistics.psu.edu/datasets/HIP_star.dat", header=T,
                  fill=T)
```

__Una vez descargado comprueba la dimensión y los nombres de las columnas del dataset. ¿Qué dimensión tiene? ¿qué tipo de datos alberga? (e.g. 4 variables numéricas continuas)__
```{r}
cat("Dimensiones: ")
dim(hip)
cat("Columnas: ")
colnames(hip)
hip %>% apply(2, class)
```
2719 filas y 9 columnas todas numéricas


__Calcula las tendencias centrales de todos los datos del dataset (mean, media) Tip: puedes usar la función apply()__
```{r}
# También nos vale summary
cat("Medias: ")
hip %>% apply(2, mean, na.rm=T)
cat("Medianas: ")
hip %>% apply(2, median, na.rm=T)
```


__Haz lo mismo para las medidas de dispersión mínimo y máximo. ¿Seria posible hacerlo con un único comando?¿Que hace la función range()?__
```{r}
hip %>% apply(2, range, na.rm=T)
```
Nos muestra mínimos y máximos de cada variable.

__Sin embargo las medidas mas populares de dispersión son la varianza (var()), su desviación standard (sd()) y la desviación absoluta de la mediana mad(). Calcula estas medidas para los valores de la variable RA__
```{r}
cat("Varianza: ")
hip$RA %>% var(na.rm=T)
cat("Desviación standard: ")
hip$RA %>% sd(na.rm=T)
cat("Desviación absoluta de la mediana: ")
hip$RA %>% mad(na.rm=T)
```


__Imagina que quieres calcular dos de estos valores de una sola vez. ¿Te serviría este código?__
```{r}
f = function(x) c(median(x), mad(x))  
f(hip[,3])
```

Nos calcula la media y la desviación absoluta de la mediana.


__¿Cuál sería el resultado de aplicar apply(hip,2,f)?__
```{r}
apply(hip,2,f)
```

Aplica la función anterior a todas las columnas.


__Vamos a medir la dispersión de la muestra utilizando el concepto de cuartiles. El percentil 90 es aquel dato que excede en un 10% a todos los demás datos. El cuartil (quantile) es el mismo concento, solo que habla de proporciones en vez de porcentajes. De forma que el percentil 90 es lo mismo que el cuartil 0.90. La mediana “median” de un dataset es el valor más central, en otras palabras exactamente la mitad del dataset excede la media. Calcula el cuartil .10 y .50 para la columna RA del dataset hip. Sugerencia: quantile()__
```{r}
hip$RA %>% quantile(c(0.1,0.5), na.rm=T)
```


__Los cuantiles 0.25 y 0.75 se conocen como el  “first quartile” o Q1 y el “third quartile” o Q3, respectivamente. Calcula los cuatro cuartiles para RA con un único comando.__
```{r}
hip$RA %>% quantile(seq(0.25,0.75,0.25), na.rm=T)
```


__Otra medida de dispersion es la diferencia entre el primer y el tercer cuartil conocida como rango intercuartil (IQR) Inter Quantile Range. ¿Obtienes ese valor con la función summary()?__
```{r}
hip$RA %>% IQR()
hip$RA %>% summary()
```

El IQR no se obtiene en summary() pero se puede calcular con la diferencia del 1er y 3er cuartil


__Hasta ahora has ignorado la presencia de  valores perdidos NA. La función any() devuelve TRUE si se encuentra al menos un TRUE en el vector que damos como argumento. Su combinación con is.na es muy útil. ¿qué obtienes cuando ejecutas el siguiente comando? ¿Cómo lo interpretas?__
```{r}
hasNA = function(x) any(is.na(x)) 
apply(hip,2,hasNA)
```

La columna B.V es la única que tiene NA.


__Prueba a ejecutar el siguiente comando.__
```{r}
min(hip$B.V)
```

__hip1 = na.omit(hip) Como has observado  nos devuelve NA para toda la columna,  normalmente querríamos poder usar la función sobre el resto de datos que no son NA: Para ello podemos utilizar la función na.omit. ¿Que ocurre cuando lo hacemos?. Usando apply calcula la media para hip. Intenta calcular la media de forma que solo cambie la de B.V cuando ignores los valores NA.__
```{r}
hip1 <- na.omit(hip) 
mean(hip$B.V)
mean(hip1$B.V)
hip %>% apply(2,mean,na.rm=T)
```

__Obten una idea aproximada de tus datos mediante la creación de un boxplot del hip dataset__
```{r}
for (col in colnames(hip)) {
  ggplot(hip, aes_string(col)) +
    geom_boxplot() +
    theme_light() -> p
  print(p)
}

# ggplot(pivot_longer(hip, 1:9), aes(x=name, y=value, fill=name)) +
#   geom_boxplot() +
#   facet_wrap(~name) +
#   scale_y_continuous()
```

__Crea un scatterplot que te compare los valores de RA y DE. Representa los puntos con el símbolo ‘.’ Y que estos puntos sean de color rojo si DE excede de 0. Sugerencia: puedes usar dplyr/tidyverse o Rbase ifelse()__
```{r}
ggplot(hip, aes(x=RA, y=DE)) +
  geom_point(aes(color=ifelse(DE > 0, "red", "blue"))) +  # shape="." si quisieramos ese
  scale_color_identity() +  # Importante para que se interpreten bien los colores
  theme_light()
```

__En vez de crear los plots por separado para cada par de columnas, hazlos con un solo comando con el scatterplot matrix__
```{r}
scatterplotMatrix(hip, pch=20, col="deepskyblue")
```

__Para poder acceder a las variables por su nombre usa attach(hip).Vamos a seleccionar las estrellas Hyadas del dataset aplicando los siguientes filtros:__

- RA in the range (50,100) 
- DE in the range (0,25) 
- pmRA in the range (90,130) 
- pmDE in the range (-60,-10) 
- e_Plx <5 
- Vmag >4 OR B.V <0.2 (this eliminates 4 red giants) 
```{r}
attach(hip)
hip %>% filter(RA > 50 & RA < 100) %>% 
  filter(DE > 0 & DE < 25) %>% 
  filter(pmRA > 90 & pmRA < 130) %>% 
  filter(pmDE > -60 & pmDE < -10) %>% 
  filter(e_Plx < 5) %>% 
  filter(Vmag > 4 | B.V < 0.2)
```

__Crea un nuevo dataset con la aplicación de estos filtro. El Nuevo dataset se llama hyades. ¿Que dimensiones tiene? Grafica un scatterplot de Vmag vs B.V__
```{r}
hyades <- hip %>% 
  filter(RA > 50 & RA < 100) %>% 
  filter(DE > 0 & DE < 25) %>% 
  filter(pmRA > 90 & pmRA < 130) %>% 
  filter(pmDE > -60 & pmDE < -10) %>% 
  filter(e_Plx < 5) %>% 
  filter(Vmag > 4 | B.V < 0.2) %>% data.frame()

cat("Dimensiones: ")
dim(hyades)

ggplot(hyades, aes(x=Vmag, y=B.V)) +
  geom_point() + 
  geom_smooth(method=lm) +
  theme_light()
```


## Ejemplo 2, InsectSprays dataset

__El dataset “InsectSprays” está incluido en la libreria de R “datasets”. Contiene el conteo de insectos extraidos de diferentes zonas agrícolas tratadas de forma experimental con diferentes insecticidas. Haz un boxplot para determiner que insecticida parece ser el más efectivo.__
```{r}
insects <- InsectSprays
str(insects)
summary(insects)
head(insects)

ggplot(insects, aes(x=spray, y=count, fill=spray)) +
  geom_boxplot() +
  theme_light()
```

Para responder a la pregunta en este caso estamos interesados en los insecticidas con menos número de insectos, por tanto fácilmente podemos descartar los tipos A, B, y F por tener un gran número en comparación al resto.

De los tres restantes, C parece ser el más eficaz puesto más del 50% de las mediciones han dado un conteo inferior al resto.


## Ejemplo 3, Carseats

__Instala la library(ISLR), vamos a trabajar con el dataset Carseats. Si vas a usar dplyr puedes inspeccionar el paquete “dlookr”__
```{r}
carseat <- Carseats
```

__Encuentra que variables tienen skewness__
```{r}
colnames(carseat)[find_skewness(carseat)]
```

__Genera dos listas, una de variables con skewness a la derecha y otra con skewness a la izquierda__
```{r}
y <- sapply(carseat, class)
y <- y[y != "factor"] %>% names()
split(y, describe(carseat)$skewness > 0)
```

__Averigua que variables no están distribuidas de forma normal, crea gráficos que lo prueben__
```{r}
nonNormals <- normality(carseat) %>% filter(p_value < 0.05)
nonNormals$vars
```

```{r}
ggplot(carseat, aes(x=Income)) + 
  geom_histogram(aes(y=..density..), bins=30, color="blue", fill="white") +
  geom_density(alpha=.2, fill="black") +
  theme_light()
```

```{r}
ggplot(carseat, aes(x=Advertising)) + 
  geom_histogram(aes(y=..density..), bins=10, color="red", fill="white") +
  geom_density(alpha=.2, fill="black") +
  theme_light()
```
```{r}
ggplot(carseat, aes(x=Population)) + 
  geom_histogram(aes(y=..density..), bins=50, color="green3", fill="white") +
  geom_density(alpha=.2, fill="black") +
  theme_light()
```
```{r}
ggplot(carseat, aes(x=Age)) + 
  geom_histogram(aes(y=..density..), bins=10, color="orange", fill="white") +
  geom_density(alpha=.2, fill="black") +
  theme_light()
```

```{r}
ggplot(carseat, aes(x=Education)) + 
  geom_histogram(aes(y=..density..), bins=15, color="black", fill="white") +
  geom_density(alpha=.2, fill="black") +
  theme_light()
```

__Encuentra que variables están correlaccionadas positiva y cuales negativamente. Crea el gráfico apropiado.__
```{r}
correlate(carseat)

corrCarseat <- carseat[, sapply(carseat, is.numeric)]

corrplot(cor(corrCarseat), method="color")
```

## Ejemplo 4, iris dataset

__Vamos a utilizar el ejemplo del dataset iris que está incluido en la distribución de R. Este dataset fue creado por Douglas Fisher.  Consta de tres clases y tipos de 3 clases de tipos de flores:__

- setosa
- virginica
- versicolor
  
__Cada una de ellas con cuatro atributos:__

- sepal width
- sepal length
- petal width
- petal length

__Inspecciona las primeras filas del dataset y calcula el summary() del mismo con cada atributo del dataset__
```{r}
head(iris)
summary(iris)
```

__Crea un histograma de petal.width , teniendo en cuenta que el numero de bins es variable fija este a 9. Añádele color y nombres al eje x "Petal Width"y al gráfico dale el nombre de  "Histogram of Petal Width". __
```{r}
ggplot(iris, aes(x=Petal.Width)) + 
  geom_histogram(aes(y=..density..), bins=9, color="black", fill="grey") +
  geom_density(alpha=.2, fill="black") +
  labs(title="Histogram of Petal Width", x="Petal Width") +
  theme_light()
```
__Crea un histograma para cada variable__
```{r}
for (col in head(colnames(iris), -1)) {
  ggplot(iris, aes_string(x=col)) + 
    geom_histogram(aes(y=..density..), bins=9, color="black", fill="grey") +
    geom_density(alpha=.2, fill="black") +
    labs(title=sprintf("Histogram of %s",col), x="Petal Width") +
    theme_light() -> p
  
  print(p)
}
```

__Crea los cuartiles del dataset__
```{r}
iris %>% filter(Species=="setosa") %>% 
    select_if(is.numeric) %>% 
    lapply(quantile, c(0.25,0.5,0.75)) %>% 
    as.data.frame() -> 
  setosa

iris %>% filter(Species=="virginica") %>% 
    select_if(is.numeric) %>% 
    lapply(quantile, c(0.25,0.5,0.75)) %>% 
    as.data.frame() -> 
  virginica

iris %>% filter(Species=="versicolor") %>% 
    select_if(is.numeric) %>% 
    lapply(quantile, c(0.25,0.5,0.75)) %>% 
    as.data.frame() -> 
  versicolor

setosa
virginica
versicolor
```

__Representa en un boxplot la variable de ancho de hoja dependiendo del tipo de hoja que tengan__
```{r}
ggplot(iris, aes(y=Sepal.Width, fill=Species)) +
  geom_boxplot() +
  theme_light()
```

__Crea los cuartiles para cada tipo de iris y represéntalos en un plot como líneas cada una de un color__
```{r}
setosa <- setosa %>% mutate(prob = c(0.25,0.5,0.75), species="setosa")
virginica <- virginica %>% mutate(prob = c(0.25,0.5,0.75), species="virginica")
versicolor <- versicolor %>% mutate(prob = c(0.25,0.5,0.75), species="versicolor")

irisCuartiles <- bind_rows(setosa, virginica, versicolor)
irisCuartiles

ggplot(gather(irisCuartiles, key="key", value="value", 1:4), 
       aes(x=prob, y=value, color=species)) +
  geom_point() +
  geom_line() +
  scale_y_continuous() +
  facet_wrap(~key, scales="free_x", nrow = 1) +
  theme_light()
```

__Crea los boxplot de la longitud del pétalo en función de la especie de Iris.__
```{r}
ggplot(iris, aes(y=Petal.Length, fill=Species)) +
  geom_boxplot() +
  theme_light()
```

__Compara con scatter plots las variables entre sí.__
```{r}
scatterplotMatrix(select_if(iris, is.numeric), col="skyblue2", smooth=list(spread=FALSE), pch=20)
```

__Crea una nueva columna llamada proporción que es el ratio entre Sepal.Length y Sepal.Width. Podeis hacerlo en R base o usando el paquete dplyr.__
```{r}
iris2 <- iris
iris2 %>% mutate(Sepal.Ratio=Sepal.Length/Sepal.Width)
```



### Ejemplo 5, swiss dataset

__El conjunto de datos “swiss” contiene una medida estandarizada de fecundidad y varios indicadores socioeconómicos para cada una de las 47 provincias francófonas de Suiza.__
```{r}
sw <- swiss
```


__1. ¿Qué diagrama dibujaría para mostrar la distribución de todos los valores? ¿Qué conclusiones sacarías?__
```{r}
head(sw)
cat("Dimensiones: ")
dim(sw)
```
El dataset contiene 5 variables numéricas para cada provincia.
Podemos hacer un histograma por variable o un scatterplot para ver correlación entre ellas.


```{r}
ggplot(gather(sw), aes(value)) +
  geom_histogram(bins = 15, color="white") +
  facet_wrap(~key, scales = 'free_x') +
  theme_light()

scatterplotMatrix(sw, diagonal=list(method ="histogram", breaks="FD"),
                  col="skyblue2", regLine=F, smooth=list(spread=FALSE), pch=20)
```


__2. Dibuje gráficos para cada variable. ¿Qué puede concluir de las distribuciones con respecto a su forma y posibles valores atípicos?__
Salvo la varibale Catholics todas se pueden asociar a una normal.
A pesar de esto, la variable Education tiene outliers en los valores altos y Agriculture tiene una forma bastante achatada.

__3. Dibuja un diagrama de dispersión de Fertilidad frente a % Catholic. ¿Qué tipo de áreas tienen las tasas de fertilidad más bajas?__
```{r}
ggplot(sw, aes(x=Fertility, y=percent_rank(Catholic))) +
  geom_point(pch=15) +
  labs(y = "% Catholic") +
  theme_light()
```
Provincias con menor porcentaje de católicos tienden a tener menos tasa de fertilidad.

__4. ¿Qué tipo de relación existe entre las variables Educación y Agricultura?__
```{r}
ggplot(sw, aes(y=Education, x=Agriculture)) +
  geom_point() +
  geom_smooth(method=lm) +
  theme_light()
```

Cuanto mayor tasa de agricultura menor nivel de educación.

__El conjunto de datos de aceites de oliva es bien conocido y se puede encontrar en varios paquetes, por ejemplo, como aceitunas en extracat. La fuente original de los datos es el artículo [Forina et al., 1983].__
```{r}
data("olive")
head(olive)
```

__1. Dibuje un scatterplot  de las ocho variables continuas. ¿Cuáles de los ácidos grasos están fuertemente asociados positivamente y cuáles fuertemente asociados negativamente? __
```{r}
scatterplotMatrix(select_if(olive, is.numeric), pch = 20)

corrplot(cor(select_if(olive, is.numeric)), method="color")
```
Asociados positivamente: palmitic-palmitoleic, linoleic-palmitoleic ...
Asociados negativamente: palmitic-oleic, palmitoleic-oleic...

__2. ¿Hay valores atípicos u otras características que valga la pena mencionar?__
Muchas de las variables cuentan con un fuerte máximo local cerca del global, es posible que no sigan distribuciones normales. 
```{r}
shapiro.test(olive$eicosenoic)
shapiro.test(olive$palmitic)
```
Por ejemplo, para este par de variables el test de Shapiro nos dice que no se puede asumir la normalidad.

### Ejemplo 6, HSAUR2 dataset

__El conjunto de datos se llama Lanza del paquete HSAUR2.__
```{r}
lanza <- Lanza
head(lanza)
str(lanza)
```

__1. Se informan los datos de cuatro estudios. Dibuje un diagrama para mostrar si los cuatro estudios son igualmente grandes.__ 
```{r}
ggplot(lanza, aes(x=study, color=study)) +
  geom_bar(fill="snow2", size=2) +
  theme_light()
```

__2. El resultado se mide por la clasificación de la variable con puntuaciones de 1 (mejor) a 5 (peor). ¿Cómo describirías la distribución?__
```{r}
ggplot(lanza, aes(x=classification, fill=study)) +
  geom_bar(size=1) +
  facet_wrap(~study) +
  theme_light()

ggplot(lanza, aes(x=classification, fill=study)) +
  geom_bar(size=1) +
  theme_light()
```

Predominan mejores puntuaciones, aunque está fuertemente contrastada con notas bajas.
Las notas sobre el estudio IV son mayoritariamente negativas, y en el II positivas.
El estudio III esta mayormente igualado en número total de notas positivas/negativas.
En el I parece que se da lo mismo pues existe una acumulación de notas 4/5 que probablemente igualan a 1/2.

Una tabla de recuentos nos daría información más precisa.
```{r}
lanza %>% group_by(study) %>% count(classification) #%>% mutate(x = )
```



__El paquete vcdExtra incluye datos de un viejo estudio de cáncer de mama sobre la supervivencia o muerte de 474 pacientes.__
```{r}
vcd <- vcdExtra::Cancer %>% as.data.frame()
str(vcd)
vcd
```

__1. Convierta los datos en un data frame y dibuje gráficos para comparar las tasas de supervivencia, primero, por grado de malignidad y, en segundo lugar, por centro de diagnóstico. __
```{r}
ggplot(vcd, aes(x=Grade, y=Freq, fill=Survival)) +
  geom_bar(stat="identity") +
  facet_wrap(~Survival) +
  theme_light()

ggplot(vcd, aes(x=Center, y=Freq, fill=Survival)) +
  geom_bar(stat="identity") +
  facet_wrap(~Survival) +
  theme_light()
```

__2. ¿Qué diagrama dibujaría para comparar las tasas de supervivencia tanto por grado de malignidad como por centro de diagnóstico? ¿Importa el orden de las variables explicativas?__
```{r}
ggplot(vcd) +
  geom_bar(aes(x=Grade, y=Freq, fill=Survival), stat="identity") +
  geom_bar(aes(x=Center, y=Freq, fill=Survival), stat="identity") +
  facet_wrap(~Center + Grade) +
  labs(x = "Center and grade") +
  theme_light()
```


__Dataset crabs (del paquete MASS) [Venables y Ripley, 2002]. Los autores inicialmente se transforman a una escala logarítmica y luego escriben que:__

 “The data are very highly correlated and scatterplot matrices and brush plots [i.e. interactive graphics] are none too revealing.”. 

```{r}
data(crabs)
head(crabs)
str(crabs)
```


__Utilizando gráficos generales, comente si la transformación logaritmica fue una buena idea y si está de acuerdo con su afirmación sobre las correlaciones.__
```{r}
crabsSelected <- gather(select_if(crabs[-3], is.numeric))

ggplot(crabsSelected, aes(value)) +
  geom_histogram(aes(y=..density..), bins = 15, color="white") +
  geom_density(alpha=.2, fill="blue") +
  facet_wrap(~key, scales = "free_x") +
  theme_light()
```

Podemos ver que la transformación logarítmica ha resultado en una normalización de los datos numéricos (se ha eliminado la variable index por únicamente numerar las instancias).
Ya que la normalización es una condición necesaria para una gran parte de test estadísticos, podemos concluir que ha sido buena idea.

```{r}
scatterplotMatrix(select_if(crabs, is.numeric), pch=20)

corrplot(cor(select_if(crabs, is.numeric)), method="color")
```

Los gráficos muestran una alta correlación en las variables númericas.

## Extra 1.	Como crear subgrupos de datos en R

__Busca información sobre la function cut(). Para ilustrar su uso vamos a utilizar el dataset state.x77. Si no lo tienes instalado instala el paquete R-Datasets. Usa la función head() para ver como son tus datos.__
```{r}
states <- state.x77 %>% as.data.frame()
head(states)
```

__- Extrae la columna Frost y asigna el resultado a la variable frost__
```{r}
frost <- states$Frost
```

__- Tu Nuevo objeto es un vector numérico__
```{r}
frost
```

__- Ahora intenta agrupar los datos en frost en tres niveles. Para crear bins en tus datos puedes utilizar la función cut(). __
```{r}
frost %>% cut(3)
```

__- ¿Que obtienes como nombres de los niveles?__
Se obtienen los rangos de los datos.

__- En la realidad no existen estados que tengan frost en días negativos. Esto es porque R añade un poco de padding. Prueba a solucionar el problema utilizando el parámetro include.lowest=TRUE en cut()__
```{r}
frost %>% cut(b=3, include.lowest = TRUE)
# Sigue incluyendo el padding, solo que ahora el rango es cerrado
```

__- Los nombres de los niveles no son demasiado informativos, especifica nuevos nombres para los niveles__
```{r}
niveles <- frost %>% cut(b=3, include.lowest = TRUE, labels=c("Min", "Med", "Max"))
niveles
```

__- Después de este paso has creado un factor que clasifica los estados en bajo, medio y alto según el numero de heladas.__

__- Ahora cuenta el número de estados que  hay en cada uno de los niveles. PISTA: utiliza la función table()__
```{r}
table(niveles)
```


### Extra 2.	Como ordenar tablas (dplyr)

```{r}
data("murders")
head(murders)
```

Utiizando el paquete dplyr realiza el siguiente análisis sobre el dataset murders.

__- Averigua cual es el estado con la mayor población__
```{r}
murders %>% top_n(1, population)
```

__- Averigua cual es el estado con el mayor ratio de asesinatos__
```{r}
murders %>% top_n(1, total/population)
```

__- Averigua cual es el estado con el menor ratio de asesinatos__
```{r}
murders %>% top_n(1, desc(total/population))
```

__- Crea una tabla ordenada alfabeticamente por region y por el ratio de asesinatos__
```{r}
# En orden decreciente
murders %>% arrange(region, desc(total/population))
```

__- Crea una tabla con la media del ratio de asesinatos por region__
```{r}
murders %>% mutate(ratio = total/population) %>% group_by(region) %>% summarize(media = mean(ratio), .groups="keep")
```