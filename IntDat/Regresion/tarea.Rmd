---
title: "Tarea-California"
author: "Ignacio Vellido"
date: "11/11/2020"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
# Configuración RMarkdown
knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
library(tidyverse)
library(ggplot2)
library(ISLR)
library(kknn)
library(car)
library(reshape2)
library(corrplot)
```

## Descripción del dataset

This data set contains information about all the block groups in California from the 1990 Census. In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.

The task is to aproximate the median house value of each block from the values of the rest of the variables.


Contiene las siguientes variables:

- Longitude: real[-124.35,-114.31]
- Latitude: real[32.54,41.95]
- HousingMedianAge: real[1,52]
- TotalRooms: real[2,39320]
- TotalBedrooms: real[1,6445]
- Population: real[3,35682]
- Households: real[1,6082]
- MedianIncome: real[0.4999,15.0001]
- Output - __MedianHouseValue__: real[14999,500001] 

Cargamos los datos
```{r}
xtra <- read.csv("./data/california.dat", comment.char="@", header = FALSE)

# Asignamos nombres automáticamente para facilitar el acceso
n <- length(names(xtra)) - 1
names(xtra)[1:n] <- paste("X", 1:n, sep="")
names(xtra)[n+1] <- "Y"

attach(xtra)
head(xtra)
```


Previsualicación de las variables respecto a la salida
```{r}
# Usando ggplot
aux <- melt(xtra, "Y")

ggplot(aux, aes(x=value, y=Y, color=variable)) +
  geom_point(alpha=0.1, pch=20) +
  facet_wrap(.~variable, scale="free", ncol = 2) +
  theme_light()
```

No se ve linealidad aparente entre las variables y la salida.

X8 podría seguir una logarítmica.

Ampliamos el gráfico para ella:
```{r}
ggplot(xtra, aes(x=X8, y=Y)) +
  geom_point(alpha=0.1) +
  theme_light()
```

Podría haber sido lineal pero la nube de puntos de arriba no da buenas sensaciones.

## Modelos univariables

Obtenemos un modelo simple para cada variable
```{r}
fit1 <- lm(Y~X1, data=xtra)
fit2 <- lm(Y~X2, data=xtra)
fit3 <- lm(Y~X3, data=xtra)
fit4 <- lm(Y~X4, data=xtra)
fit5 <- lm(Y~X5, data=xtra)
fit6 <- lm(Y~X6, data=xtra)
fit7 <- lm(Y~X7, data=xtra)
fit8 <- lm(Y~X8, data=xtra)
```


Visualizamos cada fit
```{r}
summary(fit1)
```


```{r}
summary(fit2)
```


```{r}
summary(fit3)
```


```{r}
summary(fit4)
```


```{r}
summary(fit5)
```


```{r}
summary(fit6)
```


```{r}
summary(fit7)
```


```{r}
summary(fit8)
```

Todas las variables tienen un p-value bajo, pero a excepción de X8, ninguna tiene un R^2 alto.
Resumiento tendríamos:
X8 - 0.4734
X4 - 0.01795
X3 - 0.01111
. . .

Mostramos el ajuste para X8
```{r}
plot(Y~X8, xtra)
abline(fit8, col="red")
confint(fit8)
```

--------------------------------------------------------------------------------

## Modelos multivariables

Vamos a probar un modelo multivariable descendente.
Con todas:
```{r}
fit0 <- lm(Y~., data=xtra)
summary(fit0)
```

Todas nos sale con un p-value < 0.05, por lo que no nos indica que alguna sea irrelevante.
Hay que notar que el R2 es mayor que ajustando X8 únicamente. Puesto que sigue siendo lineal (no se ha añadido mucha complejidad), podemos aceptar este modelo como mejor.

Pese a ello, vamos a comparar los R2 con eliminando una variable (a excepción de X8, que sabemos que es importante).
(Todas tienen un p-value demasiado bajo como para dejarnos guiar por eso a la hora de eliminar, mejor probar con todas)
```{r}
fit1 <- lm(Y~.-X1, data=xtra)
fit2 <- lm(Y~.-X2, data=xtra)
fit3 <- lm(Y~.-X3, data=xtra)
fit4 <- lm(Y~.-X4, data=xtra)
fit5 <- lm(Y~.-X5, data=xtra)
fit6 <- lm(Y~.-X6, data=xtra)
fit7 <- lm(Y~.-X7, data=xtra)

summary(fit1)
summary(fit2)
summary(fit3)
summary(fit4)
summary(fit5)
summary(fit6)
summary(fit7)
```

Obtenemos R2s muy cercanos pero ligeramente inferiores, cosa que no pasaría quitando X8
```{r}
fit8 <- lm(Y~.-X8, data=xtra)
summary(fit8)
```

Por obtener mayor simplicidad en el modelo, podríamos quitar X7 sin afectar mucho el R2.
Podríamos seguir haciendo esto con X4 perdiendo solo un 0,2%

## Interacciones

Por la descripción de las variables X4 (Rooms) y X8 (Households) sería relevante para calcular la Y
```{r}
fit1 <- lm(Y~.-X7+X4*X8, data=xtra)
summary(fit1)
```

Y vemos que la interacción aporta significativamente, tiene un p-value bajo (tambien el estadístico F)

Variables con correlación
```{r}
corrplot.mixed(cor(xtra), tl.pos="lt", upper="color", title="Pearson")
```

Probamos a añadir X1 con X2
```{r}
fit1 <- lm(Y~.-X7+X4*X8+X1*X2, data=xtra)
summary(fit1)
```
Nos dice de quitar X2 pero al ser un término de jerarquía no podemos.
No aporta una gran mejora respecto al modelo aditivo previo.

```{r}
fit1 <- lm(Y~.+X4*X8+X5*X7, data=xtra)
summary(fit1)
```

Añadimos interacción X5-X7 y el modelo mejora, y no necesitamos quitar ninguna variable

## No linealidad

```{r}
fit1 <- lm(Y~.+X4*X8+X5*X7+I(X1^2), data=xtra)
summary(fit1)
```
El modelo es válido, pero la mejora es ínfima y añade peor interpretabilidad, no merece la pena.

```{r}
fit1 <- lm(Y~.+X4*X8+X5*X7+I(log(X8)), data=xtra)
summary(fit1)
```
Lo mismo que en el caso anterior

## KNN

```{r}
fitknn1 <- kknn(Y ~ ., xtra, xtra) # Por defecto k = 7, distance = 2, kernel = "optimal" y scale=TRUE
summary(fitknn1$fitted.values)
```

RMSE
```{r}
yprime <- fitknn1$fitted.values

sqrt(sum((xtra$Y-yprime)^2)/length(yprime)) #RMSE
```

Modelo aditivo anterior
```{r}
fitknn2 <- kknn(Y~.+X4*X8+X5*X7, xtra, xtra)

yprime <- fitknn2$fitted.values
sqrt(sum((xtra$Y-yprime)^2)/length(yprime)) #RMSE
```

Bajamos algo, pero poco.

## Cross-validation

```{r}
#------------- 5-fold cross-validation LM todas las variables
nombre <- "data/california"

run_lm_fold <- function(i, x, tt = "test") {
    file <- paste(x, "-5-", i, "tra.dat", sep="")
    x_tra <- read.csv(file, comment.char="@", header=FALSE)
    file <- paste(x, "-5-", i, "tst.dat", sep="")
    x_tst <- read.csv(file, comment.char="@", header=FALSE)
    In <- length(names(x_tra)) - 1
    
    names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tra)[In+1] <- "Y"
    names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tst)[In+1] <- "Y"
    
    if (tt == "train") {
        test <- x_tra
    }
    else {
        test <- x_tst
    }
    
    fitMulti=lm(Y~.,x_tra)
    yprime=predict(fitMulti,test)
    sum(abs(test$Y-yprime)^2)/length(yprime) ## RMSE
}

lmMSEtrain<-mean(sapply(1:5,run_lm_fold,nombre,"train"))
lmMSEtest<-mean(sapply(1:5,run_lm_fold,nombre,"test"))
```


```{r}
#------------- 5-fold cross-validation KNN todas las variables
nombre <- "data/california"

run_knn_fold <- function(i, x, tt = "test") {
    file <- paste(x, "-5-", i, "tra.dat", sep="")
    x_tra <- read.csv(file, comment.char="@", header=FALSE)
    file <- paste(x, "-5-", i, "tst.dat", sep="")
    x_tst <- read.csv(file, comment.char="@", header=FALSE)
    In <- length(names(x_tra)) - 1
    
    names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tra)[In+1] <- "Y"
    names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tst)[In+1] <- "Y"
    
    if (tt == "train") {
        test <- x_tra
    }
    else {
        test <- x_tst
    }
    
    fitMulti=kknn(Y~.,x_tra,test)
    yprime=fitMulti$fitted.values
    sum(abs(test$Y-yprime)^2)/length(yprime) ##RMSE
}
knnMSEtrain<-mean(sapply(1:5,run_knn_fold,nombre,"train"))
knnMSEtest<-mean(sapply(1:5,run_knn_fold,nombre,"test"))
```


Resultados
```{r}
lmMSEtest
knnMSEtest
```
KNN supera, y por mucho

En training
```{r}
lmMSEtrain
knnMSEtrain
```

## Comparativas entre algoritmos

```{r}
#------------------- COMPARATIVAS GENERALES ENTRE ALGORITMOS

# Leemos la tabla con los errores medios de test
resultados <- read.csv("data/regr_test_alumnos.csv")
tablatst <- cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatst) <- names(resultados)[2:dim(resultados)[2]]
rownames(tablatst) <- resultados[,1]

# Leemos la tabla con los errores medios de entrenamiento
resultados <- read.csv("data/regr_train_alumnos.csv")
tablatra <- cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatra) <- names(resultados)[2:dim(resultados)[2]]
rownames(tablatra) <- resultados[,1]

# TABLA NORMALIZADA - lm (other) vs knn (ref) para WILCOXON
# + 0.1 porque wilcox R falla para valores == 0 en la tabla

difs <- (tablatst[,1] - tablatst[,2]) / tablatst[,1]
wilc_1_2 <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(wilc_1_2) <- c(colnames(tablatst)[1], colnames(tablatst)[2])
head(wilc_1_2)
```


```{r}
# Aplicación del test de WILCOXON
LMvsKNNtst <- wilcox.test(wilc_1_2[,1], wilc_1_2[,2], alternative = "two.sided", paired=TRUE)
Rmas <- LMvsKNNtst$statistic

pvalue <- LMvsKNNtst$p.value

LMvsKNNtst <- wilcox.test(wilc_1_2[,2], wilc_1_2[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- LMvsKNNtst$statistic

Rmas
Rmenos
pvalue
```

Nos dice que gana KNN pero el p-value no es suficiente para tomar esta afirmación como cierta.


```{r}
# Aplicación del test de Friedman
test_friedman <- friedman.test(as.matrix(tablatst))
test_friedman
```
El p-value es <0.05 por lo que podemos concluir que al menos hay un par de algoritmos de calidad diferente.


```{r}
# Aplicación del test post-hoc de HOLM
tam <- dim(tablatst)
groups <- rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(as.matrix(tablatst), groups, p.adjust = "holm", paired = TRUE)
```

Con el test post-hoc de HOLM podemos asegurar que 3-1 son diferentes, pero no del resto no podemos afirmar nada