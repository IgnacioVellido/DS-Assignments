---
title: "Regresion"
author: "Ignacio Vellido"
date: "11/17/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
# Para PDF output
# pdf_document: 
#     keep_tex: yes
#     df_print: kable

knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
library(tidyverse)
library(ggplot2)
library(cowplot)  # plot_grid
library(corrplot) # corr y corrplot
library(reshape2) # melt
library(dlookr) # normality
library(caret)  # preprocess
library(moments)  # skewness
library(car)  # scatterplotMatrix
library(MASS) # fit
library(kknn) # kknn
# library(ISLR)
# library(HSAUR2)
# library(vcdExtra)
# library(cepp)
# library("dslabs")
```


Cargamos los datos:
```{r}
names <- c("Displacement", "Horse_power", "Weight", "Acceleration", "Model_year", "Mpg")

auto <- read_csv("Data/autoMPG6/autoMPG6.dat", comment = "@", col_names = names)
```

--------------------------------------------------------------------------------

Recordamos que la descripción de los datos se encuentra en el apartado [??]

Como se comentó en el apartado de EDA:

"Se nos pide elegir 5 regresores para la regresión y contamos exactamente con ese número, por lo que no podemos descartar ninguna variable.
Aún así, hemos visto que tenemos algunas variables más interesentas que otras.
Varibles correladas con la salida nos aumentan las posibilidades de obtener un buen regresor, pero debemos evitar usar variables correladas entre sí para evitar la multicolinealidad. Sería conveniente evitarla para aumentar la interpretabilidad del modelo, pero la potencia en sí de este no cambia."

Graficamos la relación de cada variable respecto a la salida
```{r}
ggplot(melt(auto, "Mpg"), aes(x=value, y=Mpg, color=variable)) +
  geom_point(alpha=0.3) +
  facet_wrap(.~variable, scale="free") +
  theme_light()
```

Como dijimos, se aprecia alta correlación entre Displacement, Horse_power, Weight respecto de la salida, probablemente de forma logarítmica. 

Las matrices nos correlación nos confirman esta idea (con coeficientes de Pearson y Kendall)
```{r}
corrplot.mixed(cor(auto), tl.pos="lt", upper="color", title="Pearson")
corrplot.mixed(cor(auto, method="kendall"), tl.pos="lt", upper="color", title="Kendall")
```

Por tando, si las ordenáramos por cuáles parecen ser más prometedoras, tendríamos:
Weight > Displacement > Horse_power > Model_year > Acceleration

También tenemos que tener en cuenta que las tres primeras variables están correladas entre sí.

--------------------------------------------------------------------------------

## Ajustes de regresión lineal univariables

Vamos a analizar un ajuste con cada una de las características:
```{r}
lm(Mpg ~ Weight, data=auto) %>% summary()
print("-------------------------------------------")
lm(Mpg ~ Displacement, data=auto) %>% summary()
print("-------------------------------------------")
lm(Mpg ~ Horse_power, data=auto) %>% summary()
print("-------------------------------------------")
lm(Mpg ~ Model_year, data=auto) %>% summary()
print("-------------------------------------------")
lm(Mpg ~ Acceleration, data=auto) %>% summary()
```

Al ser univariable, no es necesario fijarse en el estadístico F por ahora.
Para ver el potencial de la variable, debemos darle importancia al p-valor (comprobar de que sea lo suficientemente bajo), y posteriormente ver el R<sup>2</sup> para everiguar el porcentaje de la salida explicada.

En base a los resultados vemos que el test de correlación nos había ayudado correctamente: de forma individual todas las variables tienen dependencia lineal, y el orden de calidad coincide con el orden de fuerza en las correlaciones.

Guardamos el modelo aditivo hasta ahora
```{r}
fit <- lm(Mpg ~ Weight, data=auto)
```


Ya con el uso de la variable Weight vemos que podemos explicar un ~69% de la salida, un buen valor de partida.
Graficamos el ajuste:
```{r}
ggplot(auto, aes(x=Weight, y=Mpg)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method=lm, col="red") +
  theme_light()
```

Y vemos sus coeficientes:
```{r}
confint(fit)
```
Aunque los valores del intervalo del coeficiente de Weight sea bajo, vemos que no incluye el cero (y con el p-valor obtenido anteriormente, lo podemos asegurar con bastante certeza).
Probablemente la razón de estos coeficientes tan pequeños es que los datos no están estandarizados (se podría hacer perfectamente, se han dejado con sus rangos normales para interpretarlos mejor) y los valores de las unidades de medida son bastante diferentes (hablamos de rangos de [9.0,46.6] en Mpg frente a [1613,5140] en Weight)

Ya con esto podemos intentan interpretar un poco los datos, tendríamos por ahora la fórmula de regresión lineal:
(poner fórmula)

(INTERPRETAR ?)
La fórmula nos indica que por cada unidad de peso el Mpg decrementa ?

--------------------------------------------------------------------------------

## Ajustes de regresión lineal multivariable

Aplicamos un método descendente

```{r}
lm(Mpg ~ ., data=auto) %>% summary()
```
El p-valor del F estadístico nos dice que al menos hay una variable (realmente ya lo sabíamos de los ajustes univariables) con dependencia linea.

Vemos que hay 3 variables con mal p-valor, empezamos quitando la que lo tiene más alto, Horse_power
```{r}
lm(Mpg ~ . - Horse_power, data=auto) %>% summary()
```
El F estadístico está correcto, y seguimos teniendo variables con p-valor grande, quitamos Displacement
```{r}
lm(Mpg ~ . - Horse_power - Displacement, data=auto) %>% summary()
```

idem. a lo anterior, quitamos Acceleration.
```{r}
lm(Mpg ~ . - Horse_power - Displacement - Acceleration, data=auto) %>% summary()
```

El estadístico F sigue bien, y los p-valores de las variables son extremadamente bajos.
Nos fijamos en el R<sup>2</sup> y vemos que ha subido considerablemente (un 10%) respecto al univariable, por lo que este sería nuestro modelo aditivo por ahora.

<!-- (Poner fórmula) -->

A partir de ahora hay que tener cuidado si el R<sup>2</sup> sigue aumentando, hay que evitar el overfitting en el modelo.

--------------------------------------------------------------------------------

## Inserción de interacciones

Del modelo aditivo solo nos han quedado dos regresores, así que probamos a incluirlos como interacción.
```{r}
lm(Mpg ~ + Weight * Model_year, data=auto) %>% summary()
```
El F estadístico sigue bien y los p-valores son bajos, el nuevo R<sup>2</sup> ha mejorado un 3%, así que no es demasiado para considerar un overfitting. Probablemente más de un 90% sería preocupante, pero también tenemos que tener en cuenta que las variables están fuertemente correladas con la salida.

Podríamos probar a añadir alguna interacción más con alguna variable que no hubiera entrado en el modelo aditivo, pero no se espera que mejore:
```{r}
lm(Mpg ~ + Weight * Model_year + Acceleration * Displacement, data=auto) %>% summary()
```
A pesar de nuestra suposición los p-valores son válidos y el R<sup>2</sup> aumenta un 1%.
Es cuestionable si el aumento de la complejidad del modelo merece con este incremento de R<sup>2</sup>.
Por simplificar vamos a quedarnos con el modelo aditivo anterior y probar con otra interacción.

Podemos probar combinando la variable Acceleration con una de que teníamos (Weight y Model_year)
```{r}
lm(Mpg ~ + Weight * Model_year + Acceleration * Weight, data=auto) %>% summary()
```

```{r}
lm(Mpg ~ + Weight * Model_year + Acceleration * Model_year, data=auto) %>% summary()
```

Y entre los dos nos podríamos quedar con el primero por tener mejores p-valores y un mejor R<sup>2</sup>.
Aun así, el incremento es pequeño respecto a nuestro modelo aditivo.

La fórmula del modelo aditivo que llevamos por ahora es:
<!-- formula con coeficientes -->

Y la graficamos:
```{r}
fit <- lm(Mpg ~ Weight * Model_year + Acceleration * Weight, data=auto)

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Weight)

# this is the predicted line of multiple linear regression
ggplot(data = auto, aes(y = Mpg, x = Weight)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()
```

Se aprecia posible overfitting en el modelo, vamos a dejarlo por ahora e intentar solucionarlo con el modelo no lineal.

--------------------------------------------------------------------------------

## Ajustes de regresión no lineal

Habíamos dicho que las gráficas nos mostraban una tendencia logarítmica, vamos a incluír la de Weight en nuestro modelo aditivo
```{r}
lm(Mpg ~ + Weight * Model_year + Acceleration * Weight + I(log(Weight)), data=auto) %>% 
  summary()
```
El estadístico F está bien y los p-valores también, aunque el de la interacción Weight-Acceleration es alto comparado con el resto (aún así sigue siendo aceptable).

Como el R<sup>2</sup> ha subido, por ver si mejora, vamos a quitar esta interacción.
```{r}
lm(Mpg ~ + Weight * Model_year + I(log(Weight)), data=auto) %>% 
  summary()
```
Hemos empeorado un 0.5%, bastante poco, y el modelo es más simple. La dejamos quitada.

Podemos mostrarlo en un gráfico:
```{r}
fit <- lm(Mpg ~ + Weight * Model_year + I(log(Weight)), data=auto)

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Weight)

# this is the predicted line of multiple linear regression
ggplot(data = auto, aes(y = Mpg, x = Weight)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()
```
Esta gráfica nos indica que con casi toda probabilidad se está generando sobreajuste, se ve necesario simplificar el modelo.

Si quitamos la otra interacción:
```{r}
lm(Mpg ~ Weight + Model_year + I(log(Weight)), data=auto) %>% summary()
```

No hemos perdido apenas R<sup>2</sup>, mostramos la gráfica:
```{r}
fit <- lm(Mpg ~ Weight + Model_year+ I(log(Weight)), data=auto)

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Weight)

# this is the predicted line of multiple linear regression
ggplot(data = auto, aes(y = Mpg, x = Weight)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()
```

Seguimos con el mismo problema, probablemente se deba a una de las variables.
Quitamos Model_year por tener poca correlación con la variable de salida:
```{r}
lm(Mpg ~ Weight + I(log(Weight)), data=auto) %>% summary()
```

El p-valor de Weight nos indica que hay que quitarla, y al no estar incluída ninguna interacción, no es un término de jerarquía, por lo que podemos hacerlo.
Se puede porque la variable sigue siendo independiente, solamente no está modelada de forma lineal, sino logarítmicamente.
```{r}
lm(Mpg ~ I(log(Weight)), data=auto) %>% summary()
```

```{r}
fit <- lm(Mpg ~ I(log(Weight)), data=auto)

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Weight)

# this is the predicted line of multiple linear regression
ggplot(data = auto, aes(y = Mpg, x = Weight)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Acceleration)

ggplot(data = auto, aes(y = Mpg, x = Acceleration)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Displacement)

ggplot(data = auto, aes(y = Mpg, x = Displacement)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()
```

Vemos un empeoramiento significativo en la calidad de R<sup>2</sup> respecto al modelo multivariable, pero la forma del modelo no está tan ajustada a los datos y parece sensato mantenerlo así.

Aún así, no resulta lógico intentar predecir el Mpg de un coche únicamente en base al peso, alguna de las otras variables deberían ayudarnos en la predección. Por ejemplo, alguna característica del motor, como la cilindrada o los caballos de vapor.

Para resumir, mostramos el modelo con mejor R<sup>2</sup> tras hacer múltiples pruebas, e intentando evitar un overfitting:
```{r}
lm(Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement)), data=auto) %>% 
  summary()
```

Los p-valores no son muy fuertes, pero siguen siendo aceptables, y gráficamente el modelo se ve un poco mejor:
```{r}
fit <- lm(Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement)), data=auto)

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Weight)

# this is the predicted line of multiple linear regression
ggplot(data = auto, aes(y = Mpg, x = Weight)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Acceleration)

ggplot(data = auto, aes(y = Mpg, x = Acceleration)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()

predicted_df <- data.frame(mpg_pred = predict(fit, auto), hp=auto$Displacement)

ggplot(data = auto, aes(y = Mpg, x = Displacement)) + 
  geom_point() +
  geom_line(color='red', data = predicted_df, aes(y=mpg_pred, x=hp)) +
  theme_light()
```

Pensando en el problema, y tras el análisis hecho en el apartado de EDA, creemos que usar Model_year para predecir Mpg no parece buena idea. La gráfica de la variable nos muestran mucha dispersión en los datos y, aunque sí se ve un cierta tendencia lineal, no parece suficiente para usarla.
Claramente nos ajusta mejor los datos pero parece que nos estamos pegando a ellos.

De cara a comprobar este razonamiento en el cross-validation, vamos a guardar dos modelos:
- Modelo con mejor R<sup>2</sup>
Mpg ~ Weight + Model_year + I(log(Weight)

- Modelo intentando evitar el overfitting
Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement))

--------------------------------------------------------------------------------

## Ajustes con KNN

Sabemos que la función por defecto usa la distancia de Minkowski y escala los datos a igual rango.
También usa un k de 7, y sería recomendable probar con varios.


Vamos a probar con diferentes modelos, primero el multivariable con todas
```{r}
fitknn <- kknn(Mpg~., auto, auto)

yprime <- fitknn$fitted.values
sqrt(sum((auto$Mpg - yprime)^2)/length(yprime)) #RMSE
```

Y probando con varios obtenemos el menor error con este
```{r}
fitknn <- kknn(Mpg~.-Acceleration, auto, auto)

yprime <- fitknn$fitted.values
sqrt(sum((auto$Mpg - yprime)^2)/length(yprime)) #RMSE
```
Que visualmente nos quedaría
```{r}
plot(auto$Mpg~auto$Weight)
points(auto$Weight,fitknn$fitted.values,col="blue",pch=20)

plot(auto$Mpg~auto$Acceleration)
points(auto$Acceleration,fitknn$fitted.values,col="blue",pch=20)
```

Si probamos el modelo no lineal obtenido en los pasos anteriores (con mejor R<sup>2</sup>)
```{r}
fitknn <- kknn(Mpg ~ Weight + Model_year + I(log(Weight)), auto, auto)

yprime <- fitknn$fitted.values
sqrt(sum((auto$Mpg - yprime)^2)/length(yprime)) #RMSE
```

Nos da peor error.

Y si probamos el modelo no lineal en el que intemos resolver el overfitting
```{r}
fitknn2 <- kknn(Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement)), auto, auto)

yprime <- fitknn2$fitted.values
sqrt(sum((auto$Mpg - yprime)^2)/length(yprime)) #RMSE
```

Obtenemos aún mayor empeoramiento.
Gráficamente:
```{r}
plot(auto$Mpg~auto$Weight)
points(auto$Weight,fitknn$fitted.values,col="red",pch=20)

plot(auto$Mpg~auto$Acceleration)
points(auto$Acceleration,fitknn2$fitted.values,col="blue",pch=20)
```

El RMSE (Root Mean Square Error) o raíz del error cuadrático medio nos permite calcular el error en nuestro conjunto de test o training producido en las predicciones, definido por la fórmula:
<!-- formula -->

El método para evitar el overfitting que usamos en el apartado anterior probablemente no funcione con KNN por seguir una metodología totalmente diferente. El ajuste de KNN para regresión no tiene nada que ver con los modelos LM. 
Podemos aún así guardarlo para comprobarlo.

Tendríamos por tanto los siguientes modelos para KNN:
Mpg ~ . - Acceleration
Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement))

Comparandolos gráficamente, vemos que son similares
```{r}
fitknn <- kknn(Mpg ~ . - Acceleration, auto, auto)
fitknn2 <- kknn(Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement)), auto, auto)

plot(auto$Mpg~auto$Weight)
points(auto$Weight,fitknn$fitted.values,col="red",pch=20)
points(auto$Weight,fitknn2$fitted.values,col="blue",pch=20)

plot(auto$Mpg~auto$Acceleration)
points(auto$Acceleration,fitknn$fitted.values,col="red",pch=20)
points(auto$Acceleration,fitknn2$fitted.values,col="blue",pch=20)
```

Aunque el que intenta evitar el overfitting (en color azul en la gráfica), tiene menor dispersión.

--------------------------------------------------------------------------------

## Comparativa de los ajustes anteriores con cross-validation

Recordamos los modelos obtenidos:
LM:
Mpg ~ Weight + Model_year + I(log(Weight)
Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement))

KNN:
Mpg ~ . - Acceleration
Mpg ~ Acceleration + I(log(Weight)) + I(log(Displacement))

c("Displacement", "Horse_power", "Weight", "Acceleration", "Model_year", "Mpg")
      X1                X2          X3            X4            X5         Y    
      
LM:
Y ~ X3 + X5 + I(log(X3))
Y ~ X4 + I(log(X3)) + I(log(X1))

KNN:
Y ~ . - X4
Y ~ X4 + I(log(X3)) + I(log(X1))

```{r}
nombre <- "Data/autoMPG6/autoMPG6"
```

Modelo de regresión con mejor R<sup>2</sup>
```{r}
#------------- 5-fold cross-validation LM todas las variables
run_lm_fold <- function(i, x, tt = "test") {
    file <- paste(x, "-5-", i, "tra.dat", sep="")
    x_tra <- read.csv(file, comment.char="@", header=FALSE)
    file <- paste(x, "-5-", i, "tst.dat", sep="")
    x_tst <- read.csv(file, comment.char="@", header=FALSE)
    In <- length(names(x_tra)) - 1
    
    names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tra)[In+1] <- "Y"
    names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tst)[In+1] <- "Y"
    
    if (tt == "train") {
        test <- x_tra
    }
    else {
        test <- x_tst
    }
    
    fitMulti=lm(Y ~ X3 + X5 + I(log(X3)),x_tra)  # Poner la fórmula aquí
    
    yprime=predict(fitMulti,test)
    sum(abs(test$Y-yprime)^2)/length(yprime) ## RMSE
}

lmMSEtrain1 <- mean(sapply(1:5,run_lm_fold,nombre,"train"))
lmMSEtest1 <- mean(sapply(1:5,run_lm_fold,nombre,"test"))
```

Modelo de regresión evitando overfitting
```{r}
#------------- 5-fold cross-validation LM todas las variables
run_lm_fold <- function(i, x, tt = "test") {
    file <- paste(x, "-5-", i, "tra.dat", sep="")
    x_tra <- read.csv(file, comment.char="@", header=FALSE)
    file <- paste(x, "-5-", i, "tst.dat", sep="")
    x_tst <- read.csv(file, comment.char="@", header=FALSE)
    In <- length(names(x_tra)) - 1
    
    names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tra)[In+1] <- "Y"
    names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tst)[In+1] <- "Y"
    
    if (tt == "train") {
        test <- x_tra
    }
    else {
        test <- x_tst
    }
    
    fitMulti=lm(Y ~ X4 + I(log(X3)) + I(log(X1)),x_tra)  # Poner la fórmula aquí
    
    yprime=predict(fitMulti,test)
    sum(abs(test$Y-yprime)^2)/length(yprime) ## RMSE
}

lmMSEtrain2 <- mean(sapply(1:5,run_lm_fold,nombre,"train"))
lmMSEtest2 <- mean(sapply(1:5,run_lm_fold,nombre,"test"))
```


Modelo KNN con menor RSME
```{r}
#------------- 5-fold cross-validation KNN todas las variables
run_knn_fold <- function(i, x, tt = "test") {
    file <- paste(x, "-5-", i, "tra.dat", sep="")
    x_tra <- read.csv(file, comment.char="@", header=FALSE)
    file <- paste(x, "-5-", i, "tst.dat", sep="")
    x_tst <- read.csv(file, comment.char="@", header=FALSE)
    In <- length(names(x_tra)) - 1
    
    names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tra)[In+1] <- "Y"
    names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tst)[In+1] <- "Y"
    
    if (tt == "train") {
        test <- x_tra
    }
    else {
        test <- x_tst
    }
    
    fitMulti=kknn(Y ~ . - X4,x_tra,test)  # Poner la fórmula aquí
    
    yprime=fitMulti$fitted.values
    sum(abs(test$Y-yprime)^2)/length(yprime) ##RMSE
}

knnMSEtrain1 <- mean(sapply(1:5,run_knn_fold,nombre,"train"))
knnMSEtest1 <- mean(sapply(1:5,run_knn_fold,nombre,"test"))
```


Modelo KNN evitando overfitting
```{r}
#------------- 5-fold cross-validation KNN todas las variables
run_knn_fold <- function(i, x, tt = "test") {
    file <- paste(x, "-5-", i, "tra.dat", sep="")
    x_tra <- read.csv(file, comment.char="@", header=FALSE)
    file <- paste(x, "-5-", i, "tst.dat", sep="")
    x_tst <- read.csv(file, comment.char="@", header=FALSE)
    In <- length(names(x_tra)) - 1
    
    names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tra)[In+1] <- "Y"
    names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
    names(x_tst)[In+1] <- "Y"
    
    if (tt == "train") {
        test <- x_tra
    }
    else {
        test <- x_tst
    }
    
    fitMulti=kknn(Y ~ X4 + I(log(X3)) + I(log(X1)),x_tra,test)  # Poner la fórmula aquí
    
    yprime=fitMulti$fitted.values
    sum(abs(test$Y-yprime)^2)/length(yprime) ##RMSE
}

knnMSEtrain2 <- mean(sapply(1:5,run_knn_fold,nombre,"train"))
knnMSEtest2 <- mean(sapply(1:5,run_knn_fold,nombre,"test"))
```


Resultados obtenidos
```{r}
cat("Regresión 1: ")
lmMSEtest1
cat("Regresión 2: ")
lmMSEtest2
cat("KNN 1: ")
knnMSEtest1
cat("KNN 2: ")
knnMSEtest2
```

Con el proceso de cross-validation dividimos el dataset en N subconjuntos (folds) y repetimos el entrenamiento N veces.
Cada entrenamiento se aplica reservando uno de los subconjuntos como test y entrenando con el resto.
Al final, el error obtenido para el modelo es la media de los errores en cada fold.
La elección del número de folds es importante y si el problema lo permite (en términos de gasto computacional), se debería probar con varios. En este caso hemos utilizado 5 folds.

Con esto conseguimos no desperdiciar el conocimiento del conjunto de test y no guiarnos por una única evaluación del modelo.


Como resultados, nos muestra que los modelos con los que obtuvimos mejores resultados de R2 y RSME en sus apartados han acabado con mejor RSME tras el cross-validation.
También apreciamos que con KNN conseguimos ligeramente mejores resultados.

Por completitud, mostramos también los resultados en training
```{r}
cat("Regresión 1: ")
lmMSEtrain1
cat("Regresión 2: ")
lmMSEtrain2
cat("KNN 1: ")
knnMSEtrain1
cat("KNN 2: ")
knnMSEtrain2
```

Que nos muestran que ninguno de los modelos LM estaban haciendo overfitting, pero en cambio en KNN si existe una diferencia significativa entre training y test.

--------------------------------------------------------------------------------

## Comparativa de tests

Para comparar los algoritmos vamos a aplicar test estadísticos en base a los resultados obtenidos en múltiples datasets.
Para asegurar la igualdad de condiciones los algoritmos hacen uso de parámetros genéricos y utilizan las mismas particiones de cross-validation.

Estas son las tablas de resultados que tenemos:
```{r}
# Leemos la tabla con los errores medios de test
resultados <- read.csv("Data/regr_test_alumnos.csv")
tablatst <- cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatst) <- names(resultados)[2:dim(resultados)[2]]
rownames(tablatst) <- resultados[,1]

# Leemos la tabla con los errores medios de entrenamiento
resultados <- read.csv("Data/regr_train_alumnos.csv")
tablatra <- cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatra) <- names(resultados)[2:dim(resultados)[2]]
rownames(tablatra) <- resultados[,1]

# TABLA NORMALIZADA - lm (other) vs knn (ref) para WILCOXON
# + 0.1 porque wilcox R falla para valores == 0 en la tabla

difs <- (tablatst[,1] - tablatst[,2]) / tablatst[,1]
wilc_1_2 <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(wilc_1_2) <- c(colnames(tablatst)[1], colnames(tablatst)[2])
head(wilc_1_2 %>% as.data.frame())
```

Aplicamos el test de Wilconxon a LM y KNN
```{r}
# Aplicación del test de WILCOXON
LMvsKNNtst <- wilcox.test(wilc_1_2[,1], wilc_1_2[,2], alternative = "two.sided", paired=TRUE)
Rmas <- LMvsKNNtst$statistic

pvalue <- LMvsKNNtst$p.value

LMvsKNNtst <- wilcox.test(wilc_1_2[,2], wilc_1_2[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- LMvsKNNtst$statistic

Rmas
Rmenos
cat("p-value: ")
pvalue
```

Obtenemos un ranking de 78 para LM y 93 para KNN, con un p-valor de 0.77 (o nivel de confianza del 33%).

Esto nos dice que gana KNN pero puesto que el p-value no es lo suficientemente grande no podemos afirmar con un nivel alto de significación que las diferencias entre los tests sean notorias.


Ahora aplicamos en test de Friedman a los dos algoritmos anterios junto al algoritmo M5:
```{r}
# Aplicación del test de Friedman
test_friedman <- friedman.test(as.matrix(tablatst))
test_friedman
```
El p-value es <0.05 por lo que podemos concluir que al menos hay un par de algoritmos de calidad diferente.

Vemos cuáles de ellos lo son haciendo el test post-hoc de HOLM
```{r}
# Aplicación del test post-hoc de HOLM
tam <- dim(tablatst)
groups <- rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(as.matrix(tablatst), groups, p.adjust = "holm", paired = TRUE)
```

Con el test post-hoc de HOLM podemos asegurar que 3-1 (M5 vs LM) son diferentes. También podemos afirmar M5 respecto de KNN pero con un nivel de confianza menor.

De KNN y LM no podemos afirmar nada puesto que el p-valor es extremadamente grande.