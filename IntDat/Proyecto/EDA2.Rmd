---
title: "EDA"
author: "Ignacio Vellido"
date: "11/13/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
# Para PDF output
# pdf_document: 
#     keep_tex: yes
#     df_print: kable

knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
library(tidyverse)
library(ggplot2)
library(cowplot)  # plot_grid
library(corrplot) # corr y corrplot
library(reshape2) # melt
library(dlookr) # normality
library(caret)  # preprocess
library(moments)  # skewness
library(car)  # scatterplotMatrix
library(MASS) # fit
# require(vcd)
# library(ISLR)
# library(HSAUR2)
# library(vcdExtra)
# library(cepp)
# library("dslabs")
```

# Intro
Para este trabajo contamos con dos datasets distintos: __habermanMPG6__ para aplicar Regresión y __haberman__ para aplicar Clasificación.

## Descripciones de los problemas

### haberman

http://archive.ics.uci.edu/ml/datasets/Haberman%27s+Survival
https://sci2s.ugr.es/keel/dataset.php?cod=62

Este dataset codifica el ratio de supervivencia de pacientes operados de cáncer de pecho en el Hospital Universitario de Chicago, en base a las siguientes características:

1. Age: Indica la edad del paciente en el momento de la operación.
2. Year: Los dos últimas cifras del año en el que se operó el paciente.
3. Positive: Número de nodos auxiliares positivos detectados. Esta variable hace referencia a los ganglios linfáticos que dan positivos como presentes de cáncer. A mayor número de nodos detectados, mayor es la gravedad del cáncer.
Aunque normalmente la primera zona de propagación del cáncer son estos nodos, no es la única medida de la seriedad, pues este puede propagarse a otras zonas del cuerpo.
En principio deberíamos suponer la posibilidad de que puede haber cosas de no supervivencia con bajo número de positivos, pero la bibliografía nos asegura que la probabilidad es baja.

Viendo que solo tenemos esta medida del cáncer en el dataset es posible que la operación que recivieron los pacientes sea algún tipo de cirugía de ganglios linfáticos, donde el cirujano intenta extraer los nodos afectados por el tumor.
Por consiguiente, cuanto mayor es la cantidad de nodos detectados, más complicaciones pueden acarrear de la operación.
(poner referencias)

https://www.cancer.org/cancer/breast-cancer/treatment/surgery-for-breast-cancer/lymph-node-surgery-for-breast-cancer.html
https://en.wikipedia.org/wiki/Lymph_node#:~:text=A%20lymph%20node%2C%20or%20lymph,include%20B%20and%20T%20cells.

El objetivo es poder clasificar, en base a los tres atributos, si los pacientes pueden sobrevir 5 años o más:

4. Survival: Sí/No indicando la supervivencia del paciente tras 5 años.

BUSCAR POSIBLES COMPLICACIONES

Contamos por tanto con un problema de clasificación binario en base a tres características, y con un número total de 306 instancias.

---------------------------------------------------------------------------------------------------------------------

# Análisis Estadístico de Datos

### haberman

La descripción del problema nos da alguna información adicional sobre las variables:

1. Age: Variable numérica discreta, contamos con valores enteros en el rango [30,83].
2. Year: Variable numérica discreta, contamos con valores enteros en el rango [58,69].
3. Positive: Variable numérica discreta, contamos con valores enteros en el rango [0,52].
4. Survival: Variable binaria

#### Hipótesis de partida

- H.1: Habrá menor ratio de supervivencia cuanto mayor sea el número de nodos positivos encontrados: Por los razonamientos explicados en la introducción del problema.
- H.2: Habrá mayor ratio de supervivencia cuanto más joven sea el paciente.
- H.3: El rango de Year es pequeño. La influencia de esta variable creemos que podría darse solo si durante ese período se hubieran descubierto técnicas mejores de cirugía. Este razonamiento va orientado de cara a la población y no a la muestra.
Puesto que contamos con datos de un solo hospital durante pocos años, es posible que el equipo de cirugía hubiera sido el mismo para la mayoría de pacientes.
- H.4: Podría haber relación entre la edad y el número de positivos, posiblemente indicando lo tardío que se descubre el cáncer.
- H.5: La bibliografía nos dice que el cáncer puede aparecer a diferentes edades con diferentes factores de riesgo (alcoholismo, herencia genética...). Podría ser que el número de variables con las que contamos sea insuficiente para la clasificación.

---------------------------------------------------------------------------------------------------------------------

Cargamos los datos:
```{r}
names <- c("Age", "Year", "Positive", "Survival")

haberman <- read_csv("Data/haberman/haberman.dat", comment = "@", col_names = names)
```

R por defecto nos carga las variables Age, Year y Positive como numéricas y Survival como carácter.

Vamos a transformar Survival a Factor
```{r}
haberman$Survival <- haberman$Survival %>% factor(levels = c("negative", "positive"), labels = c("No", "Yes"))
```


El resto de variables las mantenemos como numéricas


#### Análisis univariable

Los datos nos quedan por tanto de la siguiente manera:
```{r}
head(haberman)
```

Hacemos summary para sacar datos de relevancia
```{r}
summary(haberman)
```

En las distribuciones de los clasificadores nos fijaremos más adelante. Aquí hacemos notar que los valores de salida en nuestros datos están bastante desbalanceados, solo un 26.5% de los paciente sobrevivieron a los 5 años.


El dataset cuenta con valores repetidos
```{r}
sum(duplicated(haberman))
```

Mostramos estas ocurrencias:
```{r}
ind <- duplicated(haberman) | duplicated(haberman, fromLast = TRUE)
haberman[ind,] %>% arrange(Age)
```

Existen dos posibilidades para el origen de estos datos:

1. Errores en la introducción de los datos, entradas repetidas por error.
2. Sean entradas de pacientes distintos casualmente con las mismas características.

Como en este caso tenemos muy pocas variables (y un número moderado de entradas, 306), es probable que los pacientes coincidan en las características.
Además, podemos ver que las entradas en la mayoría de los casos las variables solo están duplicadas (solo hay una entrada triplicada).

Por tanto proseguimos sin eliminar estas instancias duplicadas.

---

No contamos con missing values
```{r}
sum(is.na(haberman))
```


Separamos los datos de las etiquetas
```{r}
labels <- haberman[4]
haberman <- haberman[-4]
names <- colnames(haberman)
```


Vamos a sacar plots de cada variable para verlo mejor
```{r}
ggplot(gather(haberman), aes(value)) +
  geom_histogram(bins = 15, color="white") +
  facet_wrap(~key, scales = 'free_x') +
  theme_light() +
  theme(strip.background = element_rect(fill="grey", size=2))+
  theme(strip.text = element_text(colour = 'black')) +
  labs(title="Histogramas de cada variable", x = "")
```
Una a una
<!-- MODIFICAR LOS BINS -->
```{r}
colors <- c("chocolate", "deepskyblue1", "plum1", "hotpink4", "orange", "springgreen4")
bins <- c(10,10,15,15,14,18)
plt <- list(length = length(names))

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(x=names[i])) + 
    geom_histogram(aes(y=..density..), bins=bins[i], color="white", fill=colors[i]) +
    geom_density(alpha=.3, fill="black", size=1) +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("Histograma %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

```{r}
colors <- c("chocolate", "deepskyblue1", "plum1", "hotpink4", "orange", "springgreen4")
plt <- list(length = length(names))

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(x=names[i])) + 
    geom_boxplot(fill = colors[i]) +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("Boxplot %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

```{r}
ggplot(melt(haberman), aes(x=variable, y=value)) + 
  geom_boxplot() +
  labs(title="Boxplot con mismo rango") +
  theme(axis.text.x = element_text(angle = 90))
```

Ya la descripción del problema nos lo decía, los rangos en los que se distribuyen los datos son muy diferentes dependiendo de la variable. Es necesario aplicar un proceso de estandarización para clasificación.

Podemos comparar los rangos intercuartiles si estandarizamos antes el dataset
```{r}
scale(haberman) %>% apply(2, IQR)
```

También podemos ver la distancia entre mínimos y máximos
```{r}
scale(haberman) %>% apply(2, range) %>% apply(2, dist)
```


<!-- library(vcd) -->
<!-- mosaic(Titanic -->
<!-- ,shade=TRUE) -->

#### Age

Vemos que no contamos con valores de todos los años:
```{r}
table(haberman$Age)

(83-30+1) == table(haberman$Age) %>% length
```

#### Year

Aunque no se vea bien en las gráficas, contamos con valores de todos los años, con mayor cantidad en los iniciales:
```{r}
table(haberman$Year)
```

##### Positive

La variable Positive parece llevar una distribución exponencial, y problablemente por ello aparezcan tantos posibles outliers.

--------------------------------------------------------------------------------

### Análisis sobre las distribuciones

GGPAIRS
TEST ANOVA EN REGRESIÓN
CORRELACIÓN DE 3 VARIABLES EN REGRESIÓN

Hemos comentado antes que no apreciamos semejanzas con una distribución normal en algunas de las variables, lo comprobamos con un test estadístico (Shapiro-Wilk test):
```{r}
normality(haberman) %>% filter(p_value < 0.05)
```
El test de Shapiro nos dice que ninguna variable sigue una distribución normal, con bastante certeza excepto en Acceleration.

Se muestra aquí como no hay que dejarse engañar por los gráficos, puesto que Acceleration parecía seguirla.
El p-value de Acceleration está muy cerca del umbral (0.03 vs 0.05). Es bastante probable de que la parte central derecha de la distribución sea la causante de no asegurar la normalidad.

Vamos a mostrarlo con gráficos Q-Q para verlo mejor:
```{r}
plt <- list(length = length(names))

x<-rnorm(100, mean=0, sd=1)

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(sample=names[i])) + 
    stat_qq(alpha=.3, fill=colors[i], size=1) +
    stat_qq_line() +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("QQ-plot %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

Estos gráficos Q-Q nos muestran más claramente que las variables no siguen distribuciones normales.
La distribución de Acceleration es la que más se asemeja y eso lo vemos en el estadístico de Shapiro, pero en la cola superior existe una diferencia significativa que hace que el test rechace.

Antes habíamos mencionado que la variable Positive parece seguir una distribución exponencial, hacemos un test de Kolmogorov-Smirnov para corroborarlo:
<!-- HABRÍA QUE DEJARLO UNIQUE - PROBABLEMENTE NO SE PUEDA -->
```{r}
# data generation
ex <- rexp(10000, rate = 1.85) # generate some exponential distribution
control <- abs(rnorm(10000)) # generate some other distribution

# estimate the parameters
fit1 <- fitdistr(ex, "exponential") 
fit2 <- fitdistr(haberman$Positive %>% unique(), "exponential")

# goodness of fit test
ks.test(ex, "pexp", fit1$estimate) # p-value > 0.05 -> distribution not refused
ks.test(haberman$Positive, "pexp", fit2$estimate) #  significant p-value -> distribution refused

# plot a graph
hist(haberman$Positive, freq = FALSE, breaks = 100, xlim = c(0, quantile(haberman$Positive, 0.99)))
curve(dexp(x, rate = fit1$estimate), from = 0, col = "red", add = TRUE)

# control
```

```{r}
# From https://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution/76998
qqexp <-  function(y, line=FALSE, ...) { 
    y <- y[!is.na(y)]
    n <- length(y)
    x <- qexp(c(1:n)/(n+1))
    m <- mean(y)
    if (any(range(y)<0)) stop("Data contains negative values")
    ylim <- c(0,max(y))
    qqplot(x, y, xlab="Exponential plotting position",ylim=ylim,ylab="Ordered sample", ...)
    if (line) abline(0,m,lty=2)
    invisible()
}

qqexp(haberman$Positive, line=TRUE)
```


Al ser el p-valor <0.05 el test nos lo rechaza.
El plot también nos lo muestra más claramente, la forma de la cola de la distribución probablemente sea la causante de que no siga ese tipo de distribución

Skewness:
```{r}
skewCols <- find_skewness(haberman)
colnames(haberman)[skewCols]
```
Era de esperar que la variable Positive nos saliera con skewness

```{r}
cat("Positive: ")
skewness(haberman$Positive)
cat("Year: ")
skewness(haberman$Year)
cat("Age: ")
skewness(haberman$Age)
```
Sobre la skewness, tal y como se había visto en las gráficas, algunas de las variables la tienen, en los 3 casos positivas (hacia la izquierda).

Los plots nos han dado idea de que Mpg tiene cierta skewness, pero cae por debajo del umbral de 0.5.

-------------------------------------------------------------

### Transformaciones

Las transformaciones necesarias para normalizar una distribución dependen de la variable en cuestion.
Primero debemos averiguar que tipo de distribución siguen.

Algunas parecen tener una distribución exponencial

<!-- Para normalizarlas podemos usar el paqueta carret, de forma que nos ajuste las variables a una distribución normal de media cero y desviación típica 1 -->

<!-- (Scale + center es estandarizar) -->
```{r}
# haberman_transform <- preProcess(haberman[,skewCols], method=c("YeoJohnson"))
# haberman_norm <- predict(haberman_transform, haberman[,skewCols])

# haberman_transform <- preProcess(haberman[,1:6], method=c("scale", "center"))
haberman_transform <- preProcess(haberman[,1:6], method=c("YeoJohnson","scale", "center"))
# transform the dataset using the parameters
haberman_norm <- predict(haberman_transform, haberman[,1:6])

summary(haberman_norm)
```

Para la variable Acceleration aplicando una transformación de YeoJohnson es suficiente.

Aunque para regresión no es absolutamente necesario, podemos estandarizar los datos a media 0 y dev 1, facilitando un poco los cálculos. La inferencia estadística de la regresión no va a variar, por lo que es conveniente hacerlo.
Haciendo esto debemos tener cuidado a la hora de interpretar los resultados de la regresión para no confundirnos.

------------------------------

<!-- Algunas variables parecen seguir una distribución exponencial, lo comprobamos: -->
```{r}
# haberman %>% apply(2, function(c) {
#   fit <- fitdistr(c, "exponential") 
# 
#   # goodness of fit test
#   ks.test(c, "pexp", fit$estimate) # p-value > 0.05 -> distribution not refused
# })
# 
# haberman %>% apply(2, function(c) {
#   fit <- fitdistr(c, "chi-squared", start=list(df=3),method="Brent",lower=0.1,upper=100) 
# 
#   # goodness of fit test
#   # ks.test(c, "pexp", fit$estimate) # p-value > 0.05 -> distribution not refused
#   ks.test(c,pchisq,df=10)
# })


# hist(haberman$Displacement, freq = FALSE, breaks = 100, xlim = c(0, quantile(haberman$Displacement, 0.99)))
# curve(dexp(x, rate = fit1$estimate), from = 0, col = "red", add = TRUE)
```



<!-- Vamos a demostrar que las variables están normalizadas: -->
```{r}
# normality(haberman_norm) %>% filter(p_value < 0.05)
```


```{r}
# colors <- c("chocolate", "deepskyblue1", "plum1", "hotpink4", "orange", "springgreen4")
# bins <- c(10,10,15,15,14,18)
# plt <- list(length = length(names))
# 
# x<-rnorm(100, mean=0, sd=1)
# 
# for (i in 1:length(names)) {
#   ggplot(haberman_norm, aes_string(sample=names[i])) + 
#     stat_qq(alpha=.3, fill=colors[i], size=1) +
#     stat_qq_line() +
#     labs(title="", x="", y="") +
#     theme_light() -> plt[[i]]
#   
#   print(plt[[i]] + labs(title=sprintf("%s", names[i]), x=""))
# }
# 
# plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```


```{r}
# shapiro.test(haberman_norm$Acceleration)
# 
# ggplot(haberman_norm, aes(sample=Acceleration)) +
#     stat_qq(alpha=.3, size=1) +
#     stat_qq_line() +
#     labs(title="", x="", y="") +
#     theme_light()

# colors <- c("chocolate", "deepskyblue1", "plum1", "hotpink4", "orange", "springgreen4")
# bins <- c(10,10,15,15,14,18)
# plt <- list(length = length(names))
# 
# for (i in 1:length(names)) {
#   ggplot(haberman, aes_string(x=names[i])) + 
#     geom_histogram(aes(y=..density..), bins=bins[i], color="white", fill=colors[i]) +
#     geom_density(alpha=.3, fill="black", size=1) +
#     # labs(title=sprintf("%s", names[i]), x="") +
#     labs(title="", x="", y="") +
#     theme_light() -> plt[[i]]
#   
#   print(plt[i])
# }
# 
# plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
# 
# 
# # QQ-plots
# 
# x<-rnorm(100, mean=0, sd=1)
# 
# for (i in 1:length(names)) {
#   ggplot(haberman_norm, aes_string(sample=names[i])) + 
#     stat_qq(alpha=.3, fill=colors[i], size=1) +
#     stat_qq_line() +
#     labs(title="", x="", y="") +
#     theme_light() -> plt[[i]]
#   
#   print(plt[i])
# }
# 
# plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

-----------------------------------------------------------

### Outliers

Como hemos visto anteriormente en los boxplots, las únicas variables con valores muy alejados del centro de la distribución son Acceleration y Horse_power.

Por el significado del problema, probablemente estos posibles outliers correspondan a coches de alta gama o potentes en la época. Esto tampoco lo podemos asegurar puesto que contamos con pocas características, pero se considera un razonamiento coherente. Además, puesto que los valores caen dentro de los rangos posibles para coches de la época, podemos descartar que sean errores de medida.

Deberíamos decidir si mantener o no estas instancias. Como en nuestro caso se nos ha pedido predecir el consumo Mpg, sin darnos consideraciones sobre los tipos/gamas de coches a los que se enfoca, proseguimos dejándo estas filas.

-----------------------------------------------------------

### Análisis de correlación

Como este es un problema de clasificación, necesitamos aquellas variables correladas para que la información se aporte de manera equitativa.

Tenemos que tener en cuenta que las variables no siguen distribuciones normales.
Aunque el coeficiente de Pearson no asume normalidad (si asume varianza y covarianza finitas), podemos usar el coeficiente de Kendall para los cálculos.
Independientemente del método usado vamos a obtener las mismas correlaciones en este dataset, solo varía la fuerza con la que se dan.
<!-- Las variables no siguen distribución normal, pero no pasa nada, Pearson no da problemas -->

Corrplot
```{r}
corrplot.mixed(cor(haberman), tl.pos="lt", upper="color", title="Pearson")
corrplot.mixed(cor(haberman, method="kendall"), tl.pos="lt", upper="color", title="Kendall")
```

Estas gráficas nos dicen que existe una alta correlación en el dataset, generalmente entre todas las variables (a excepción de Model_year), pero extremadamente fuerte en las parejas:

1. Horse_power & Displacement
2. Weight & Displacement
3. Weight & Horse_power
4. Acceleration & Horse_power
5. Mpg & Horse_power
6. Mpg & Displacement
7. Mpg & Weight

```{r}
scatterplotMatrix(haberman, pch=20, col="deepskyblue")
```
El scatterplot anterior nos muestra mejor la forma de estas correlaciones.
Vemos que en todos los casos en los que se da una correlación positiva existe una tendencia lineal entre los datos de ambas variables, y en las negativas una tendencia logarítmica.

Vamos a mostrar algunas
Positivas
```{r}
ggplot(haberman, aes(x=Horse_power, y=Displacement)) +
  geom_point() +
  geom_smooth(formula = y~x, method=glm) +
  theme_light()

ggplot(haberman, aes(x=Weight, y=Displacement)) +
  geom_point() +
  geom_smooth(formula = y~x, method=glm) +
  theme_light()
```

Negativas
```{r}
ggplot(haberman, aes(x=Displacement, y=Mpg)) +
  geom_point() +
  geom_smooth(formula = y~log(x), method=glm) +
  theme_light()

ggplot(haberman, aes(x=Weight, y=Mpg)) +
  geom_point() +
  geom_smooth(formula = y~log(x), method=glm) +
  theme_light()
```

Previsualicación de las variables respecto a la salida
```{r}
ggplot(melt(haberman, "Mpg"), aes(x=value, y=Mpg, color=variable)) +
  geom_point(alpha=0.3) +
  facet_wrap(.~variable, scale="free") +
  theme_light()
```
Se aprecia alta correlación entre Displacement, Horse_power, Weight respecto de la salida.

Como habíamos supuesto en la hipótesis H.9, Horse_power podría depender de Displacement y Weight.
Esta claro que la potencia de un motor va a depender de la cilindrada y el peso que tenga.

```{r}
haberman %>%
  dplyr::select(Displacement, Horse_power, Weight) %>%  
  scatterplotMatrix(pch=20, col="deepskyblue")
```
Podemos apreciar como la función de densidad de Horse_power parece una ("MEDIANIZACIÓN") de las otras dos.

Vamos a intentar comprobarlo
```{r}
scale(haberman) %>%
  as.data.frame() %>% 
  mutate(hp = (Displacement+Weight) / 2) %>% 
  dplyr::select(Horse_power, hp) %>%  
  scatterplotMatrix(pch=20, col="deepskyblue")
```
Viendo que no son tan similares como creíamos, buscamos diferentes fórmulas para el cálculo de los caballos de vapor, y vemos que las fórmulas son un poco más complejas y no tenemos exactamente los datos necesarios para utilizarlas (no se descarta que no se puedan deducir, pero no sería un cálculo evidente)

(poner fórmulas https://www.ajdesigner.com/phphorsepower/horsepower_equation_trap_speed_method_increase_horsepower.php#:~:text=Solving%20for%20the%20change%20in,the%20vehicle%2C%20driver%20and%20passenger.)

---------------------------------------------------------------------------

### Tratamiento de variables

Para este dataset, al ser casi todas las variables numéricas continuas, existen pocos tratamientos que aplicar.

No tenemos variables categóricas que transformar.


Para añadir interpretabilidad, podríamos agrupar la variable Weight en intervalos, pero puesto que vamos a aplicar regresión sería más conveniente realizarlo con los resultados finales.

--------------------------------------------------------------------------

### Ordenaciones

Volvemos a mostrar la cabecera de los datos:
```{r}
head(haberman)
```

En este caso no es necesario aplicar ninguna reorganización.
Cada variable ocupa su propia columna, y contiene un único tipo de información, con unidades de observación diferentes
No existe ninguna relación entre variables sobre la información que codifican (en el sentido de que podrían agruparse).

<!-- Column headers are values, not variable names. -->
<!-- • Multiple variables are stored in one column. -->
<!-- • Variables are stored in both rows and columns. -->
<!-- • Multiple types of observational units are stored in the -->
<!-- same table. -->
<!-- • A single observational unit is stored in multiple tables. -->

--------------------------------------------------------------------------

#### Resolución de hipótesis

Nos habíamos planteado las siguientes hipótesis

- H.1: Horse_power puede influir en Mpg: A más potencia, más consumo.
```{r}
ggplot(haberman, aes(x=Horse_power, y=Mpg)) +
  geom_point() +
  geom_smooth(formula = y~log(x), method=glm) +
  theme_light()
```
Con el plot y los resultados de la matriz de correlación queda claro que existe una correlación negativa entre estas dos variables.
Por tanto, podemos considerar Horse_power como un buen candidato para la regresión

- H.2: Weight debe influir en Mpg: Un coche más pesado debería consumir más
idem. a la hipótesis anterior, lo hemos visto anteriormente en la figura X

- H.3: Debería haber correlación entre displacement (cilindrada) con horse y acceleration
La hemos referenciado anteriormente

- H.4: Horse y acceleration podrían estar relacionadas
```{r}
ggplot(haberman, aes(x=Horse_power, y=Acceleration)) +
  geom_point() +
  geom_smooth(formula = y~log(x), method=glm) +
  theme_light()
```
idem. se aprecia una correlación logarítmica entre las dos variables.
Similarmente a lo ocurrido con la hipótesis anterior, esto puede ser un problema para nuestro problema de regresión.

- H.5: Viendo que contamos con un rango pequeño de años, no debería haber un cambio significativo de prestaciones entre años.
```{r}
ggplot(melt(haberman, "Model_year"), aes(y=value, x=Model_year, color=variable)) +
  geom_point(alpha=0.3) +
  facet_wrap(.~variable, scale="free") +
  theme_light()
```

Existe una alta dispersión de los datos en cada una de las variables, pero aún así se aprecia tendencias en las variables.
Acceleartion y Mpg tienden a aumentar, y Displacement, Horse_power y Weight tienden a disminuir.
También vemos que la dispersión en las prestaciones de los coches disminuyen ligeramente.

Podemos creer en principio que puede deberse a un decremento del número de instancias con el paso de los años, pero recordamos que en general los datos están repartidos equitativamente
```{r}
table(haberman$Model_year)
```

Podemos ver cómo varían los rangos para cada año
```{r}
years <- haberman %>% group_split(Model_year)

for (y in years) {
  cat("Year: ")
  y$Model_year[1] %>% cat()
  y %>% apply(2, range) %>% as.data.frame() %>% print()
}
```

O mejor, de manera gráfica
```{r}

```


- H.6: Pero debería existir una tendencia de mejora de prestaciones con los años, incluyendo aumento de Displacement, Horse_power y Acceleration.

Ciertamente. Se ha comprobado en la hipótesis anterior.

- H.7: Model_year podría no mostrar relación con Mpg: Pese al paso de los años si contamos con diferentes tipos de vehículos (todoterrenos, familiares, deportivos...) podría haber un consumo dispar. (Si existiera tendencia, viendo que los años son de las últimas décadas del siglo XX, podría ir el consumo hacia abajo)

Hemos visto que existe tendencia, lineal con gran dispersión, y positiva.
```{r}
ggplot(haberman, aes(x=Model_year, y=Mpg)) +
  geom_point() +
  geom_smooth(formula = y~x, method=glm) +
  theme_light()
```

Por desgracia no contamos información sobre los modelos de los coches

Podemos ver como se ubican los diferentes años en un plot Horse_power vs Mpg
```{r}
ggplot(haberman, aes(x=Horse_power, y=Mpg, color=Model_year)) +
  geom_point() +
  theme_light()
```

Y vemos que no se puede afirmar la hipótesis, los coches están entremezclados por diferentes años

- H.8: Esta última hipótesis se puede aplicar al resto de variables, indicándonos que Model_year no debería tener relevancia para este problema de regresión.

No podemos afirmar la hipótesis anterior y por consiguiente esta tampoco.

- H.9: Horse_power podría depender de las variables Displacement y Weight

Lo hemos comentado anteriormente

--------------------------------------------------------------------------

#### Conclusiones

Como conclusiones podemos decir que tenemos un dataset altamente correlacionado, distribuído de forma no normal pero con la información bien representada.
Existen relaciones fuertes entre las variables de entrada y de las de salida para la regresión que probablemente nos ayuden a solucionar con facilidad el problema.

A falta de descubrir las distribuciones que siguen las variables para ver si merece la pena transformarlas a una distribución normal, podemos sin ninguna duda aplicar una estandarización de los datos (puesto que sabemos que no afecta negativamente al problema de regresión), siempre y cuando lo tengamos en cuenta a la hora de analizar los resultados.

Se nos pide elegir 5 regresores para la regresión y contamos exactamente con ese número, por lo que no podemos descartar ninguna variable.
Aún así, hemos visto que tenemos algunas variables más interesentas que otras.
Varibles correladas con la salida nos aumentan las posibilidades de obtener un buen regresor, pero debemos evitar usar variables correladas entre sí para evitar la multicolinealidad.
(https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/#:~:text=Multicollinearity%20occurs%20when%20independent%20variables,model%20and%20interpret%20the%20results.)
(referenciar esta frase en el apartado de regresión)