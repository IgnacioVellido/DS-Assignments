---
title: "EDA"
author: "Ignacio Vellido"
date: "11/13/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
# Para PDF output
# pdf_document: 
#     keep_tex: yes
#     df_print: kable

knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
library(tidyverse)
library(ggplot2)
library(cowplot)  # plot_grid
library(corrplot) # corr y corrplot
library(reshape2) # melt
library(dlookr) # normality
library(caret)  # preprocess
library(moments)  # skewness
library(car)  # scatterplotMatrix
library(MASS) # fit
# require(vcd)
# library(ISLR)
# library(HSAUR2)
# library(vcdExtra)
# library(cepp)
# library("dslabs")
```

# Intro
Para este trabajo contamos con dos datasets distintos: __habermanMPG6__ para aplicar Regresión y __haberman__ para aplicar Clasificación.

## Descripciones de los problemas

### haberman

http://archive.ics.uci.edu/ml/datasets/Haberman%27s+Survival
https://sci2s.ugr.es/keel/dataset.php?cod=62

Este dataset codifica el ratio de supervivencia de pacientes operados de cáncer de pecho en el Hospital Universitario de Chicago, en base a las siguientes características:

1. Age: Indica la edad del paciente en el momento de la operación.
2. Year: Los dos últimas cifras del año en el que se operó el paciente.
3. Positive: Número de nodos auxiliares positivos detectados. Esta variable hace referencia a los ganglios linfáticos que dan positivos como presentes de cáncer. A mayor número de nodos detectados, mayor es la gravedad del cáncer.
Aunque normalmente la primera zona de propagación del cáncer son estos nodos, no es la única medida de la seriedad, pues este puede propagarse a otras zonas del cuerpo.
En principio deberíamos suponer la posibilidad de que puede haber cosas de no supervivencia con bajo número de positivos, pero la bibliografía nos asegura que la probabilidad es baja.

Viendo que solo tenemos esta medida del cáncer en el dataset es posible que la operación que recivieron los pacientes sea algún tipo de cirugía de ganglios linfáticos, donde el cirujano intenta extraer los nodos afectados por el tumor.
Por consiguiente, cuanto mayor es la cantidad de nodos detectados, más complicaciones pueden acarrear de la operación.
(poner referencias)

https://www.cancer.org/cancer/breast-cancer/treatment/surgery-for-breast-cancer/lymph-node-surgery-for-breast-cancer.html
https://en.wikipedia.org/wiki/Lymph_node#:~:text=A%20lymph%20node%2C%20or%20lymph,include%20B%20and%20T%20cells.

El objetivo es poder clasificar, en base a los tres atributos, si los pacientes pueden sobrevir 5 años o más:

4. Survival: Sí/No indicando la supervivencia del paciente tras 5 años.

BUSCAR POSIBLES COMPLICACIONES

Contamos por tanto con un problema de clasificación binario en base a tres características, y con un número total de 306 instancias.

---------------------------------------------------------------------------------------------------------------------

# Análisis Estadístico de Datos

### haberman

La descripción del problema nos da alguna información adicional sobre las variables:

1. Age: Variable numérica discreta, contamos con valores enteros en el rango [30,83].
2. Year: Variable numérica discreta, contamos con valores enteros en el rango [58,69].
3. Positive: Variable numérica discreta, contamos con valores enteros en el rango [0,52].
4. Survival: Variable binaria

#### Hipótesis de partida

- H.1: Habrá menor ratio de supervivencia cuanto mayor sea el número de nodos positivos encontrados: Por los razonamientos explicados en la introducción del problema.
- H.2: Habrá mayor ratio de supervivencia cuanto más joven sea el paciente.
- H.3: El rango de Year es pequeño. La influencia de esta variable creemos que podría darse solo si durante ese período se hubieran descubierto técnicas mejores de cirugía. Este razonamiento va orientado de cara a la población y no a la muestra.
Puesto que contamos con datos de un solo hospital durante pocos años, es posible que el equipo de cirugía hubiera sido el mismo para la mayoría de pacientes.
- H.4: Podría haber relación entre la edad y el número de positivos, posiblemente indicando lo tardío que se descubre el cáncer.
- H.5: La bibliografía nos dice que el cáncer puede aparecer a diferentes edades con diferentes factores de riesgo (alcoholismo, herencia genética...). Podría ser que el número de variables con las que contamos sea insuficiente para la clasificación.

---------------------------------------------------------------------------------------------------------------------

Cargamos los datos:
```{r}
names <- c("Age", "Year", "Positive", "Survival")

haberman <- read_csv("Data/haberman/haberman.dat", comment = "@", col_names = names)
```

R por defecto nos carga las variables Age, Year y Positive como numéricas y Survival como carácter.

Vamos a transformar Survival a Factor
```{r}
haberman$Survival <- haberman$Survival %>% factor(levels = c("negative", "positive"), labels = c("No", "Yes"))
```


El resto de variables las mantenemos como numéricas


#### Análisis univariable

Los datos nos quedan por tanto de la siguiente manera:
```{r}
head(haberman)
```

Hacemos summary para sacar datos de relevancia
```{r}
summary(haberman)
```

En las distribuciones de los clasificadores nos fijaremos más adelante. Aquí hacemos notar que los valores de salida en nuestros datos están bastante desbalanceados, solo un 26.5% de los paciente sobrevivieron a los 5 años.


El dataset cuenta con valores repetidos
```{r}
sum(duplicated(haberman))
```

Mostramos estas ocurrencias:
```{r}
ind <- duplicated(haberman) | duplicated(haberman, fromLast = TRUE)
haberman[ind,] %>% arrange(Age)
```

Existen dos posibilidades para el origen de estos datos:

1. Errores en la introducción de los datos, entradas repetidas por error.
2. Sean entradas de pacientes distintos casualmente con las mismas características.

Como en este caso tenemos muy pocas variables (y un número moderado de entradas, 306), es probable que los pacientes coincidan en las características.
Además, podemos ver que las entradas en la mayoría de los casos las variables solo están duplicadas (solo hay una entrada triplicada).

Por tanto proseguimos sin eliminar estas instancias duplicadas.

---

No contamos con missing values
```{r}
sum(is.na(haberman))
```


Separamos los datos de las etiquetas
```{r}
labels <- haberman[4]
haberman <- haberman[-4]
names <- colnames(haberman)
```


Vamos a sacar plots de cada variable para verlo mejor
```{r}
ggplot(gather(haberman), aes(value)) +
  geom_histogram(bins = 13, color="white") +
  facet_wrap(~key, scales = 'free_x') +
  theme_light() +
  theme(strip.background = element_rect(fill="grey", size=2))+
  theme(strip.text = element_text(colour = 'black')) +
  labs(title="Histogramas de cada variable", x = "")
```
Una a una
<!-- MODIFICAR LOS BINS -->
```{r}
colors <- c("chocolate", "deepskyblue1", "plum1")
bins <- c(15,10,20)
plt <- list(length = length(names))

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(x=names[i])) + 
    geom_histogram(aes(y=..density..), size=1, bins=bins[i], color="black", fill=colors[i]) +
    geom_density(alpha=.3, fill="black", color="green", size=.5) +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("Histograma %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

```{r}
colors <- c("chocolate", "deepskyblue1", "plum1")
plt <- list(length = length(names))

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(x=names[i])) + 
    geom_boxplot(fill = colors[i]) +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("Boxplot %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

```{r}
ggplot(melt(haberman), aes(x=variable, y=value)) + 
  geom_boxplot() +
  labs(title="Boxplot con mismo rango") +
  theme(axis.text.x = element_text(angle = 90))
```

---------------------------------------------------------------------------

## Missing values

Nos cuestionamos la ocurrencia de instancias con cero en el número de positivos.
Creemos que se trata de una codificación de missing values porque los datos provienen de operaciones de cáncer que como ya hemos comentado probablemente consistan en eliminar estos nodos positivos.

Tenemos que ver cómo tratar estas instancias:
```{r}
haberman %>% filter(Positive == 0) %>% count()
```

Vemos que contamos con un gran número de elas, por lo que directamente descartamos eliminarlas del dataset.
Podemos ahora aplicar interpolación, o insertar un valor fijo (ES LO MISMO, NO ?)

---------------------------------------------------------------------------

Repetimos los plots anteriores, ya que tenemos los datos tratados:
```{r}
ggplot(gather(haberman), aes(value)) +
  geom_histogram(bins = 13, color="white") +
  facet_wrap(~key, scales = 'free_x') +
  theme_light() +
  theme(strip.background = element_rect(fill="grey", size=2))+
  theme(strip.text = element_text(colour = 'black')) +
  labs(title="Histogramas de cada variable", x = "")
```

Una a una
```{r}
colors <- c("chocolate", "deepskyblue1", "plum1")
bins <- c(15,10,20)
plt <- list(length = length(names))

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(x=names[i])) + 
    geom_histogram(aes(y=..density..), size=1, bins=bins[i], color="black", fill=colors[i]) +
    geom_density(alpha=.3, fill="black", color="green", size=.5) +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("Histograma %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

```{r}
colors <- c("chocolate", "deepskyblue1", "plum1")
plt <- list(length = length(names))

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(x=names[i])) + 
    geom_boxplot(fill = colors[i]) +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("Boxplot %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

```{r}
ggplot(melt(haberman), aes(x=variable, y=value)) + 
  geom_boxplot() +
  labs(title="Boxplot con mismo rango") +
  theme(axis.text.x = element_text(angle = 90))
```

Ya la descripción del problema nos lo decía, los rangos en los que se distribuyen los datos son muy diferentes dependiendo de la variable. Es necesario aplicar un proceso de estandarización para clasificación.

Podemos comparar los rangos intercuartiles si estandarizamos antes el dataset
```{r}
scale(haberman) %>% apply(2, IQR)
```

También podemos ver la distancia entre mínimos y máximos
```{r}
scale(haberman) %>% apply(2, range) %>% apply(2, dist)
```


<!-- library(vcd) -->
<!-- mosaic(Titanic -->
<!-- ,shade=TRUE) -->

#### Age

Vemos que no contamos con valores de todos los años:
```{r}
table(haberman$Age)
table(haberman$Age) %>% length

(83-30+1) == table(haberman$Age) %>% length
```

#### Year

Aunque no se vea bien en las gráficas, contamos con valores de todos los años, con mayor cantidad en los iniciales:
```{r}
table(haberman$Year)
```

##### Positive

La variable Positive parece llevar una distribución exponencial, y problablemente por ello aparezcan tantos posibles outliers.

SEGURAMENTE TRAS QUITAR LOS MISSING VALUES YA NO SIGA UNA EXP

--------------------------------------------------------------------------------

### Análisis sobre las distribuciones

Ninguna variable parece seguir una distribución semejante a una distribución normal, lo comprobamos con un test estadístico (Shapiro-Wilk test):
```{r}
normality(haberman) %>% filter(p_value < 0.05)
```
El test de Shapiro nos dice claramente que ninguna variable sigue una distribución normal.

Lo mostramos gráficamente con plots Q-Q:
```{r}
plt <- list(length = length(names))

x<-rnorm(100, mean=0, sd=1)

for (i in 1:length(names)) {
  ggplot(haberman, aes_string(sample=names[i])) + 
    stat_qq(alpha=.3, fill=colors[i], size=1) +
    stat_qq_line() +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("QQ-plot %s", names[i]), x=""))
}

plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

Se ve que las distribuciones no siguen los cuartiles normales, mayormente en las colas.

La variable Positive parece seguir una distribución exponencial, hacemos un test de Kolmogorov-Smirnov para corroborarlo:
<!-- HABRÍA QUE DEJARLO UNIQUE - PROBABLEMENTE NO SE PUEDA -->
```{r}
# data generation
ex <- rexp(10000, rate = 1.85) # generate some exponential distribution
control <- abs(rnorm(10000)) # generate some other distribution

# estimate the parameters
fit1 <- fitdistr(ex, "exponential") 
fit2 <- fitdistr(haberman$Positive %>% unique(), "exponential")

# goodness of fit test
ks.test(ex, "pexp", fit1$estimate) # p-value > 0.05 -> distribution not refused
ks.test(haberman$Positive, "pexp", fit2$estimate) #  significant p-value -> distribution refused

# plot a graph
hist(haberman$Positive, freq = FALSE, breaks = 100, xlim = c(0, quantile(haberman$Positive, 0.99)))
curve(dexp(x, rate = fit1$estimate), from = 0, col = "red", add = TRUE)

# control
```

Al ser el p-valor <0.05 el test nos lo rechaza.
El plot también nos lo muestra más claramente, la forma de la cola de la distribución probablemente sea la causante de que no siga ese tipo de distribución

Podemos hacer un gráfico QQ con los cuartiles de una distribución exponencial
```{r}
# From https://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution/76998
qqexp <-  function(y, line=FALSE, ...) { 
    y <- y[!is.na(y)]
    n <- length(y)
    x <- qexp(c(1:n)/(n+1))
    m <- mean(y)
    if (any(range(y)<0)) stop("Data contains negative values")
    ylim <- c(0,max(y))
    qqplot(x, y, xlab="Exponential plotting position",ylim=ylim,ylab="Ordered sample", ...)
    if (line) abline(0,m,lty=2)
    invisible()
}

qqexp(haberman$Positive, line=TRUE)
```

Solo vemos skewness en la variable Positive, lo comprobamos:
```{r}
skewCols <- find_skewness(haberman)
colnames(haberman)[skewCols]
```
Calculamos el grado que tienen:
```{r}
cat("Positive: ")
skewness(haberman$Positive)
cat("Year: ")
skewness(haberman$Year)
cat("Age: ")
skewness(haberman$Age)
```
Positive tiene skewness positiva en un alto grado, las demás tienen tan poco como para poder considerarlo.

-------------------------------------------------------------

### Transformaciones

Dejamos que el paquete caret nos proponga metodos de preprocesado
```{r}
preProcess(haberman)
```

Nos sugiere una estandarización a media cero y desviación típica 1.
Para un problema de clasificación esto es totalmente necesario puesto que no queremos que los diferentes rangos de las variables hagan que haya información de más peso que otra.

Según el método utilizado la necesitad de normalidad puede ser o no necesaria. (CORROBORAR)
Aplicar métodos de reducción de skewness en Positive no parece interesante puesto que está demasiado ladeado.

```{r}
haberman_transform <- preProcess(haberman, method=c("scale", "center"))
# haberman_transform <- preProcess(haberman, method=c("YeoJohnson","scale", "center"))
haberman_norm <- predict(haberman_transform, haberman)
```

<!-- Por ver lo mal que queda con YeoJohnson -->
```{r}
# colors <- c("chocolate", "deepskyblue1", "plum1")
# bins <- c(15,10,20)
# plt <- list(length = length(names))
# 
# for (i in 1:length(names)) {
#   ggplot(haberman_norm, aes_string(x=names[i])) + 
#     geom_histogram(aes(y=..density..), size=1, bins=bins[i], color="black", fill=colors[i]) +
#     geom_density(alpha=.3, fill="black", color="green", size=.5) +
#     labs(title="", x="", y="") +
#     theme_light() -> plt[[i]]
#   
#   print(plt[[i]] + labs(title=sprintf("Histograma %s", names[i]), x=""))
# }
# 
# plot_grid(plotlist=plt, ncol=2, labels = names, label_size = 8)
```

-----------------------------------------------------------

### Outliers

La única variable en la que podríamos considerar outliers es Positive. Tanto para la edad como para los años no tiene sentido, además de que hemos visto en los boxplots que en ellas todos los valores caen en el 95% de la distribución.

A la hora de considerar los outliers en Positive, tal y como habíamos mencionado en la descripción del problema, un alto número de nodos detectados complica la operación y el pronóstico para el paciente. 

Podemos mostrar para aquellos valores outliers la cantidad de sobrevivientes:
```{r}
haberman %>% 
  bind_cols(labels) %>% 
  filter(Positive>10) %>%
  summarise(Survival) %>% 
  table()
```

Vemos que realmente está equilibrado.
Aún así, si hubiera una tendencia negativa por un alto número en Positive queríamos que nuestro clasificador fuera capaz de aprenderlo, por lo que proseguimos sin eliminar outliers.

-----------------------------------------------------------

### Análisis de correlación

Como este es un problema de clasificación, necesitamos eliminar aquellas variables correladas para que la información se aporte de manera equitativa.
Las gráficas no nos han dado ninguna señal de una posible correlación, pero debemos asegurarnos de forma estadística.

Tenemos que tener en cuenta que las variables no siguen distribuciones normales.
Aunque el coeficiente de Pearson no asume normalidad (si asume varianza y covarianza finitas), podemos usar el coeficiente de Kendall para los cálculos.
Independientemente del método usado vamos a obtener las mismas correlaciones en este dataset, solo varía la fuerza con la que se dan.
<!-- Las variables no siguen distribución normal, pero no pasa nada, Pearson no da problemas -->

Corrplot
```{r}
corrplot.mixed(cor(haberman), tl.pos="lt", upper="color", title="Pearson")
corrplot.mixed(cor(haberman, method="kendall"), tl.pos="lt", upper="color", title="Kendall")
```

Nos muestra que no existe correlación alguna entre las variables.

```{r}
scatterplotMatrix(haberman, pch=20, col="deepskyblue")
```
El scatterplot anterior nos muestra mejor la forma de las relaciones entre variables, vemos que no existe tendencia alguna.

Miramos la distribución de las variables con su clasificación
```{r}
haberman %>% 
  bind_cols(labels) %>% 
  ggplot(aes(x=Age, y=Year, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()

haberman %>% 
  bind_cols(labels) %>% 
  ggplot(aes(y=Age, x=Positive, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()

haberman %>% 
  bind_cols(labels) %>% 
  ggplot(aes(x=Year, y=Positive, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()
```

No se aprecia ninguna relación visual que nos ayude a clasificar el Survival.

---------------------------------------------------------------------------

### Tratamiento de variables

Para este dataset, al ser casi todas las variables numéricas continuas, existen pocos tratamientos que aplicar.

No tenemos variables categóricas que transformar.


Para añadir interpretabilidad, podríamos agrupar la variable Weight en intervalos, pero puesto que vamos a aplicar regresión sería más conveniente realizarlo con los resultados finales.

--------------------------------------------------------------------------

### Ordenaciones

Volvemos a mostrar la cabecera de los datos:
```{r}
head(haberman)
```

En este caso no es necesario aplicar ninguna reorganización.
Cada variable ocupa su propia columna, y contiene un único tipo de información, con unidades de observación diferentes
No existe ninguna relación entre variables sobre la información que codifican (en el sentido de que podrían agruparse).

<!-- Column headers are values, not variable names. -->
<!-- • Multiple variables are stored in one column. -->
<!-- • Variables are stored in both rows and columns. -->
<!-- • Multiple types of observational units are stored in the -->
<!-- same table. -->
<!-- • A single observational unit is stored in multiple tables. -->

--------------------------------------------------------------------------

#### Resolución de hipótesis

Nos habíamos planteado las siguientes hipótesis

- H.1: Horse_power puede influir en Mpg: A más potencia, más consumo.
```{r}
ggplot(haberman, aes(x=Horse_power, y=Mpg)) +
  geom_point() +
  geom_smooth(formula = y~log(x), method=glm) +
  theme_light()
```
Con el plot y los resultados de la matriz de correlación queda claro que existe una correlación negativa entre estas dos variables.
Por tanto, podemos considerar Horse_power como un buen candidato para la regresión

- H.2: Weight debe influir en Mpg: Un coche más pesado debería consumir más
idem. a la hipótesis anterior, lo hemos visto anteriormente en la figura X

- H.3: Debería haber correlación entre displacement (cilindrada) con horse y acceleration
La hemos referenciado anteriormente

- H.4: Horse y acceleration podrían estar relacionadas
```{r}
ggplot(haberman, aes(x=Horse_power, y=Acceleration)) +
  geom_point() +
  geom_smooth(formula = y~log(x), method=glm) +
  theme_light()
```
idem. se aprecia una correlación logarítmica entre las dos variables.
Similarmente a lo ocurrido con la hipótesis anterior, esto puede ser un problema para nuestro problema de regresión.

- H.5: Viendo que contamos con un rango pequeño de años, no debería haber un cambio significativo de prestaciones entre años.
```{r}
ggplot(melt(haberman, "Model_year"), aes(y=value, x=Model_year, color=variable)) +
  geom_point(alpha=0.3) +
  facet_wrap(.~variable, scale="free") +
  theme_light()
```

Existe una alta dispersión de los datos en cada una de las variables, pero aún así se aprecia tendencias en las variables.
Acceleartion y Mpg tienden a aumentar, y Displacement, Horse_power y Weight tienden a disminuir.
También vemos que la dispersión en las prestaciones de los coches disminuyen ligeramente.

Podemos creer en principio que puede deberse a un decremento del número de instancias con el paso de los años, pero recordamos que en general los datos están repartidos equitativamente
```{r}
table(haberman$Model_year)
```

Podemos ver cómo varían los rangos para cada año
```{r}
years <- haberman %>% group_split(Model_year)

for (y in years) {
  cat("Year: ")
  y$Model_year[1] %>% cat()
  y %>% apply(2, range) %>% as.data.frame() %>% print()
}
```

O mejor, de manera gráfica
```{r}

```


- H.6: Pero debería existir una tendencia de mejora de prestaciones con los años, incluyendo aumento de Displacement, Horse_power y Acceleration.

Ciertamente. Se ha comprobado en la hipótesis anterior.

- H.7: Model_year podría no mostrar relación con Mpg: Pese al paso de los años si contamos con diferentes tipos de vehículos (todoterrenos, familiares, deportivos...) podría haber un consumo dispar. (Si existiera tendencia, viendo que los años son de las últimas décadas del siglo XX, podría ir el consumo hacia abajo)

Hemos visto que existe tendencia, lineal con gran dispersión, y positiva.
```{r}
ggplot(haberman, aes(x=Model_year, y=Mpg)) +
  geom_point() +
  geom_smooth(formula = y~x, method=glm) +
  theme_light()
```

Por desgracia no contamos información sobre los modelos de los coches

Podemos ver como se ubican los diferentes años en un plot Horse_power vs Mpg
```{r}
ggplot(haberman, aes(x=Horse_power, y=Mpg, color=Model_year)) +
  geom_point() +
  theme_light()
```

Y vemos que no se puede afirmar la hipótesis, los coches están entremezclados por diferentes años

- H.8: Esta última hipótesis se puede aplicar al resto de variables, indicándonos que Model_year no debería tener relevancia para este problema de regresión.

No podemos afirmar la hipótesis anterior y por consiguiente esta tampoco.

- H.9: Horse_power podría depender de las variables Displacement y Weight

Lo hemos comentado anteriormente

--------------------------------------------------------------------------

#### Conclusiones

Como conclusiones podemos decir que tenemos un dataset altamente correlacionado, distribuído de forma no normal pero con la información bien representada.
Existen relaciones fuertes entre las variables de entrada y de las de salida para la regresión que probablemente nos ayuden a solucionar con facilidad el problema.

A falta de descubrir las distribuciones que siguen las variables para ver si merece la pena transformarlas a una distribución normal, podemos sin ninguna duda aplicar una estandarización de los datos (puesto que sabemos que no afecta negativamente al problema de regresión), siempre y cuando lo tengamos en cuenta a la hora de analizar los resultados.

Se nos pide elegir 5 regresores para la regresión y contamos exactamente con ese número, por lo que no podemos descartar ninguna variable.
Aún así, hemos visto que tenemos algunas variables más interesentas que otras.
Varibles correladas con la salida nos aumentan las posibilidades de obtener un buen regresor, pero debemos evitar usar variables correladas entre sí para evitar la multicolinealidad.
(https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/#:~:text=Multicollinearity%20occurs%20when%20independent%20variables,model%20and%20interpret%20the%20results.)
(referenciar esta frase en el apartado de regresión)