---
title: "Clasificacion"
author: "Ignacio Vellido"
date: "11/24/2020"
output: 
  pdf_document:
    keep_tex: yes
    df_print: kable
---

```{r setup, set.seed(222), include=FALSE}
set.seed(222)
RNGkind(sample.kind="Rejection")
knitr::opts_chunk$set(echo = F, results="hold", fig.align="center", 
                      comment=NA, cache=T, messages=FALSE)

library(tidyverse)
library(ggplot2)
library(cowplot)  # plot_grid
library(corrplot) # corr y corrplot
library(reshape2) # melt
library(dlookr) # normality
library(caret)  # preprocess
library(moments)  # skewness
library(car)  # scatterplotMatrix
library(MASS) # fit
library(scatterplot3d)
```


Cargamos los datos
```{r}
names <- c("Age", "Year", "Positive", "Survival")

haberman <- read_csv("Data/haberman/haberman.dat", comment = "@", col_names = names)
haberman$Survival <- haberman$Survival %>% factor(levels = c("negative", "positive"), labels = c("No", "Yes"))
```

Los preprocesamos (estandarización)
```{r}
haberman_transform <- preProcess(haberman, method=c("scale", "center"))
haberman_norm <- predict(haberman_transform, haberman)
```


Creamos holdout del 90%
```{r}
# Create training and test data (holdout 90%-10%)
shuffle_ds <- sample(dim(haberman_norm)[1])

pct90 <- (dim(haberman_norm)[1] * 90) %/% 100

train <- haberman_norm[shuffle_ds[1:pct90], -4] %>% as.data.frame()
test <- haberman_norm[shuffle_ds[(pct90+1):dim(haberman_norm)[1]], -4] %>% as.data.frame()
```


Separamos los datos de las etiquetas
```{r}
train_labels <- haberman[shuffle_ds[1:pct90], 4] %>% unlist()
test_labels <- haberman[shuffle_ds[(pct90+1):dim(haberman)[1]], 4] %>% unlist()
```

--------------------------------------------------------------------------------

## Utilizar el algoritmo k-NN probando con diferentes valores de k. Elegir el que considere más adecuado para su conjunto de datos. Analice qué ocurre en los valores de precisión en training y test con los diferentes valores de k.

Recordamos los gráficos 1-1 con las clasificaciones, vistos en el EDA.
```{r}
haberman %>%
  ggplot(aes(x=Age, y=Year, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()

haberman %>%
  ggplot(aes(y=Age, x=Positive, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()

haberman %>%
  ggplot(aes(x=Year, y=Positive, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()
```

```{r}
colors <- c("coral2", "deepskyblue")
colors <- colors[haberman$Survival]

scatterplot3d(haberman[,1:3], angle = 290,
              main = "3D plot",
              pch=20, color = colors)

scatterplot3d(haberman[,1:3], angle = 50,
              main = "3D plot",
              pch=20, color = colors)
```

De cara a un algoritmo KNN, apreciamos los datos muy entremezclados, con mayor tendencia a agruparse los no supervivientes que los que sí, pero nada que nos llame la atención.

Debido a esto vamos a empezar con un valor de K relativamente bajo y vamos a ir aumentándolo poco a poco.
Tenemos que tener cuidado con el overfitting, eso sí.

```{r}
knnModel <- train(train, y = train_labels,
                  method = "knn", 
                  trControl = trainControl(method = "cv"), 
                  tuneGrid = data.frame(.k=3:15))

knnModel
```

Recordamos que los datos ya estaban previamente preprocesados (estandarizados, concretamente)

```{r}
knnPred <- predict(knnModel, newdata = test)
ctable <- table(knnPred, test_labels)

ctable
knnResample <- postResample(pred = knnPred, obs = test_labels)
knnResample
```

```{r}
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix KNN")
```
Vemos que al estar los datos tan entremezclados ni siquiera con un K pequeño sobreaprende, es ya con un K medianamente alto (= 13) donde obtiene mayor accuracy en train.

Probablemente esto se deba a la gran mezcla de los datos, de forma que necesite la "opinión" de un gran número de vecinos para poder predecir con mayor confianza el nuevo valor.

```{r}
ggplot(train, aes(x = Age, y = Year, color=predict(knnModel, newdata = train))) +
  geom_point(show.legend = F) +
  labs(title="Prediction KNN") +
  theme_light() -> knnplot

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point() +
  labs(title="True values") +
  theme_light() -> trueplot

# Get legend
legend <- get_legend(trueplot)

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point(show.legend = F) +
  labs(title="True values") +
  theme_light() -> trueplot

plot_grid(plotlist=list(knnplot,trueplot,legend), ncol=2, label_size = 1)
```

Vemos que el uso de un K alto hace que perdamos puntos de Yes. Probablemente al ser minoría el error cometido clasificándolos como No es menor y por eso obtiene mejor accuracy.

En principio (a falta de evaluar con CV) creemos que en comparación con otros algoritmos el hecho de que el dataset este desbalanceado y los datos muy entremezclados la calidad real (fuera del entrenamiento) no sea muy buena.
Podría ser que en la población se mantega este desbalanceo en los datos y funcione bien, pero en caso de que no lo sea, el valor de K tan alto haría que probablemente se tienda a devolver una predicción de No.

Como habíamos comentado al principio, para el problema que nos atañe quizás esto podría ser incluso un hecho positivo, ya que los falsos positivos sería algo que querríamos evitar a toda costa.

--------------------------------------------------------------------------------

Podemos también coger otros valores de K en test. Puesto que hemos obtenido los mejores resultados en training con un K de 13, que es un valor relativamente alto, podemo probar con uno bajo y uno intermedio (3 y 7)

```{r}
knn3Model <- train(train, y = train_labels,
                  method = "knn", 
                  trControl = trainControl(method = "cv"), 
                  tuneGrid = data.frame(.k=3))

knn3Model
```
```{r}
knn3Pred <- predict(knn3Model, newdata = test)
ctable <- table(knn3Pred, test_labels)

ctable
postResample(pred = knn3Pred, obs = test_labels)

fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix KNN - K=3")
```

```{r}
knn7Model <- train(train, y = train_labels,
                  method = "knn", 
                  trControl = trainControl(method = "cv"), 
                  tuneGrid = data.frame(.k=7))

knn7Model
```

```{r}
knn7Pred <- predict(knn7Model, newdata = test)
ctable <- table(knn7Pred, test_labels)

ctable
postResample(pred = knn7Pred, obs = test_labels)

fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix KNN - K=7")
```

K=7 vemos que es el que más sufre al evaluar en test, y ambos (tal y como nos había indicado CV) tienen una calidad bastante inferior a un K=13

--------------------------------------------------------------------------------

## Utilizar el algoritmo LDA para clasificar. No olvide comprobar las asunciones.

Comprobamos asunciones:

1- Distribución aleatoria: No nos queda más remedio que creer que sí.

2- Cada predictor sigue una distribución normal: Ya vimos en el EDA que esto no era cierto. El test de Shapiro nos demostraba que no y los QQ-plots nos lo hacían ver claramente.
Técnicamente sabiendo esto no deberíamos usar LDA, pero puesto que esto es un proyecto seguimos igualmente.

Aún así, las variables Age y Year no parecen seguir una distribución demasiado "rara" (en comparación con una normal), por lo que es posible que obtengamos resultados decentes.

3- Las clases siguen la misma matriz de covarianza:
Solo nos interesa la diagonal
```{r}
yes <- train %>% bind_cols(train_labels) %>% setNames(names) %>% filter(Survival == "Yes") %>% dplyr::select(-Survival)
no  <- train %>% bind_cols(train_labels) %>% setNames(names) %>% filter(Survival == "No") %>% dplyr::select(-Survival)

cat("Para clase Yes:\n")
cov(yes) %>% diag()
cat("Para clase No:\n")
cov(no) %>% diag()
```
Las variables Age y Positive parecen seguir distintas varianzas, lo aseguramos con un test estadístico.

Puesto que nuestras variables no siguen una distribución normal, no podemos hacer el test de homogeneidad de Barlett.
Utilizamos por tanto el de Levene
```{r}
cat("Age:\n")
leveneTest(Age ~ Survival, haberman)
cat("Year:\n")
leveneTest(Year ~ Survival, haberman)
cat("Positive:\n")
leveneTest(Positive ~ Survival, haberman)
```

Indicándonos que solo se puede asegurar que la variable Positive no tiene homogeneidad entre clases diferentes.
<!-- Se puede apreciar la diferencia en los p-values de las variables Age y Year -->

Podemos verlo más claro gráficamente
```{r}
ggplot(haberman, aes(x = Year, color = Survival, fill=Survival)) + 
  geom_boxplot(color="black") + 
  labs(title = "Varianza entre clases para Year") +
  theme_light()

ggplot(haberman, aes(x = Age, color = Survival, fill=Survival)) + 
  geom_boxplot(color="black") + 
  labs(title = "Varianza entre clases para Age") +
  theme_light()

ggplot(haberman, aes(x = Positive, color = Survival, fill=Survival)) + 
  geom_boxplot(color="black") + 
  labs(title = "Varianza entre clases para Positive") +
  theme_light()
```


Podemos mostrar las varianzas en grafos de elipses:
```{r}
ggplot(haberman, aes(x = Year, y = Age, color = Survival)) + 
  geom_point() + 
  stat_ellipse() +
  theme_light()

ggplot(haberman, aes(x = Year, y = Positive, color = Survival)) + 
  geom_point() + 
  stat_ellipse() +
  theme_light()

ggplot(haberman, aes(x = Positive, y = Age, color = Survival)) + 
  geom_point() + 
  stat_ellipse() +
  theme_light()
```

Se nota que la causa de que no se rechace el test para esta variable es la gran cantidad de datos con Positive=0

Por tanto para LDA no podemos hacer uso de la variable Positive, pero sí de las otras dos.

<!-- http://thatdatatho.com/2018/02/19/assumption-checking-lda-vs-qda-r-tutorial-2/ -->


Aunque solo es recomendable, y no son cualidades necesarias para obtener solución en LDA:
- Tenemos más instancias que predictores, por varios órdenes de magnitud.
- Los predictores son independientes.
- No tenemos varianza cercana a cero

### Aplicando LDA

```{r}
ldaModel <- train(train %>% dplyr::select(-Positive), y = train_labels,
                method = "lda",
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

ldaModel$finalModel
confusionMatrix(ldaModel)
```
Predecimos en test
```{r}
ldaPred <- predict(ldaModel, newdata = test)
ctable <- table(ldaPred, test_labels)

ctable
ldaResample <- postResample(pred = ldaPred, obs = test_labels)
ldaResample
```

```{r}
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix LDA")
```

En training
```{r}
ggplot(train, aes(x = Age, y = Year, color=predict(ldaModel, newdata = train))) +
  geom_point(show.legend = F) +
  labs(title="Prediction LDA") +
  theme_light() -> ldaplot

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point() +
  labs(title="True values") +
  theme_light() -> trueplot

# Get legend
legend <- get_legend(trueplot)

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point(show.legend = F) +
  labs(title="True values") +
  theme_light() -> trueplot

plot_grid(plotlist=list(ldaplot,trueplot,legend), ncol=2, label_size = 1)
```


Hacemos un plot del ajuste
```{r}
data <- train %>% bind_cols(train_labels) %>% setNames(names) 
ldafit <- lda(Survival~Age+Year, data = data)
plot(ldafit)
ldafit$post
```


```{r}
x <- seq(min(train$Age), max(train$Age), length.out=dim(train)[1])
y <- seq(min(train$Year), max(train$Year), length.out=dim(train)[1])

Xcon <- matrix(c(rep(x,length(y)), rep(y, rep(length(x), length(y)))),ncol=2) #Set all possible pairs of x and y on a grid

#posterior probabilities of a point belonging to each class
out <- predict(ldafit, data.frame(Age=Xcon[,1], Year=Xcon[,2]))$post

pr <- data.frame(x=rep(x, length(y)), 
                 y=rep(y, each=length(x)), 
                 z=as.vector(out))

ggplot(data, aes(x=Age, y=Year)) +
  geom_point(size = 2, aes(pch=Survival,  col=Survival)) +
  geom_contour(data = pr, aes(x=x, y=y, z=z)) +
  theme_light()
```
(Estamos pintando un contorno 3D y por eso nos salen múltiples líneas en el gráfico)

Tenemos un dataset bastante desbalanceado, y LDA no predice para la clase Yes. Con esto se asegura un alto accuracy en nuestro entrenamiento, pero no asegura de que para datos externos vaya a ser así.
Pese a ello, no nos queda más remedio que suponer que nuestros datos vienen de la misma muestra aleatoria y por tanto son releventes para la clasificación.

--------------------------------------------------------------------------------

## Utilizar el algoritmo QDA para clasificar. No olvide comprobar las asunciones.

QDA tiene las mismas asunciones de LDA salvo que relaja la norma de que las clases tengan igual covarianza. Esto nos permite usar la variable Positive que habíamos descartado en LDA.

Por tanto tenemos los requisitos de:
- Distribución aleatoria
- Distribución normal

Técnicamente el no cumplir normalidad no imposibilita que se encuentre solución, pero ya no nos lo asegura.

Además tenemos de forma recomendada que:
- El número de predictores debe ser menor que el número de instancias de cada clase.
Cosa que sabemos que sí por la tabla Yes/No
- Los predictores dentro de cada clase no deben estar correlacionados.

```{r}
corrplot.mixed(cor(yes), tl.pos="lt", upper="color", lower.col="black", title="Pearson", mar=c(0,0,1,0))
corrplot.mixed(cor(yes, method="kendall"), tl.pos="lt", upper="color", lower.col="black", title="Kendall", mar=c(0,0,1,0))
```


```{r}
corrplot.mixed(cor(no), tl.pos="lt", upper="color", lower.col="black", title="Pearson", mar=c(0,0,1,0))
corrplot.mixed(cor(no, method="kendall"), tl.pos="lt", upper="color", lower.col="black", title="Kendall", mar=c(0,0,1,0))
```

No tenemos predictores correlacionados dentro de cada clase.

--------------------------------------------------------------------------------

### Aplicando QDA

```{r}
qdaModel <- train(train, y = train_labels,
                method = "qda",
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

qdaModel$finalModel
confusionMatrix(qdaModel)
```

```{r}
qdaPred <- predict(qdaModel, newdata = test)
ctable <- table(qdaPred, test_labels)

ctable
qdaResample <- postResample(pred = qdaPred, obs = test_labels)
qdaResample
```

```{r}
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix QDA")
```

En training
```{r}
ggplot(train, aes(x = Age, y = Year, color=predict(qdaModel, newdata = train))) +
  geom_point(show.legend = F) +
  labs(title="Prediction QDA") +
  theme_light() -> qdaplot

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point() +
  labs(title="True values") +
  theme_light() -> trueplot

# Get legend
legend <- get_legend(trueplot)

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point(show.legend = F) +
  labs(title="True values") +
  theme_light() -> trueplot

plot_grid(plotlist=list(qdaplot,trueplot,legend), ncol=2, label_size = 1)
```

Obtenemos resultados extremadamente similares a LDA, pero en este caso vemos que sí se predice la clase Yes.

--------------------------------------------------------------------------------

## Comparar los resultados de los tres algoritmos


Si nos fijamos únicamente en los resultados obtenidos para este problema, los tres algoritmos obtienen el mismo accuracy en nuestro conjunto de test.
Aunque las etiquetas de este conjunto contienen elementos de ambas clases, podemos ver que se predice mayoritariamente la clase No.
Como se había mencionado nuestro dataset está bastante desbalanceado, por lo que era más probable que se predijera esa clase con mayor facilidad.

(Recordamos que las medidas devueltas en cada algoritmo provienen de un CV de 10-fold usando el paquete caret)

```{r}
cat("Predicciones LDA:\n")
ldaPred
cat("Predicciones QDA:\n")
qdaPred
cat("Predicciones KNN:\n")
knnPred
cat("\nEtiquetas:\n")
test_labels %>% unname()
```

```{r}
cat("Accuracy KNN:\n")
knnResample
cat("\nAccuracy LDA:\n")
ldaResample
cat("\nAccuracy QDA:\n")
qdaResample
```

```{r}
acc <- matrix(data = c(knnResample, ldaResample, qdaResample), nrow = 2) %>% t() %>% as.data.frame()
colnames(acc) <- c("Accuracy","Kappa")
acc <- acc %>% mutate(name=c("KNN", "LDA", "QDA"))

ggplot(acc, aes(x=name, y=Accuracy, fill=name)) +
  geom_col() +
  ylim(0,1) +
  labs(title="Accuracy en test") +
  xlab("") +
  guides(fill=FALSE) +
  theme_light()
```

<!-- https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english -->
Tenemos la misma accuracy, pero diferentes valores de Kappa, siendo en general todos bajos.

Pese a esto, puesto que no cumplimos las asunciones necesarias para asegurarnos resultados en LDA y QDA, para este problema obtaríamos por usar el algoritmo KNN.

------------------------------------

Por otro lado, podemos hacer una comparativa general de la calidad de los algoritmos haciendo uso de las tablas proporcionadas para la práctica. Para ello, primeramente...

Podríamos leer los de training pero para comparar necesitamos los de test
```{r}
results <- read_csv("Data/clasif_test_alumos.csv")
# results <- results[-1]
results
```

### Comparativas 1-1 con Wilconxon

LDA vs QDA

```{r}
# TABLA NORMALIZADA - lda (other) vs qda (ref) para WILCOXON 
# + 0.1 porque wilcox R falla para valores == 0 en la tabla

difs <- (results[,3] - results[,4]) / results[,3]
difs <- difs %>% as.matrix()
res_norm <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(res_norm) <- c(colnames(results)[3], colnames(results)[4])
```


```{r}
# Aplicación del test de WILCOXON
wilcoxon <- wilcox.test(res_norm[,1], res_norm[,2], alternative = "two.sided", paired=TRUE)
Rmas <- wilcoxon$statistic

pvalue <- wilcoxon$p.value

wilcoxon <- wilcox.test(res_norm[,2], res_norm[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- wilcoxon$statistic

cat("LDA vs QDA: ")
wilcoxon
cat("R+: ")
Rmas
cat("R-: ")
Rmenos
cat("p-value: ")
pvalue
```

Obtenemos un ranking de 144 para LDA y 96 para QDA, con un p-valor de 0.75 (o nivel de confianza del 25%).

Esto nos dice que LDA obtiene mejores resultados pero puesto que el p-value es extremadamente grande no podemos afirmar con garantía estadística que las diferencias entre los tests sean notorias.

--------------------------------------------------------------------------------

LDA vs KNN

```{r}
# TABLA NORMALIZADA +0.1 porque wilcox R falla para valores == 0 en la tabla
difs <- (results[,3] - results[,2]) / results[,3]
difs <- difs %>% as.matrix()
res_norm <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(res_norm) <- c(colnames(results)[3], colnames(results)[2])
```


```{r}
# Aplicación del test de WILCOXON
wilcoxon <- wilcox.test(res_norm[,1], res_norm[,2], alternative = "two.sided", paired=TRUE)
Rmas <- wilcoxon$statistic

pvalue <- wilcoxon$p.value

wilcoxon <- wilcox.test(res_norm[,2], res_norm[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- wilcoxon$statistic

cat("LDA vs KNN: ")
wilcoxon
cat("R+: ")
Rmas
cat("R-: ")
Rmenos
cat("p-value: ")
pvalue
```

Ahora obtenemos un ranking de 90 para LDA y 120 para QDA, con un p-valor de 0.59 (o nivel de confianza del 41%).

Seguimos teniendo un p-valor demasiado grande para poder asegurar con significación la diferencia.

--------------------------------------------------------------------------------

QDA vs KNN

```{r}
# TABLA NORMALIZADA +0.1 porque wilcox R falla para valores == 0 en la tabla
difs <- (results[,4] - results[,2]) / results[,3]
difs <- difs %>% as.matrix()
res_norm <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(res_norm) <- c(colnames(results)[4], colnames(results)[2])
```


```{r}
# Aplicación del test de WILCOXON
wilcoxon <- wilcox.test(res_norm[,1], res_norm[,2], alternative = "two.sided", paired=TRUE)
Rmas <- wilcoxon$statistic

pvalue <- wilcoxon$p.value

wilcoxon <- wilcox.test(res_norm[,2], res_norm[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- wilcoxon$statistic

cat("QDA vs KNN: ")
wilcoxon
cat("R+: ")
Rmas
cat("R-: ")
Rmenos
cat("p-value: ")
pvalue
```

Por último tenemos un ranking de 69 para LDA y 141 para KNN, con un p-valor de 0.18 (o nivel de confianza del 82%).

Ya ahora podemos afirmar al 82% que los resultados de ambos algoritmos sí son significativamente diferentes, pero para hacerlo con bastante seguridad realmente buscaríamos al menos un 95% de confianza.


--------------------------------------------------------------------------------

### Comparativa múltiple con Friedman
```{r}
# Aplicación del test de Friedman
test_friedman <- friedman.test(as.matrix(results[,2:4]))
test_friedman
```
El p-value es >0.05 por lo que no podemos concluir que haya al menos un par de algoritmos de calidad diferente.

--------------------------------------------------------------------------------

### Análisis post-hoc con Holm

El resultado del test de Friedman ya nos indica que es post-hoc es inútil. Los resultados que se obtengan no nos van a asegurar la diferencia en la calidad de los algoritmos.

Aún así, por completitud en la memoria, aplicamos el test estadístico
```{r}
# Aplicación del test post-hoc de HOLM
tam <- dim(results[,2:4])
groups <- rep(1:tam[2], each=tam[1])

cat("1 = KNN, 2 = LDA, 3 = QDA")
pairwise.wilcox.test(as.matrix(results[,2:4]), groups, p.adjust = "holm", paired = TRUE)
```

Vemos que los p-value son lo más altos posibles, carece de sentido intentar diferenciar los algoritmos.
Aunque podemos notar, tal y como habíamos visto en los test de Wilcoxon, que la diferencia KNN-QDA probablemente sea mayor que el resto de parejas.