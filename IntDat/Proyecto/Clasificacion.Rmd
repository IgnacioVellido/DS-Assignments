---
title: "Clasificacion"
author: "Ignacio Vellido"
date: "11/24/2020"
output: 
  pdf_document:
    keep_tex: yes
    df_print: kable
---

```{r setup, include=FALSE}
# Para PDF output
# pdf_document:
#     keep_tex: yes
#     df_print: kable

knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      include = FALSE, # para PDF
                      comment=NA, messages=FALSE)
library(tidyverse)
library(ggplot2)
library(cowplot)  # plot_grid
library(corrplot) # corr y corrplot
library(reshape2) # melt
library(dlookr) # normality
library(caret)  # preprocess
library(moments)  # skewness
library(car)  # scatterplotMatrix
library(MASS) # fit
library(scatterplot3d)
# library(stats)  # barlett
# require(vcd)
# library(ISLR)
# library(HSAUR2)
# library(vcdExtra)
# library(cepp)
# library("dslabs")

set.seed(111)
```


Cargamos los datos
```{r}
names <- c("Age", "Year", "Positive", "Survival")

haberman <- read_csv("Data/haberman/haberman.dat", comment = "@", col_names = names)
haberman$Survival <- haberman$Survival %>% factor(levels = c("negative", "positive"), labels = c("No", "Yes"))
```

<!-- Leer CV data -->
```{r echo=F}
# names <- c("Age", "Year", "Positive", "Survival")
# 
# train1 <- read_csv("Data/haberman/haberman-10-1tra.dat", comment = "@", col_names = names)
# train2 <- read_csv("Data/haberman/haberman-10-2tra.dat", comment = "@", col_names = names)
# train3 <- read_csv("Data/haberman/haberman-10-3tra.dat", comment = "@", col_names = names)
# train4 <- read_csv("Data/haberman/haberman-10-4tra.dat", comment = "@", col_names = names)
# train5 <- read_csv("Data/haberman/haberman-10-5tra.dat", comment = "@", col_names = names)
# train6 <- read_csv("Data/haberman/haberman-10-6tra.dat", comment = "@", col_names = names)
# train7 <- read_csv("Data/haberman/haberman-10-7tra.dat", comment = "@", col_names = names)
# train8 <- read_csv("Data/haberman/haberman-10-8tra.dat", comment = "@", col_names = names)
# train9 <- read_csv("Data/haberman/haberman-10-9tra.dat", comment = "@", col_names = names)
# train10 <- read_csv("Data/haberman/haberman-10-10tra.dat", comment = "@", col_names = names)
# 
# test1 <- read_csv("Data/haberman/haberman-10-1tst.dat", comment = "@", col_names = names)
# test2 <- read_csv("Data/haberman/haberman-10-2tst.dat", comment = "@", col_names = names)
# test3 <- read_csv("Data/haberman/haberman-10-3tst.dat", comment = "@", col_names = names)
# test4 <- read_csv("Data/haberman/haberman-10-4tst.dat", comment = "@", col_names = names)
# test5 <- read_csv("Data/haberman/haberman-10-5tst.dat", comment = "@", col_names = names)
# test6 <- read_csv("Data/haberman/haberman-10-6tst.dat", comment = "@", col_names = names)
# test7 <- read_csv("Data/haberman/haberman-10-7tst.dat", comment = "@", col_names = names)
# test8 <- read_csv("Data/haberman/haberman-10-8tst.dat", comment = "@", col_names = names)
# test9 <- read_csv("Data/haberman/haberman-10-9tst.dat", comment = "@", col_names = names)
# test10 <- read_csv("Data/haberman/haberman-10-10tst.dat", comment = "@", col_names = names)
```


<!-- Guardamos los indices -->
```{r}
# Add the ranking column in ddc
# haberman <- haberman %>%
#   group_by_all() %>%
#   mutate(rank_row = row_number()) %>% 
#   ungroup()
# 
# # Add a dummy ranking column in dcc, which is always = 1
# train1 <- train1 %>% mutate(rank_row = 1)
# test1 <- test1 %>%  mutate(rank_row = 1)
# 
# a_ = do.call("paste", haberman[,1:5])
# b_ = do.call("paste", train1[,1:5])
# 
# tra1 <- which(a_ %in% b_)
# b_ = do.call("paste", test1[,1:5])
# tst1 <- which(a_ %in% b_)
# 
# # a_
# 
# tra1
# tst1

# and now the anti_join by = c("x1", "x2", "x3", "x4", "x5", "rank_row") or by = names(ddc)
# compare(haberman, train1, by = names(haberman))


# train$Survival <- train$Survival %>% factor(levels = c("negative", "positive"), labels = c("No", "Yes"))
# 
# train1
# haberman
```


<!-- Creamos control para caret -->
```{r}
# ctrl 
```


Los preprocesamos (estandarización)
```{r}
haberman_transform <- preProcess(haberman, method=c("scale", "center"))
haberman_norm <- predict(haberman_transform, haberman)
```


Creamos holdout del 90%
```{r}
# Create training and test data (holdout 90%-10%)
shuffle_ds <- sample(dim(haberman_norm)[1])

pct90 <- (dim(haberman_norm)[1] * 90) %/% 100

train <- haberman_norm[shuffle_ds[1:pct90], -4] %>% as.data.frame()
test <- haberman_norm[shuffle_ds[(pct90+1):dim(haberman_norm)[1]], -4] %>% as.data.frame()
```


Separamos los datos de las etiquetas
```{r}
train_labels <- haberman[shuffle_ds[1:pct90], 4] %>% unlist()
test_labels <- haberman[shuffle_ds[(pct90+1):dim(haberman)[1]], 4] %>% unlist()
```

--------------------------------------------------------------------------------

## Utilizar el algoritmo k-NN probando con diferentes valores de k. Elegir el que considere más adecuado para su conjunto de datos. Analice qué ocurre en los valores de precisión en training y test con los diferentes valores de k.

Recordamos los gráficos 1-1 con las clasificaciones, vistos en el EDA.
```{r}
haberman %>%
  ggplot(aes(x=Age, y=Year, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()

haberman %>%
  ggplot(aes(y=Age, x=Positive, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()

haberman %>%
  ggplot(aes(x=Year, y=Positive, color=Survival)) +
    geom_point() +
    labs(title="Plot Age-Year con su Survival") +
    theme_light()
```

```{r}
colors <- c("coral2", "deepskyblue")
colors <- colors[haberman$Survival]

scatterplot3d(haberman[,1:3], angle = 290,
              main = "3D plot",
              pch=20, color = colors)

scatterplot3d(haberman[,1:3], angle = 50,
              main = "3D plot",
              pch=20, color = colors)
```

De cara a un algoritmo KNN, apreciamos los datos muy entremezclados, con mayor tendencia a agruparse los no supervivientes que los que sí, pero nada que nos llame la atención.

Debido a esto vamos a empezar con un valor de K relativamente bajo y vamos a ir aumentándolo poco a poco.
Tenemos que tener cuidado con el overfitting, eso sí.

```{r}
knnModel <- train(train, y = train_labels,
                  method = "knn", 
                  trControl = trainControl(method = "cv"), 
                  tuneGrid = data.frame(.k=3:15))

knnModel
```

Recordamos que los datos ya estaban previamente preprocesados (estandarizados, concretamente)

```{r}
knnPred <- predict(knnModel, newdata = test)
ctable <- table(knnPred, test_labels)

ctable
postResample(pred = knnPred, obs = test_labels)
```

```{r}
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix KNN")
```
Vemos que al estar los datos tan entremezclados ni siquiera con un K pequeño sobreaprende, es ya con un K medianamente alto (= 13) donde obtiene mayor accuracy en train.

Probablemente esto se deba a la gran mezcla de los datos, de forma que necesite la "opinión" de un gran número de vecinos para poder predecir con mayor confianza el nuevo valor.

```{r}
ggplot(train, aes(x = Age, y = Year, color=predict(knnModel, newdata = train))) +
  geom_point(show.legend = F) +
  labs(title="Prediction KNN") +
  theme_light() -> knnplot

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point() +
  labs(title="True values") +
  theme_light() -> trueplot

# Get legend
legend <- get_legend(trueplot)

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point(show.legend = F) +
  labs(title="True values") +
  theme_light() -> trueplot

plot_grid(plotlist=list(knnplot,trueplot,legend), ncol=2, label_size = 1)
```

Vemos que el uso de un K alto hace que perdamos puntos de Yes. Probablemente al ser minoría el error cometido clasificándolos como No es menor y por eso obtiene mejor accuracy.

En principio (a falta de evaluar con CV) creemos que en comparación con otros algoritmos el hecho de que el dataset este desbalanceado y los datos muy entremezclados la calidad real (fuera del entrenamiento) no sea muy buena.
Podría ser que en la población se mantega este desbalanceo en los datos y funcione bien, pero en caso de que no lo sea, el valor de K tan alto haría que probablemente se tienda a devolver una predicción de No.

Como habíamos comentado al principio, para el problema que nos atañe quizás esto podría ser incluso un hecho positivo, ya que los falsos positivos sería algo que querríamos evitar a toda costa.

--------------------------------------------------------------------------------

## Utilizar el algoritmo LDA para clasificar. No olvide comprobar las asunciones.

Comprobamos asunciones:

1- Distribución aleatoria: No nos queda más remedio que creer que sí.

2- Cada predictor sigue una distribución normal: Ya vimos en el EDA que esto no era cierto. El test de Shapiro nos demostraba que no y los QQ-plots nos lo hacían ver claramente.
Técnicamente sabiendo esto no deberíamos usar LDA, pero puesto que esto es un proyecto seguimos igualmente.

Aún así, las variables Age y Year no parecen seguir una distribución demasiado "rara" (en comparación con una normal), por lo que es posible que obtengamos resultados decentes.

3- Las clases siguen la misma matriz de covarianza:
```{r}
yes <- train %>% bind_cols(train_labels) %>% setNames(names) %>% filter(Survival == "Yes") %>% dplyr::select(-Survival)
no  <- train %>% bind_cols(train_labels) %>% setNames(names) %>% filter(Survival == "No") %>% dplyr::select(-Survival)

# Solo nos interesa la diagonal
cat("Para clase Yes:\n")
cov(yes) %>% diag()
cat("Para clase No:\n")
cov(no) %>% diag()
```

Todas los valores son positivos, por tanto entre clases se mantienen la tendencia de los datos.

Puesto que nuestras variables no siguen una distribución normal, no podemos hacer el test de homogeneidad de Barlett.
Utilizamos por tanto el de Levene
```{r}
cat("Age:\n")
leveneTest(Age ~ Survival, haberman)
cat("Year:\n")
leveneTest(Year ~ Survival, haberman)
cat("Positive:\n")
leveneTest(Positive ~ Survival, haberman)
```

Indicándonos que solo se puede asegurar que la variable Positive tiene homogeneidad entre clases diferentes.

Podemos verlo más claro gráficamente
```{r}
ggplot(haberman, aes(x = Year, color = Survival, fill=Survival)) + 
  geom_boxplot(color="black") + 
  labs(title = "Varianza entre clases para Year") +
  theme_light()

ggplot(haberman, aes(x = Age, color = Survival, fill=Survival)) + 
  geom_boxplot(color="black") + 
  labs(title = "Varianza entre clases para Age") +
  theme_light()

ggplot(haberman, aes(x = Positive, color = Survival, fill=Survival)) + 
  geom_boxplot(color="black") + 
  labs(title = "Varianza entre clases para Positive") +
  theme_light()
```


Podemos mostrar las varianzas en grafors de elipses:
```{r}
ggplot(haberman, aes(x = Year, y = Age, color = Survival)) + 
  geom_point() + 
  stat_ellipse() +
  theme_light()

ggplot(haberman, aes(x = Year, y = Positive, color = Survival)) + 
  geom_point() + 
  stat_ellipse() +
  theme_light()

ggplot(haberman, aes(x = Positive, y = Age, color = Survival)) + 
  geom_point() + 
  stat_ellipse() +
  theme_light()
```

Se nota que la causa de que no se rechace el test para esta variable es la gran cantidad de datos con Positive=0

<!-- http://thatdatatho.com/2018/02/19/assumption-checking-lda-vs-qda-r-tutorial-2/ -->

Por tanto para LDA no podemos hacer uso de la variable Positive, pero sí de las otras dos.


Aunque solo es recomendable, y no son cualidades necesarias para obtener solución en LDA:
- Tenemos más instancias que predictores, por varios órdenes de magnitud.
- Los predictores son independientes.
- No tenemos varianza cercana a cero

### Aplicando LDA

```{r}
ldaModel <- train(train %>% dplyr::select(-Positive), y = train_labels,
                method = "lda",
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

ldaModel$finalModel
confusionMatrix(ldaModel)
```
Predecimos en test
```{r}
ldaPred <- predict(ldaModel, newdata = test)
ctable <- table(ldaPred, test_labels)

ctable
postResample(pred = ldaPred, obs = test_labels)
```

```{r}
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix LDA")
```


Recordamos que teníamos muchos datos de no supervivencia antes que sí
```{r}
table(haberman$Survival)
```

En training
```{r}
ggplot(train, aes(x = Age, y = Year, color=predict(ldaModel, newdata = train))) +
  geom_point(show.legend = F) +
  labs(title="Prediction LDA") +
  theme_light() -> ldaplot

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point() +
  labs(title="True values") +
  theme_light() -> trueplot

# Get legend
legend <- get_legend(trueplot)

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point(show.legend = F) +
  labs(title="True values") +
  theme_light() -> trueplot

plot_grid(plotlist=list(ldaplot,trueplot,legend), ncol=2, label_size = 1)
```


Hacemos un plot del ajuste
```{r}
data <- train %>% bind_cols(train_labels) %>% setNames(names) 
ldafit <- lda(Survival~Age+Year, data = data)
plot(ldafit)
ldafit$post
```


```{r}
x <- seq(min(train$Age), max(train$Age), length.out=dim(train)[1])
y <- seq(min(train$Year), max(train$Year), length.out=dim(train)[1])

Xcon <- matrix(c(rep(x,length(y)), rep(y, rep(length(x), length(y)))),ncol=2) #Set all possible pairs of x and y on a grid

#posterior probabilities of a point belonging to each class
out <- predict(ldafit, data.frame(Age=Xcon[,1], Year=Xcon[,2]))$post

pr <- data.frame(x=rep(x, length(y)), 
                 y=rep(y, each=length(x)), 
                 z=as.vector(out))

ggplot(data, aes(x=Age, y=Year)) +
  geom_point(size = 2, aes(pch=Survival,  col=Survival)) +
  geom_contour(data = pr, aes(x=x, y=y, z=z)) +
  theme_light()
```

Tenemos un dataset bastante desbalanceado, y LDA no predice para la clase Yes. Con esto se asegura un alto accuracy en nuestro entrenamiento, pero no asegura de que para datos externos vaya a ser así.
Pese a ello, no nos queda más remedio que suponer que nuestros datos vienen de la misma muestra aleatoria y por tanto son releventes para la clasificación.

--------------------------------------------------------------------------------

## Utilizar el algoritmo QDA para clasificar. No olvide comprobar las asunciones.

QDA tiene las mismas asunciones de LDA salvo que relaja la norma de que las clases tengan igual covarianza.

Por tanto tenemos que:
- Distribución aleatoria
- Distribución normal

Técnicamente el no cumplir normalidad no imposibilita que se encuentre solución, pero ya no nos lo asegura.

Además tenemos que:
- El número de predictores debe ser menor que el número de instancias de cada clase.
Cosa que sabemos que sí por la tabla Yes/No
- Los predictores dentro de cada clase no deben estar correlacionados.

```{r}
corrplot.mixed(cor(yes), tl.pos="lt", upper="color", lower.col="black", title="Pearson")
corrplot.mixed(cor(yes, method="kendall"), tl.pos="lt", upper="color", lower.col="black", title="Kendall")
```


```{r}
corrplot.mixed(cor(no), tl.pos="lt", upper="color", lower.col="black", title="Pearson")
corrplot.mixed(cor(no, method="kendall"), tl.pos="lt", upper="color", lower.col="black", title="Kendall")
```

Cosa que tampoco se da.

--------------------------------------------------------------------------------

### Aplicando QDA

```{r}
qdaModel <- train(train %>% dplyr::select(-Positive), y = train_labels,
                method = "qda",
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

qdaModel$finalModel
confusionMatrix(qdaModel)
```

```{r}
qdaPred <- predict(qdaModel, newdata = test)
ctable <- table(qdaPred, test_labels)

ctable
postResample(pred = qdaPred, obs = test_labels)
```

```{r}
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix QDA")
```

En training
```{r}
ggplot(train, aes(x = Age, y = Year, color=predict(qdaModel, newdata = train))) +
  geom_point(show.legend = F) +
  labs(title="Prediction QDA") +
  theme_light() -> qdaplot

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point() +
  labs(title="True values") +
  theme_light() -> trueplot

# Get legend
legend <- get_legend(trueplot)

ggplot(train, aes(x = Age, y = Year, color=train_labels)) +
  geom_point(show.legend = F) +
  labs(title="True values") +
  theme_light() -> trueplot

plot_grid(plotlist=list(qdaplot,trueplot,legend), ncol=2, label_size = 1)
```

Hacemos un plot del ajuste
```{r}
data <- train %>% bind_cols(train_labels) %>% setNames(names) 
qdafit <- qda(Survival~Age+Year, data = data)

x <- seq(min(train$Age), max(train$Age), length.out=dim(train)[1])
y <- seq(min(train$Year), max(train$Year), length.out=dim(train)[1])

Xcon <- matrix(c(rep(x,length(y)), rep(y, rep(length(x), length(y)))),ncol=2) #Set all possible pairs of x and y on a grid

#posterior probabilities of a point belonging to each class
out <- predict(qdafit, data.frame(Age=Xcon[,1], Year=Xcon[,2]))$post

pr <- data.frame(x=rep(x, length(y)), 
                 y=rep(y, each=length(x)), 
                 z=as.vector(out))

ggplot(data, aes(x=Age, y=Year)) +
  geom_point(size = 2, aes(pch=Survival,  col=Survival)) +
  geom_contour(data = pr, aes(x=x, y=y, z=z)) +
  theme_light()
```

Obtenemos resultados extremadamente similares a LDA.

--------------------------------------------------------------------------------

## Comparar los resultados de los tres algoritmos

Podríamos leer los de training pero para comparar necesitamos los de test
```{r}
results <- read_csv("Data/clasif_test_alumos.csv")
# results <- results[-1]
results
```

### Comparativas 1-1 con Wilconxon

LDA vs QDA

```{r}
# TABLA NORMALIZADA - lda (other) vs qda (ref) para WILCOXON 
# + 0.1 porque wilcox R falla para valores == 0 en la tabla

difs <- (results[,3] - results[,4]) / results[,3]
difs <- difs %>% as.matrix()
res_norm <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(res_norm) <- c(colnames(results)[3], colnames(results)[4])
```


```{r}
# Aplicación del test de WILCOXON
wilcoxon <- wilcox.test(res_norm[,1], res_norm[,2], alternative = "two.sided", paired=TRUE)
Rmas <- wilcoxon$statistic

pvalue <- wilcoxon$p.value

wilcoxon <- wilcox.test(res_norm[,2], res_norm[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- wilcoxon$statistic

cat("LDA vs QDA: ")
wilcoxon
cat("R+: ")
Rmas
cat("R-: ")
Rmenos
cat("p-value: ")
pvalue
```

Obtenemos un ranking de 144 para LDA y 96 para QDA, con un p-valor de 0.75 (o nivel de confianza del 25%).

Esto nos dice que LDA obtiene mejores resultados pero puesto que el p-value es extremadamente grande no podemos afirmar con garantía estadística que las diferencias entre los tests sean notorias.
<!-- Podemos afirmar al 85% que sí lo son, pero para hacerlo con seguridad buscaríamos al menos un 95% de confianza. -->

--------------------------------------------------------------------------------

LDA vs KNN

```{r}
# TABLA NORMALIZADA +0.1 porque wilcox R falla para valores == 0 en la tabla
difs <- (results[,3] - results[,2]) / results[,3]
difs <- difs %>% as.matrix()
res_norm <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(res_norm) <- c(colnames(results)[3], colnames(results)[2])
```


```{r}
# Aplicación del test de WILCOXON
wilcoxon <- wilcox.test(res_norm[,1], res_norm[,2], alternative = "two.sided", paired=TRUE)
Rmas <- wilcoxon$statistic

pvalue <- wilcoxon$p.value

wilcoxon <- wilcox.test(res_norm[,2], res_norm[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- wilcoxon$statistic

cat("LDA vs KNN: ")
wilcoxon
cat("R+: ")
Rmas
cat("R-: ")
Rmenos
cat("p-value: ")
pvalue
```

Ahora obtenemos un ranking de 90 para LDA y 120 para QDA, con un p-valor de 0.59 (o nivel de confianza del 41%).

Seguimos teniendo un p-valor demasiado grande para poder asegurar con significación la diferencia.

--------------------------------------------------------------------------------

QDA vs KNN

```{r}
# TABLA NORMALIZADA +0.1 porque wilcox R falla para valores == 0 en la tabla
difs <- (results[,4] - results[,2]) / results[,3]
difs <- difs %>% as.matrix()
res_norm <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(res_norm) <- c(colnames(results)[4], colnames(results)[2])
```


```{r}
# Aplicación del test de WILCOXON
wilcoxon <- wilcox.test(res_norm[,1], res_norm[,2], alternative = "two.sided", paired=TRUE)
Rmas <- wilcoxon$statistic

pvalue <- wilcoxon$p.value

wilcoxon <- wilcox.test(res_norm[,2], res_norm[,1], alternative = "two.sided", paired=TRUE)
Rmenos <- wilcoxon$statistic

cat("QDA vs KNN: ")
wilcoxon
cat("R+: ")
Rmas
cat("R-: ")
Rmenos
cat("p-value: ")
pvalue
```

Por último tenemos un ranking de 69 para LDA y 141 para KNN, con un p-valor de 0.18 (o nivel de confianza del 82%).

Ya ahora podemos afirmar al 82% que los resultados de ambos algoritmos sí son significativamente diferentes, pero para hacerlo con bastante seguridad realmente buscaríamos al menos un 95% de confianza.


--------------------------------------------------------------------------------

### Comparativa múltiple con Friedman
```{r}
# Aplicación del test de Friedman
test_friedman <- friedman.test(as.matrix(results[,2:4]))
test_friedman
```
El p-value es >0.05 por lo que no podemos concluir que haya al menos un par de algoritmos de calidad diferente.

--------------------------------------------------------------------------------

### Análisis post-hoc con Holm

El resultado del test de Friedman ya nos indica que es post-hoc es inútil. Los resultados que se obtengan no nos van a asegurar la diferencia en la calidad de los algoritmos.

Aún así, por completitud en la memoria, aplicamos el test estadístico
```{r}
# Aplicación del test post-hoc de HOLM
tam <- dim(results[,2:4])
groups <- rep(1:tam[2], each=tam[1])

cat("1 = KNN, 2 = LDA, 3 = QDA")
pairwise.wilcox.test(as.matrix(results[,2:4]), groups, p.adjust = "holm", paired = TRUE)
```

Vemos que los p-value son lo más altos posibles, carece de sentido intentar diferenciar los algoritmos.
Aunque podemos notar, tal y como habíamos visto en los test de Wilcoxon, que la diferencia KNN-QDA probablemente sea mayor que el resto de parejas.