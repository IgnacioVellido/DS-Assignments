---
title: "Untitled"
author: "Ignacio Vellido"
date: "11/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(philentropy)
```


```{r}
wbcd <- read.csv("https://resources.oreilly.com/examples/9781784393908/raw/ac9fe41596dd42fc3877cfa8ed410dd346c43548/Machine%20Learning%20with%20R,%20Second%20Edition_Code/Chapter%2003/wisc_bc_data.csv")
```

```{r}
wbcd %>% str()
```


```{r}
wbcd <- na.omit(wbcd)
classes <- wbcd[, 2]
wbcd <- wbcd[, -(1:2)]
```


```{r}
# PREPROCESS ?

# From Github
# En caso de empate, devuelve una cualquiera
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# The function will output the predictions over the test set (if given) or using the train set also as test set.
my_knn <- function(train, train_labels, test=NA, k=1, metric="euclidean") {
  
  # Aplica knn a una fila
  knn <- function(test, train, train_labels, k=1, metric="euclidean") {
    
    # Calcula distancia entre dos filas
    dist <- function(v1, v2, m="euclidean") {
      bind_rows(data.frame(t(v1)) , v2) %>% 
        distance(method = m)
    }
    
    y <- train %>% apply(1, dist, v2=test, m=metric)
    
    # Ordenamos
    z <- y %>% sort() %>% head(k)
    
    # Cogemos los Ã­ndices
    z <- z %>% names() %>% as.integer()
    
    # Cogemos la clase mayoritaria (o desempatamos)
    train_labels[z] %>% Mode()
  }
  
  # x <- ifelse(is.na(test), train, test) %>% as.data.frame()

  # x %>%  print()
  # x %>% apply(1, knn, train, train_labels, k, metric)
}


# philentropy::avg(wbcd[1], wbcd[2], testNA = FALSE)
# philentropy::distance(wbcd[1,],)
# avg(wbcd[1],wbcd[2],F)

t <- wbcd[nrow(wbcd),]
train <- wbcd[-nrow(wbcd),]
metric <- "euclidean"

my_knn(train, classes, t, 5, "euclidean")
```

