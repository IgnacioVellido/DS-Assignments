---
title: "Ejercicios"
author: "Ignacio Vellido"
date: "11/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(philentropy)
library(ggplot2)
library(caret)
library(MASS)
library(ISLR)
library(dlookr)

set.seed(5)
```

# Exercise 1: KNN

Create a function my_knn that accepts any measure from the philentropy package and performs basic knn. A possible function interface could be:

my_knn <- function(train, train_labels, test=NA, k=1, metric=“euclidean”)

The function will output the predictions over the test set (if given) or using the train set also as test set.

Select two distance/similarity measures and apply the my_knn function to each of them with different k choices for the breast cancer data and do a comparison of the results (try using a plot).

```{r}
wbcd <- read.csv("https://resources.oreilly.com/examples/9781784393908/raw/ac9fe41596dd42fc3877cfa8ed410dd346c43548/Machine%20Learning%20with%20R,%20Second%20Edition_Code/Chapter%2003/wisc_bc_data.csv")

wbcd %>% str()
```


```{r}
wbcd <- na.omit(wbcd)
classes <- wbcd[, 2]
wbcd <- wbcd[, -(1:2)] %>% scale()
```


```{r}
philentropy::getDistMethods()
```

NOTA: Los datos se deben estandarizar antes
NOTA: La función de philentropy no permite suprimir los mensajes
NOTA: El algoritmo es exageradamente lento, no usarlo con muchos datos de test
```{r include=FALSE}
# From Github
# En caso de empate, devuelve una cualquiera
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# The function will output the predictions over the test set (if given) or using the train set also as test set.
my_knn <- function(train, train_labels, test=NA, k=1, metric="euclidean") {
  
  # Aplica knn a una fila
  knn <- function(test, train, train_labels, k=1, metric="euclidean") {
    
    # Calcula distancia entre dos filas
    dist <- function(v1, v2, m="euclidean") {
      bind_rows(data.frame(t(v1)) , v2) %>% 
        distance(method = m)
    }
    
    y <- train %>% apply(1, dist, v2=test, m=metric)
    
    # Ordenamos
    z <- y %>% sort() %>% head(k)
    
    # Cogemos los índices
    z <- z %>% names() %>% as.integer()
    
    # Cogemos la clase mayoritaria (o desempatamos)
    train_labels[z] %>% Mode()
  }
  
  x <- if (is.na(test)) train else test

  x %>% apply(1, knn, train, train_labels, k, metric)
}

train <- wbcd[-c(1,22,112,121,75),]
test <- wbcd[c(1,22,112,121,75),]

c1 <- my_knn(train, classes, test, 3, "euclidean")
c2 <- my_knn(train, classes, test, 3, "motyka")
c3 <- my_knn(train, classes, test, 3, "pearson")
```

```{r}
c1
c2
c3
```

```{r}
test %>% as.data.frame() %>% 
  ggplot(aes(x=perimeter_mean, y=texture_mean, color=c1)) +
    geom_point(size=4) +
    labs(title="Euclidean") +
    theme_light()

test %>% as.data.frame() %>% 
  ggplot(aes(x=perimeter_mean, y=texture_mean, color=c2)) +
    geom_point(size=4) +
    labs(title="Avg") +
    theme_light()

test %>% as.data.frame() %>% 
  ggplot(aes(x=perimeter_mean, y=texture_mean, color=c3)) +
    geom_point(size=4) +
    labs(title="Pearson") +
    theme_light()
```

```{r}
test %>% as.data.frame() %>% 
  ggplot(aes(x=area_mean, y=radius_mean, color=c1)) +
    geom_point(size=4) +
    labs(title="Euclidean") +
    theme_light()

test %>% as.data.frame() %>% 
  ggplot(aes(x=area_mean, y=radius_mean, color=c2)) +
    geom_point(size=4) +
    labs(title="Avg") +
    theme_light()

test %>% as.data.frame() %>% 
  ggplot(aes(x=area_mean, y=radius_mean, color=c3)) +
    geom_point(size=4) +
    labs(title="Pearson") +
    theme_light()
```

--------------------------------------------------------------------------------

# Exercise 2: Logistic regression

Using the breast cancer dataset (all data, not only training) perform 10 fold-cv with logistic regression.

```{r}
wbcd <- read.csv("https://resources.oreilly.com/examples/9781784393908/raw/ac9fe41596dd42fc3877cfa8ed410dd346c43548/Machine%20Learning%20with%20R,%20Second%20Edition_Code/Chapter%2003/wisc_bc_data.csv")

wbcd <- na.omit(wbcd)
classes <- wbcd[, 2]

wbcd %>% head()
```

```{r}
train <- wbcd[-c(1,22,112,121,75),]
classes <- classes[-c(1,22,112,121,75)]
test <- wbcd[c(1,22,112,121,75),]

test %>% head()
```

```{r}
glmFit <- train(train[,-2], y = train[,2], method = "glm", preProcess = c("center", "scale"),
                tuneLength = 10, control=glm.control(maxit=500), trControl = trainControl(method = "cv"))
glmFit
```

```{r}
glm.probs <- predict(glmFit, newdata=test[,-2], type="prob")
glm.pred <- ifelse(glm.probs > 0.5,"B","M")

print("Probabilities: ")
glm.probs %>% as.matrix()
```

--------------------------------------------------------------------------------

# Exercise 3: LDA/QDA

## 1
* Try lda with all Lag variables.

```{r}
Smarket %>% str()
```

Hacemos test de normalidad con Shapiro-test:
```{r}
normality(Smarket) %>% filter(p_value < 0.05)
```

```{r}
colors <- c("chocolate", "deepskyblue1", "plum1", "hotpink4", "orange", "springgreen4")
names <- names(Smarket)
bins <- c(10,10,15,15,14,18)
plt <- list(length = length(names))

x<-rnorm(100, mean=0, sd=1)

for (i in 1:length(names)) {
  ggplot(Smarket, aes_string(sample=names[i])) + 
    stat_qq(alpha=.3, fill=colors[i], size=1) +
    stat_qq_line() +
    labs(title="", x="", y="") +
    theme_light() -> plt[[i]]
  
  print(plt[[i]] + labs(title=sprintf("%s", names[i]), x=""))
}
```


No se cumplen los requisitos de LDA para las variables Lag. 
Lo hacemos de todas maneras:
```{r}
train <- Smarket %>% dplyr::select(Lag1, Lag2, Lag3, Lag4, Lag5)
classes <- Smarket %>% dplyr::select(Direction) %>% unlist()
```


```{r}
ldaFit <- train(train, classes,
                method = "lda",
                preProcess = c("center", "scale"),
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

ldaFit$finalModel

confusionMatrix(ldaFit)
```
Obtenemos resultados muy malos.

```{r}
ctable <- confusionMatrix(ldaFit)$table
fourfoldplot(ctable, color = c("#99CC99", "#CC6666"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix LDA")
```

* Make a quick comparison between logistic regression and lda.

```{r}
glmFit <- train(train, classes,
                method = "glm",
                preProcess = c("center", "scale"),
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

glmFit$finalModel
confusionMatrix(glmFit)
```


```{r}
ctable <- confusionMatrix(glmFit)$table
fourfoldplot(ctable, color = c("#CC6666","#99CC99"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix GLM")
```

* Try with qda and compare all three methods. Plot the results.

```{r}
qdaFit <- train(train, classes,
                method = "qda",
                preProcess = c("center", "scale"),
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

qdaFit$finalModel

confusionMatrix(qdaFit)
```

```{r}
ctable <- confusionMatrix(qdaFit)$table
fourfoldplot(ctable, color = c("#99CC99", "#CC6666"), std = "all.max",
             conf.level = 0, margin = 1, main = "Confusion Matrix QDA")
```

```{r}
lda.fit <- lda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket)
plot(lda.fit, type="both")

qda.fit <- qda(Direction~Lag1+Lag2+Lag3+Lag4+Lag5, data=Smarket)
plot(qda.fit, type="both")
```

--------------------------------------------------------------------------------

## 2
Using only the information in file clasif_train_alumnos.csv:

```{r}
results <- read_csv("E:/Nacho/Desktop/GitHub/DATCOM/IntDat/Clasificacion/clasif_train_alumnos.csv")
results
```

```{r}
# TABLA NORMALIZADA - lda (other) vs qda (ref) para WILCOXON
# + 0.1 porque wilcox R falla para valores == 0 en la tabla

difs <- (results[,3] - results[,4]) / results[,3]
difs <- difs %>% as.matrix()
res_norm <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(res_norm) <- c(colnames(results)[3], colnames(results)[4])
```


* Compare lda and qda using Wilcoxon.
```{r}
# Aplicación del test de WILCOXON
wilcoxon <- wilcox.test(res_norm[,1], res_norm[,2], alternative = "two.sided", paired=TRUE)
# wilcoxon <- wilcox.test(results[,3] %>% unlist(), results[,4] %>% unlist(),
#                         alternative = "two.sided", paired=TRUE)
Rmas <- wilcoxon$statistic

pvalue <- wilcoxon$p.value

wilcoxon <- wilcox.test(res_norm[,2], res_norm[,1], alternative = "two.sided", paired=TRUE)
# wilcoxon <- wilcox.test(results[,4] %>% unlist(), results[,3] %>% unlist(),
#                         alternative = "two.sided", paired=TRUE)
Rmenos <- wilcoxon$statistic

wilcoxon
cat("R+: ")
Rmas
cat("R-: ")
Rmenos
cat("p-value: ")
pvalue
```

Obtenemos un ranking de 144 para LDA y 66 para QDA, con un p-valor de 0.15 (o nivel de confianza del 85%).

Esto nos dice que LDA obtiene mejores resultados pero puesto que el p-value no es lo suficientemente grande no podemos afirmar con un nivel alto de significación que las diferencias entre los tests sean notorias.
Podemos afirmar al 85% que sí lo son, pero para afirmarlo con exactitud buscaríamos al menos un 95% de confianza.


* Perform a multiple comparison using Friedman.
```{r}
# Aplicación del test de Friedman
test_friedman <- friedman.test(as.matrix(results[,2:4]))
test_friedman
```
El p-value es >0.05 por lo que no podemos concluir que haya al menos un par de algoritmos de calidad diferente.

* Using Holm see if there is a winning algorithm (even if Friedman says there is no chance…).
```{r}
# Aplicación del test post-hoc de HOLM
tam <- dim(results[,2:4])
groups <- rep(1:tam[2], each=tam[1])

cat("1 = KNN, 2 = LDA, 3 = QDA")
pairwise.wilcox.test(as.matrix(results[,2:4]), groups, p.adjust = "holm", paired = TRUE)
```

Todos los p-values son mayores que 0.05 por lo que tal y como nos había dicho el test de Friedman no podemos asegurar para ningún par de test que sean de calidad diferente.

Notamos que el p-value de LDA vs QDA (3-2) es menor que el devuelto por el test de Wilcoxon. Esto probablemente se deba a que el test de Wilcoxon es más potente que Holm.