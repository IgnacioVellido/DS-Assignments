\section{Introducción}

\subsection{Conjunto de datos}

% The data has been produced using Monte Carlo simulations. The first 8 features are kinematic properties measured by the particle detectors in the accelerator. The last ten features are functions of the first 8 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks and the dropout algorithm are presented in the original paper. The last 500,000 examples are used as a test set.n about your data set

Contamos con un subconjunto de SUSY Data Set (\url{https://archive.ics.uci.edu/ml/datasets/SUSY}), un problema de clasificación binaria donde existe un ratio de desbalanceo de 10-90. La tarea consiste en distinguir la señal que produce una partícula supersimétrica frente a la señal de fondo que se puede captar.

En dataset cuenta con dos millones de datos (1.000.000 para entrenamiento, 1.000.000 para evaluación) con 18 características numéricas reales, y las siguientes medidas estadísticas:

\begin{figure}[ht]
    \centerfloat
    \includegraphics[width=0.9\textwidth]{img/statistics.png}
    \caption{Medidas estadísticas del conjunto de datos.}
\end{figure}

En base a la descripción en la página de la UCI, las primeras 8 características reflejan propiedades de las partículas medidas en un acelerador, y las 10 siguientes indican el resultado de diferentes funciones a partir de estas variables. Estas variables derivadas no aportan información nueva pero indica que puede resultar de ayuda en la clasificación de la fila.

\subsection{Técnicas aplicadas}

Las diferentes técnicas aplicadas en esta práctica son:
\begin{itemize}
    \item \textbf{De aprendizaje}: \begin{itemize}
        \item Árboles de decisión (MLlib.tree.DecisionTree).
        \item Random Forest (MLlib.tree.RandomForest).
        \item kNN-IS (MLlib.clasification.kNN_IS).
        \item PCARD (MLlib.tree.PCARD).
    \end{itemize}
    \item \textbf{De preprocesamiento}: Reducción al 50\% (9 características)\begin{itemize}
        \item \textbf{Selección de características}: \begin{itemize}
            \item Principal Component Analysis.
            \item Chi Square.
        \end{itemize}
        \item \textbf{Ajuste de desbalanceo}: \begin{itemize}
            \item Random Oversampling.
            \item Random Undersampling.
        \end{itemize}
        \item \textbf{Filtrado de ruido}: \begin{itemize}
            \item Homogeneous Ensemble (HME).
            \item NCNEdit.
        \end{itemize}
        \item \textbf{Selección de instancias}: \begin{itemize}
            \item FCNN.
            \item SSMA-SFLSDE.
        \end{itemize}
    \end{itemize}
\end{itemize}


Se ha intentado probar todas las combinaciones de técnicas posibles, dentro de los límites de coherencia en el uso conjunto de algunas de ellas. En este caso, puesto que en el algoritmo PCARD aplica a los datos de entrada PCA, no se ha aplicado ninguna técnica de reducción de características en sus experimentos.

El flujo de técnicas de preprocesamiento de acorde a su uso es el siguiente:
\begin{equation}
    Selección de características -> Under/Over-sampling -> Filtrado de ruido -> Selección de instancias
\end{equation}

La justificación es la siguiente: En base al conjunto de datos de entrenamiento con el que contamos, pretendemos reducir la dimensionalidad sin perder excesiva información. Puesto que el dataset cuenta con un ratio de desbalanceo del 90\%, RO y RU ajustan los datos para evitar un sesgo en las técnicas de clasificación. El uso de estas técnicas puede generar ruido adicional además del propio ruido inherente que debemos suponer que existe en nuestros datos, por lo que aplicamos técnicas de filtrado para eliminarlo. Finalmente, para agilizar el proceso de clasificación, seleccionamos el conjunto de instancias con mayor varianza más representativo del conjunto.


% \subsubsection{Árboles de decisión}

% \subsubsection{Random Forest}

% \subsubsection{kNN-IS}

% An Iterative Spark-based design of the k-Nearest Neighbors
% classifier for big data

% \subsubsection{PCARD}

% This method implements the PCARD ensemble algorithm. PCARD ensemble method is a distributed upgrade of the method presented by A. Ahmad. The algorithm performs Random Discretization and Principal Components Analysis to the input data, then joins the results and trains a decision tree on it.

