---
title: "Imbalanced Classification Tasks"
author: "Ignacio Vellido Expósito"
date: "18/01/2021"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
# Configuración RMarkdown
knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
library(tidyverse)
library(ggplot2)
library(caret)
library(imbalance) #to be used in the optional part
library(pROC)
library(reshape2)
library(ggvis)
library(tsne)
```

# Activity 1

## Loading the dataset

```{r}
# load the CSV file from the local or web directory
dataset <- read.csv("subclus.csv", header = FALSE, stringsAsFactors = T)

# set the column names in the dataset (you must know them a priori)
colnames(dataset) <- c("Att1", "Att2", "Class")

dataset$Class <- relevel(dataset$Class,"positive") #to ensure it appears at the first class

dataset
```


## Summarizing and visualizing

Dimensions (600 rows, 2 attributes, 1 class)
```{r}
dim(dataset)
```
Types (numeric integer attributes)
```{r}
str(dataset)
```

First rows
```{r}
head(dataset)
```

Binary class
```{r}
levels(dataset$Class)
```

Statistical summary
```{r}
summary(dataset)

cat("\nImbalance ratio: ")
imbalanceRatio(dataset)
```

Split input and output
```{r}
x <- dataset[,1:2]
y <- dataset[,3]
```

Pie chart
```{r}
n_classes <- c(sum(y=="positive"),sum(y=="negative"))
pct <- round(n_classes/sum(n_classes)*100,digits=2)

# lbls <- levels(dataset$Class)
lbls <- c("positive", "negative")
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels

pie(n_classes,labels = lbls, main="Class distribution")
```

Univariate plots
```{r}
dataset %>%
  pivot_longer(c('Att1','Att2')) %>% 
  ggplot(aes(y=value)) +
    geom_boxplot() +
    facet_wrap(~ name, scales = "free") +
    theme_light()
```

Multivariate plots
```{r}
# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")
```

```{r}
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
```

```{r}
# Dataset scatter plot
dataset %>% ggvis(~Att1, ~Att2, fill = ~Class) %>% layer_points()
```

Vemos fronteras difusas y clases superpuestas. 
Imposible de encontrar separación lineal.
Modelos basados en vecinos cercanos tendrán problemas si no limpiamos previamente las fronteras. Una vez hecho, podrían funcionar bastante bien.

## Execution and evaluation of models

Create test set with same imbalance ratio
```{r}
set.seed(42) #To ensure the same output

#An easy way to create split "data partitions":
trainIndex <- createDataPartition(dataset$Class, p = .75, 
                                  list = FALSE, 
                                  times = 1)
trainData <- dataset[ trainIndex,]
testData  <- dataset[-trainIndex,]

#Check IR to ensure a stratified partition
imbalanceRatio(trainData)
imbalanceRatio(testData)
```

Define functions for learning and predict
```{r}
# a) Learning function
learn_model <-function(dataset, ctrl,message){
  model.fit <- train(Class ~ ., data = dataset, method = "knn", 
                   trControl = ctrl, preProcess = c("center","scale"), metric="ROC", 
                   tuneGrid = expand.grid(k = c(1,3,5,7,9,11)))
  model.pred <- predict(model.fit,newdata = dataset)
  #Get the confusion matrix to see accuracy value and other parameter values
  model.cm <- confusionMatrix(model.pred, dataset$Class,positive = "positive")
  model.probs <- predict(model.fit,newdata = dataset, type="prob")
  model.roc <- roc(dataset$Class,model.probs[,"positive"],color="green")
  return(model.fit)
}

# b) Estimation function
test_model <-function(dataset, model.fit,message){
  model.pred <- predict(model.fit,newdata = dataset)
  #Get the confusion matrix to see accuracy value and other parameter values
  model.cm <- confusionMatrix(model.pred, dataset$Class,positive = "positive")
  print(model.cm)
  model.probs <- predict(model.fit,newdata = dataset, type="prob")
  model.roc <- roc(dataset$Class,model.probs[,"positive"])
  #print(knn.roc)
  plot(model.roc, type="S", print.thres= 0.5,main=c("ROC Test",message),col="blue")
  #print(paste0("AUC Test ",message,auc(model.roc)))
  return(model.cm)
}
```

Train with raw data
```{r}
#Execute model ("raw" data)
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary)
model.raw <- learn_model(trainData,ctrl,"RAW ")

print(model.raw)
```

```{r}
#We may decide to plot the results from the grid search of the model's parameters
plot(model.raw,main="Grid Search RAW")

cm.raw <- test_model(testData,model.raw,"RAW ")
```

Train with Random Undersampling
```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary,sampling = "down")

model.us <- learn_model(trainData,ctrl,"US ")
cm.us <- test_model(testData,model.us,"US ")
```

Train with Random Oversamping
```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary,sampling = "up")
model.os <- learn_model(trainData,ctrl,"OS ")
cm.os <- test_model(testData,model.os,"OS ")
```

Train with SMOTE
```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary,sampling = "smote")
model.smt <- learn_model(trainData,ctrl,"SMT ")
cm.smt <- test_model(testData,model.smt,"SMT ")
```

## Comparison among models

Summarize models results
```{r}
# summarize accuracy of models
models <- list(raw = model.raw, us = model.us, 
               os = model.os, smt = model.smt)
results <- resamples(models)
summary(results)
```

Plot comparisons between models
```{r}
bwplot(results)
```

Similar values in ROC, but bigger differences in sensitivity and specificity.


Final plot with additional metrics
```{r}
#Carry out a comparison over all imbalanced metrics
comparison <- data.frame(model = names(models),
                         Sensitivity = rep(NA, length(models)),
                         Specificity = rep(NA, length(models)),
                         Precision = rep(NA, length(models)),
                         Recall = rep(NA, length(models)),
                         F1 = rep(NA, length(models)))

for (name in names(models)) {
  cm_model <- get(paste0("cm.", name))
  
  comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
    mutate(Sensitivity = cm_model$byClass["Sensitivity"],
           Specificity = cm_model$byClass["Specificity"],
           Precision = cm_model$byClass["Precision"],
           Recall = cm_model$byClass["Recall"],
           F1 = cm_model$byClass["F1"])
}

comparison %>%
  gather(x, y, Sensitivity:F1) %>%
  ggplot(aes(x = x, y = y, color = model)) +
  geom_jitter(width = 0.2, alpha = 0.8, size = 3)
```

## Conclusions

Contamos con un dataset formado por dos atributos numéricos enteros, sobre un total de 600 instancias. Existe un desbalanceo alto (80%) sobre la clase minoritaria.

Una visualización nos muestra que los datos de la clase positiva se encuentran agrupados en una sola zona central del espacio (no hay small disjuncts). Los gráficos nos hacen pensar en dos posibles hipótesis:

1. Por un lado, podemos pensar que la clase negativa se superpone sobre la positiva. Tendríamos puntos ruidosos y fronteras difusas.

2. Por otro, se puede considerar que la clase minoritaria no está distribuída en un único clúster, sino en 4 de ellos en zonas del espacio bastante cercanas, pero aún así separados. Con esta idea en mente, un método de undersampling o de eliminación de ruido que quite las franjas centrales de la clase negativa podría darnos unos muy buenos resultados en training, pero malos en test si realmente esos puntos son representativos.

Dependerá de la semántica del problema y de los atributos ver cuál de estos razonamientos sería más lógico. En principio,
puesto que estamos entrenando con KNN, esta distribución de los datos vuelve la tarea un poco más difícil, favoreciendo la clasificación de la clase negativa.

Sobre los resultados, con los valores en crudo comprobamos que la superposición de clases hace difícil obtener un valor alto de sensitivity (en comparación con el resto de métodos). Vemos que por el contrario los valores de specificity son bastante altos, obteniendo una curva ROC engañosamente optimista.

Fijándonos en las medias, RO y RU obtienen unos resultados bastante similares, ambos con una media de sensitivity notoria del 100%. Sin llegar a mostrar gráficas de los nuevos puntos generados, podemos suponer que al aplicar RU disminuímos la fuerza de las franjas negativas, y que con RO aumentamos la densidad de los clústers de la clase positiva, quitándole relevancia a estas franjas.

Con el método SMOTE se consigue un buen balanceo de las distintas métricas. Dependerá del objetivo del problema, pero si estuviéramos interesados no solo en detectar la clase minoritaria sino también clasificar bien la positiva, el AUC de SMOTE nos muestra que este podría ser la mejor opción a utilizar.
Si por otro lado pudiéramos permitirnos mayor tasa de falsos positivos, random under/over-sampling nos dan mejores resultados.

Pese a ello, de cara a enfrentarnos a este problema de forma real podría ser más interesante optar por una hibridización SMOTE-IPF o SMOTE-ENN de cara a eliminar los puntos ruidosos y limpiar las fronteras tras aplicar el oversampling.

-------------------------------------------------------------------------------------------------------------------

# Activity 2

Load the data
```{r}
dataset <- ecoli1
dataset
```

Summary
```{r}
summary(dataset)

cat("\n Imbalance Ratio: ")
imbalanceRatio(dataset)
```

Preprocess
```{r}
# Variable Chg has almost zero variance, remove it
dataset$Chg <- NULL

# Change binary factors into numeric
dataset$Lip <- dataset$Lip %>% as.numeric()
```


Train-test split
```{r}
set.seed(42) #To ensure the same output

#An easy way to create split "data partitions":
trainIndex <- createDataPartition(dataset$Class, p = .75, 
                                  list = FALSE, 
                                  times = 1)
trainData <- dataset[ trainIndex,]
testData  <- dataset[-trainIndex,]

#Check IR to ensure a stratified partition
imbalanceRatio(trainData)
imbalanceRatio(testData)
```

Apply preprocessing with oversample function
```{r}
# Basic SMOTE
smt <- trainData %>% oversample(method = "SMOTE", ratio = 0.5)

# SMOTE borderline
blsmt <- trainData %>% oversample(method = "BLSMOTE", ratio = 0.5)

# Majority Weighted Minority Oversampling
mwmt <- trainData %>% oversample(method = "MWMOTE", ratio = 0.5)

# ANSMOTE
ansmt <- trainData %>% oversample(method = "ANSMOTE", ratio = 0.5)
```

## Train with KNN

```{r}
ctrl <- trainControl(method="repeatedcv", number=5, repeats = 3,
                     classProbs=TRUE, summaryFunction = twoClassSummary)
```


```{r}
model.smt <- learn_model(smt, ctrl, "SMT ")
cm.smt <- test_model(testData, model.smt,"SMT ")
```

```{r}
model.blsmt <- learn_model(blsmt, ctrl, "BLSMT ")
cm.blsmt <- test_model(testData, model.blsmt,"BLSMT ")
```

```{r}
model.mwmt <- learn_model(mwmt, ctrl, "MWSMT ")
cm.mwmt <- test_model(testData, model.mwmt,"MWSMT ")
```

```{r}
model.ansmt <- learn_model(ansmt, ctrl, "ANSMT ")
cm.ansmt <- test_model(testData, model.ansmt,"ANSMT ")
```

Summarize models results
```{r}
models <- list(smt = model.smt, blsmt = model.blsmt, 
               mwmt = model.mwmt, ansmt = model.ansmt)
results <- resamples(models)
summary(results)
```

Plot results comparison between models
```{r}
bwplot(results)
```

# Visualize the data distribution between original and preprocess data.

```{r results="hold"}
trainData %>% 
  ggplot(aes(x=Mcg, y=Gvh, color=Class)) +
    geom_point() +
    labs(title="RAW") +
    theme_light()

smt %>% 
  ggplot(aes(x=Mcg, y=Gvh, color=Class)) +
    geom_point() +
    labs(title="SMOTE") +
    theme_light()

blsmt %>% 
  ggplot(aes(x=Mcg, y=Gvh, color=Class)) +
    geom_point() +
    labs(title="BLSMOTE") +
    theme_light()

mwmt %>% 
  ggplot(aes(x=Mcg, y=Gvh, color=Class)) +
    geom_point() +
    labs(title="MWMOTE") +
    theme_light()

ansmt %>% 
  ggplot(aes(x=Mcg, y=Gvh, color=Class)) +
    geom_point() +
    labs(title="ANSMOTE") +
    theme_light()
```

t-SNE
```{r message=FALSE, results="hold"}
tsne_out <- trainData %>% select(-Class) %>% tsne()
data.frame(x = tsne_out[,1], 
           y = tsne_out[,2], 
           col = trainData$Class) %>% 
  ggplot() + 
    geom_point(aes(x=x, y=y, color=col)) +
    labs(title="RAW") +
    theme_light()

tsne_out <- smt %>% select(-Class) %>% tsne()
data.frame(x = tsne_out[,1], 
           y = tsne_out[,2], 
           col = smt$Class) %>% 
  ggplot() + 
    geom_point(aes(x=x, y=y, color=col)) +
    labs(title="SMOTE") +
    theme_light()

tsne_out <- blsmt %>% select(-Class) %>% tsne()
data.frame(x = tsne_out[,1], 
           y = tsne_out[,2], 
           col = blsmt$Class) %>% 
  ggplot() + 
    geom_point(aes(x=x, y=y, color=col)) +
    labs(title="BLSMOTE") +
    theme_light()

tsne_out <- mwmt %>% select(-Class) %>% tsne()
data.frame(x = tsne_out[,1], 
           y = tsne_out[,2], 
           col = mwmt$Class) %>% 
  ggplot() + 
    geom_point(aes(x=x, y=y, color=col)) +
    labs(title="MWMOTE") +
    theme_light()

tsne_out <- ansmt %>% select(-Class) %>% tsne()
data.frame(x = tsne_out[,1], 
           y = tsne_out[,2], 
           col = ansmt$Class) %>% 
  ggplot() + 
    geom_point(aes(x=x, y=y, color=col)) +
    labs(title="ANSMOTE") +
    theme_light()
```


Pie plots
```{r}
plot_pie <- function(y, method="") {
  n_classes <- c(sum(y=="positive"),sum(y=="negative"))
  pct <- round(n_classes/sum(n_classes)*100,digits=2)
  
  # lbls <- levels(dataset$Class)
  lbls <- c("positive", "negative")
  lbls <- paste(lbls, pct) # add percents to labels
  lbls <- paste(lbls,"%",sep="") # ad % to labels
  
  pie(n_classes,labels = lbls, main=paste("Class distribution", method))
}
```

```{r}
plot_pie(trainData$Class, "RAW")
plot_pie(smt$Class, "SMOTE")
plot_pie(blsmt$Class, "BLSMOTE")
plot_pie(mwmt$Class, "MWMOTE")
plot_pie(ansmt$Class, "ANSMOTE")
```

Sobre el aprendizaje, en general se obtienen mejores resultados para SMOTE, pero a costa de una mayor variabilidad en specificity.
Por otro lado, ANSMOTE y MWMOTE se comportan de manera muy similar, y podemos ver en las gráficas y en los plost de tsne que los datasets resultantes se asemejan bastante.
Finalmente, BLSMOTE parece obtener resultados más inestables en KNN, pero en media su comportamiento en specificity es muy bueno.