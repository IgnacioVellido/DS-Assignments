---
title: "Clasificación monotónica"
author: "Ignacio Vellido Expósito"
date: "22/01/2021"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
# Configuración RMarkdown
knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
# Librerías
library(tidyverse)
library(xgboost)
```


```{r}
# Leer datos
df <- read_csv("data/esl.arff", col_names = FALSE, skip = 43)
```

```{r}
label_name <- colnames(df)[ncol(df)]
df$labels <- df[[label_name]]
df[[label_name]] <- NULL

df
```


```{r}
# Hacer particiones OVA
# Realizar particiones (if label > i -> 1, else -> 0)

# label_list <- unique(df$labels)
# label_list <- as.integer(unique(df$labels))
num_part <- length(unique(df$labels))

partitions <- vector("list", length(num_part))
partitions_labels <- vector("list", length(num_part))

for(i in 1:num_part) {
  partitions[[i]] <- df
  partitions[[i]] <- partitions[[i]] %>% 
                      mutate(labels = ifelse(labels <= i, 0, 1))
  partitions_labels[[i]] <- partitions[[i]]$labels %>% as.matrix()
  partitions[[i]] <- partitions[[i]][,-ncol(partitions[[i]])] %>% as.matrix()
}

partitions[[6]] %>% as.data.frame()
df
partitions_labels[[6]] %>% as.data.frame()
```


```{r}
# Función para clasificar K-1 modelos
classify <- function(df) {
    # Realizar particiones
    labels <- as.integer(unique(df$labels))
    num_part <- length(labels) - 1
    
    partitions <- vector("list", length(num_part))
    
    for(i in 1:num_part) {
      partitions[[i]] <- df
      partitions[[i]] <- partitions[[i]] %>% mutate(labels = ifelse(labels <= i, 0, 1))
      
      # Para realizar el entrenamiento, necesitamos que las etiquetas se codifiquen
      # como factores
      partitions[[i]]$labels <- partitions[[i]]$labels %>% as.factor()
    }

    # Aplicar xgboost
    models <- vector("list", length(num_part))
    for(i in 1:num_part) {
      models[[i]] <- xgboost(partitions[[i]], partitions_labels[[i]],
                     nrounds = 1, monotone_constraints=1)
    }

    # Devolver modelos
    models
}
```

```{r}
# Función para calcular probabilidades reales y hacer predicciones
make_predictions <- function(models, df) {
    # Calcular probabilidades del df para un modelo
    pr_list <- lapply(models,
                function(m,r) {
                  # Solo nos interesa la segunda probabilidad, que sea > i
                  predict(m,r) %>% as.data.frame() -> x
                  x %>% mutate(row = rownames(x))
                },
                df)
    
    
    # Juntar probabilidades por filas del df
    pr_list <- reduce(pr_list, left_join, by="row")
    

    # Mover la columna auxiliar row a la primera posición
    # y llenarla con unos, preparando el cálculo de las
    # probabilidades de cada clase
    pr_list <- pr_list[, c(2,1,3:ncol(pr_list))]
    pr_list$row <- 1
    
    
    # Pertenencia a una clase o no
    pr_list <- pr_list > 0.6
    
    # Fórmula del clasificador multiclase
    pr_list %>% apply(1, function(x) {
      1 + sum(x)
    }) %>% as.data.frame()
}

# test <- xgboost::xgb.DMatrix(data = df %>% as.matrix())
test <- df %>% select(-labels) %>% as.matrix()
pred <- make_predictions(models, test)

pred
```

```{r}
# Ver medidas de aciertos
f1_score <- function(predicted, expected, positive.class="1") {
    res <- list()
    
    cm = as.matrix(table(expected, predicted))
    res$cm <- cm
    
    tp <- diag(cm)
    fp <- cm[lower.tri(cm)]
    fn <- cm[upper.tri(cm)]

    res$precision <- tp / (tp + fp)
    res$recall <- tp / (tp + fn)
    
    
    f1 <-  ifelse(res$precision + res$recall == 0, 0,
                  2 * res$precision * res$recall / (res$precision + res$recall))

    #Assuming that F1 is zero when it's not possible compute it
    f1[is.na(f1)] <- 0
    res$precision[is.na(res$precision)] <- 0
    res$recall[is.na(res$recall)] <- 0

    #Binary F1 or Multi-class macro-averaged F1
    res$f1 <- ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))

    res
}

df$labels <- df$labels %>% as.factor()
pred$class <- factor(x = pred$class, levels = levels(df$labels))

f1_score(pred$class, df$labels)

# Vemos que los resultados obtenidos son mayormente buenos, donde los fallos
# cometidos solo se dan una o dos clases arriba o abajo.

# Notamos que la predicción de las clases extremas (la 1 y la 9) no es en ningún
# momento correcta. Es más, las probabilidades de estas clases son tan bajas en
# cada caso que nunca se predicen.
```

En esta segunda parte, se implementará un modelo similar de clasificación múltiple para problemas
con restricciones de monotonía. Se utilizará el modelo OVA básico descrito en las diapositivas de
teoría (diapositiva 107) con el algoritmo xgboost de R o Python, usando el parámetro
monotone_constraints=1. Este algoritmo obtiene monotonía global en problemas binarios, por
lo que es necesario hacer una descomposición que sea monótona-consistente. El resultado se generará
en un fichero llamado monoxgboost.[R|py].

•La propuesta OVA es simple:
• Variante de Frank y Hall.
• Preserva la monotonicidad y limita la L1.
• hc(x) = 1[y ≥ c], clasificador binario que distingue clase menor
que c de clase mayor igual que c, donde 1[condición] = 1 si
condición, sino 0.
• El clasificador multi-clase viene dado por: