{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "7489139fadedff3b08012098ca2abd2058b255578d7205af55c55f231e9a91e1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Libraries\n",
    "################################################################################\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Preprocesamiento\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks, CondensedNearestNeighbour\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Load data\n",
    "################################################################################\n",
    "\n",
    "original_train = pd.read_csv(\"data/train_values.csv\")\n",
    "original_test  = pd.read_csv(\"data/test_values.csv\")\n",
    "original_labels = pd.read_csv(\"data/train_labels.csv\")\n",
    "\n",
    "train  = original_train\n",
    "test   = original_test\n",
    "labels = original_labels\n",
    "\n",
    "# Columnas\n",
    "for i,c in enumerate(train.columns):\n",
    "    print(i, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# No hay duplicados en los ids, se pueden quitar\n",
    "############################################################################\n",
    "\n",
    "train = train.drop(columns=\"building_id\")\n",
    "test = test.drop(columns=\"building_id\")\n",
    "labels = labels.drop(columns=\"building_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Preprocesado Nº 2\n",
    "# Quitar columnnas innecesarias / no interesantes\n",
    "############################################################################\n",
    "\n",
    "# Quitamos geo_2 y geo_3. Tienen demasiadas categorías diferentes para los árboles,\n",
    "# y no se solapan\n",
    "train = train.drop(columns=[\"geo_level_2_id\",\"geo_level_3_id\"])\n",
    "test = test.drop(columns=[\"geo_level_2_id\",\"geo_level_3_id\"])\n",
    "\n",
    "\n",
    "# En estas dos predomina (+85%) una clase y el resto no sirve para determinar\n",
    "# ninguna etiqueta (se mantiene la proporción o las 3 están representadas)\n",
    "train = train.drop(columns=[\"plan_configuration\",\"legal_ownership_status\"])\n",
    "test = test.drop(columns=[\"plan_configuration\",\"legal_ownership_status\"])\n",
    "\n",
    "# Altura y nº de plantas altamente correladas, categorizamos nº de plantas y quitamos altura\n",
    "# Solo una instancia con 9 plantas, y las anteriores no siguen el mismo patrón (que todas sufrieran el mismo tipo de daño)\n",
    "# Como tenemos proporciones muy similares, juntamos las +5 con ella\n",
    "train[\"count_floors_pre_eq\"] = train[\"count_floors_pre_eq\"].replace({6:5, 7:5, 8:5, 9:5})\n",
    "test[\"count_floors_pre_eq\"] = test[\"count_floors_pre_eq\"].replace({6:5, 7:5, 8:5, 9:5})\n",
    "\n",
    "# No nos interesan variables numéricas en los árboles, acabarían discretizándose\n",
    "train = train.drop(columns=[\"height_percentage\"])\n",
    "test = test.drop(columns=[\"height_percentage\"])\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# v10 - Discretizar variables numéricas (age, area_percentage)\n",
    "# count_families y count_floors considerarlas como categóricas\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Eliminar instancias duplicadas\n",
    "############################################################################\n",
    "\n",
    "df = train.join(labels[\"damage_grade\"])\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "train = df.drop(columns=[\"damage_grade\"])\n",
    "labels = df[\"damage_grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# One-hot-enconding\n",
    "############################################################################\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_columns = [\"geo_level_1_id\",\n",
    "                        \"land_surface_condition\",\n",
    "                        \"foundation_type\",\n",
    "                        \"roof_type\",\n",
    "                        \"ground_floor_type\",\n",
    "                        \"other_floor_type\",\n",
    "                        \"position\",\n",
    "                       ]\n",
    "for column in categorical_columns:\n",
    "    # Para training\n",
    "    dummies = pd.get_dummies(train[column])\n",
    "    dummies.columns = [column + \"_\" + str(x) for x in dummies.columns]\n",
    "    train = train.drop(columns=column)\n",
    "    train = pd.concat([train, dummies], axis=1)\n",
    "\n",
    "    # Para test\n",
    "    dummies = pd.get_dummies(test[column])\n",
    "    dummies.columns = [column + \"_\" + str(x) for x in dummies.columns]\n",
    "    test = test.drop(columns=column)\n",
    "    test = pd.concat([test, dummies], axis=1)\n",
    "\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 geo_level_1_id\n1 count_floors_pre_eq\n2 age\n3 area_percentage\n4 land_surface_condition\n5 foundation_type\n6 roof_type\n7 ground_floor_type\n8 other_floor_type\n9 position\n10 has_superstructure_adobe_mud\n11 has_superstructure_mud_mortar_stone\n12 has_superstructure_stone_flag\n13 has_superstructure_cement_mortar_stone\n14 has_superstructure_mud_mortar_brick\n15 has_superstructure_cement_mortar_brick\n16 has_superstructure_timber\n17 has_superstructure_bamboo\n18 has_superstructure_rc_non_engineered\n19 has_superstructure_rc_engineered\n20 has_superstructure_other\n21 count_families\n22 has_secondary_use\n23 has_secondary_use_agriculture\n24 has_secondary_use_hotel\n25 has_secondary_use_rental\n26 has_secondary_use_institution\n27 has_secondary_use_school\n28 has_secondary_use_industry\n29 has_secondary_use_health_post\n30 has_secondary_use_gov_office\n31 has_secondary_use_use_police\n32 has_secondary_use_other\n"
     ]
    }
   ],
   "source": [
    "for i,c in enumerate(train.columns):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataset shape 179275\n",
      "Resampled dataset shape 125377\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# v7 - SMOTE + ENN\n",
    "# Oversampling de todas las clases inferiores, undersampling de puntos ruidosos\n",
    "############################################################################\n",
    "\n",
    "sme = SMOTEENN(random_state=42)\n",
    "print('Original dataset shape %s' % len(labels))\n",
    "X_res, y_res = sme.fit_resample(train, labels)\n",
    "print('Resampled dataset shape %s' % len(y_res))\n",
    "\n",
    "train = pd.DataFrame(X_res)\n",
    "labels = pd.DataFrame(y_res)\n",
    "\n",
    "labels.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataset shape 179275\n",
      "Resampled dataset shape 302703\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# v8y9 - SMOTE\n",
    "############################################################################\n",
    "\n",
    "# v8 Con onehot\n",
    "# categorical = list(range(0,len(train.columns)))\n",
    "# categorical.remove(14)\n",
    "# categorical = categorical[3:]\n",
    "\n",
    "# v9 SIN ONEHOT\n",
    "categorical = list(range(0,len(train.columns)))\n",
    "categorical.remove(1)\n",
    "categorical.remove(2)\n",
    "categorical.remove(3)\n",
    "categorical.remove(21)\n",
    "\n",
    "sm_nc = SMOTENC(categorical_features=categorical, random_state=0)\n",
    "print('Original dataset shape %s' % len(labels))\n",
    "x_smnc, y_smnc = sm_nc.fit_resample(train, labels)\n",
    "print('Resampled dataset shape %s' % len(y_smnc))\n",
    "\n",
    "train = pd.DataFrame(x_smnc)\n",
    "labels = pd.DataFrame(y_smnc)\n",
    "\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Preprocesado 3.1\n",
    "# Selección de características\n",
    "# For classification: chi2, f_classif, mutual_info_classif\n",
    "############################################################################\n",
    "\n",
    "# selector = SelectKBest(chi2, k=15)\n",
    "selector = SelectKBest(mutual_info_classif, k=15)\n",
    "selector.fit(train, labels)\n",
    "x_reduced = selector.transform(train)\n",
    "\n",
    "# Select same feautures as with train\n",
    "test = selector.transform(test)\n",
    "test = pd.DataFrame(test, columns=columns)\n",
    "\n",
    "# Ver columnas\n",
    "columns = train.columns[selector.get_support()]\n",
    "print(columns)\n",
    "\n",
    "# Mostrar scores\n",
    "# pd.DataFrame(selector.scores_).transpose()\n",
    "plt.bar([i for i in range(len(selector.scores_))], selector.scores_)\n",
    "plt.show()\n",
    "\n",
    "x_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Preprocesado 3.1 ... continuación\n",
    "# Aplicar undersampling\n",
    "############################################################################\n",
    "\n",
    "# undersample = TomekLinks()\n",
    "# undersample = CondensedNearestNeighbour(n_neighbors=1)\n",
    "undersample = EditedNearestNeighbours(n_neighbors=3)\n",
    "\n",
    "# transform the dataset\n",
    "X, y = undersample.fit_resample(x_reduced, labels)\n",
    "\n",
    "print(\"Antes: \" + str(len(x_reduced)))\n",
    "print(\"Después: \" + str(len(X)))\n",
    "\n",
    "train = pd.DataFrame(X, columns=columns)\n",
    "labels = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# v1 Juntar columnas binarias\n",
    "################################################################################\n",
    "\n",
    "# Join has_superstructure (binary) columns has strings\n",
    "# Get selected rows to string\n",
    "df = train.iloc[:,15:25].astype(str)\n",
    "df_test = test.iloc[:,15:25].astype(str)\n",
    "\n",
    "# Join them\n",
    "train[\"has_superstructure\"] = df.apply(lambda x: ''.join(x), axis=1)\n",
    "test[\"has_superstructure\"] = df_test.apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "# Join has_secondary (binary) columns has strings\n",
    "# Get selected rows to string\n",
    "df = train.iloc[:,29:38].astype(str)\n",
    "df_test = test.iloc[:,29:38].astype(str)\n",
    "\n",
    "# Join them\n",
    "train[\"has_secondary\"] = df.apply(lambda x: ''.join(x), axis=1)\n",
    "test[\"has_secondary\"] = df_test.apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "# Remove joined columns\n",
    "removed_cols = list(range(15,26)) + list(range(29,39))\n",
    "train = train.drop(columns=train.columns[removed_cols])\n",
    "test = test.drop(columns=test.columns[removed_cols])\n",
    "\n",
    "# Convert new cols to int\n",
    "# train[\"has_superstructure\"] = train[\"has_superstructure\"].apply(lambda x: int(x,2))\n",
    "# train[\"has_secondary\"] = train[\"has_secondary\"].apply(lambda x: int(x,2))\n",
    "# train\n",
    "\n",
    "# Convert to categorical in order\n",
    "# train.has_secondary = train.has_secondary.astype(\"category\").cat.codes\n",
    "# train.has_superstructure = train.has_superstructure.astype('category').cat.codes\n",
    "\n",
    "# has_secondary_use is enconded in has_secondary, remove it\n",
    "train = train.drop(columns=\"has_secondary_use\")\n",
    "test = test.drop(columns=\"has_secondary_use\")\n",
    "train"
   ]
  },
  {
   "source": [
    "################################################################################\n",
    "# Write data\n",
    "################################################################################\n",
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "labels.to_csv(\"data/preprocessed_labels.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  }
 ]
}