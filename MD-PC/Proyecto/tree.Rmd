---
title: "Tree"
author: "Ignacio Vellido Expósito"
date: "22/12/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    toc: true
    highlight: github
    df_print: paged
    number_sections: true
---

<style>
.entry-content {
    width: 95%;
    max-width: unset;
}
pre {
  overflow-x: train_values;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)
library(tidyverse)
library(ggplot2)

library(tree)
library(RWeka)
library(partykit)

# library(caret)  # scores

# library(cowplot)  # plot_grid
# library(corrplot) # corr y corrplot
# library(reshape2) # melt
# library(dlookr) # normality
# library(moments)  # skewness
# library(car)  # scatterplotMatrix
# library(MASS) # fit

set.seed(2)
```

Se puede usar J48 (basados en C4.5, con medida de entropía) y tree (basado en CART, árboles binarios con criterio según GINI (MIRAR APUNTES TEORÍA))

---------------------------------------------------------------------------------------------------------------------

Metodología:

- Realización de un EDA para descubrir información de interés. Conclusiones (entre otras):
  - Variables extremadamente desbalanceadas (más del 90%), poca información nueva aportada por ellas.
  - Sin distribuciones normales
  - Correlación solo en dos columnas (Altura y Nº de plantas)
  - Clases desbalanceadas, la de grado 1 se encuentra .
  - ~16mil instancias duplicadas, pero de edificios diferentes. Es posible que sean de una misma urbanización/zona de una ciudad. Sorprendía que se repitieran por el count_families, pero la mayoría son de 1 sola (posiblemente sean casas).
  - Puede que haya valores perdidos no codificados como NA.
  - Los IDs de posición geográfica deberían ser relevantes para el problema (un solo terremoto). Ahora bien, suponiendo que IDs consecutivos son zonas cercanas, no se aprecia relación alguna.

- Preprocesamiento inicial, agrupación de variables binarias en una. De esta manera no se pierde información alguna y mantenemos la misma expresividad en los árboles.
- Puesto que se cuentan con muchas instancias, separación de un 20% para evaluación local.
- Ejecución de los algoritmos de aprendizaje con selección de hiperparámetros por CV. Se prueba con los siguientes:
  -
  -
  - Problema: Algunas variables categóricas cuentan con demasiadas categorías para CART
- Evaluación de los modelos, resultados en training
  - C4.5
    - Precisión:
    - Recall:
  - CART
    - Precisión:
    - Recall:
- Se envía ?? a DrivenData con la predicción de test, resultados:
  - Precisión:
  - Recall:

ASÍ TARDA MUCHO EN ENTRENAR
- Hay un límite en el nº de características para tree, se quitan geo_id_2 y geo_id_3
- has_superstucture también, aunque se podrían mantener las variables originales
- Se aumenta partición de validación a 30%
- Se quitan variables muy desbalanceadas: 
(REVISAR QUE EN LAS CATEGORÍAS DESBALANCEADAS NO PREDOMINE UNA ETIQUETA)
  plan_configuration (se podrían mantener las etiquetas q y p)
  legal_ownership_status (idem con r y a)
- Se podrían quitar variables de has_superstructure y has_secondary

- Agrupar geo_level_id_1 ??

- Preprocesamiento 2: Limpieza de ruido, selección de características

Las instancias duplicadas afectan a los árboles según la medida que se use. En este caso sí, y por tanto se le da más peso a la información que está duplicada.
Podemos probar dejándola y sin ella, deberían dar resultados diferentes

(Para C4.5 si hubiera datos perdidos los ignora, pero no tenemos ninguno codificado como tal, aunque eso no implica que lo estén con otra codificación)

1. Preprocesamiento mínimo, agrupación de variables sin pérdida de información. Intento de aprendizaje con todas las variables posibles. Problemas: limítación de etiquetas en variables categóricas y alto coste computacional.
2. Selección de variables: Eliminando una correlada y las geo2y3 por tener demasiadas categorías (entre otras, mirar cuáles). Eliminación de duplicados y partición 70-30 con validación.
3. Sigue costando mucho, se quedan más de 124979 instancias. Se usa método de CNN (Condensed Nearest Neighbor) para seleccionar instancias (Condensed Nearest Neighbor selects the subset of instances that are able to correctly classifing the original datasets using a one-nearest neighbor rule.). ES POSIBLE QUE ESTEMOS REDUCIENDO EL BALANCEO CON ESTO, MIRARLO.
Se aplica un one-hot-encoding de manera que se puedan aplicar las distancias de estos métodos, en otro caso no se puede aplicar. Se consigue reducir hasta un ...

PROBAR CON DUPLICADOS Y SIN DUPLICADOS
Por un lado, queremos mantener los duplicados pues afecta a las medidas de GINI y entropía, de esta manera la información repetida se ve reflejada en los árboles pues se favorece la correcta etiquetación de estos. Por otro lado, al tener tan cantidad de instancias duplicadas podemos perder expresividad en árboles al contar con tantas columnas (y en unas pruebas iniciales se veía que sí, no se etiquetaba nunca la clase 1).
Tenemos en cuenta además que al seleccionar características aumenta el número de duplicados en el dataset.
(También se tienen en cuenta las clases de salida al eliminar duplicados)

Excesivo número de intstancias nos permite reservar una mayor parte para validación

TENIENDO VARIABLES CATEGÓRICAS LA MAYORÍA DE MÉTODOS DE UNDERSAMPLING NO SE PUEDEN APLICAR (A NO SER QUE SE HAGA UN ONE HOT ENCONDING)
<!-- https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd -->
<!-- https://scikit-learn.org/stable/modules/feature_selection.html -->
<!-- https://www.rdocumentation.org/packages/unbalanced/versions/2.0 --> 2015
<!-- https://rdrr.io/cran/NoiseFiltersR/f/README.md --> 2016
<!-- https://rdrr.io/cran/UBL/ --> 2017
<!-- https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/ -->
<!-- https://github.com/scikit-learn-contrib/imbalanced-learn -->

---------------------------------------------------------------------------------------------------------------------

Funciones
```{r}
# Calcula accuracy
accuracy <- function(bag.datos){
  return (sum (sapply(1:length(bag.datos$y), function(x){
    if (is.na(bag.datos$predicted[x]))
      0
    else if (as.numeric(bag.datos$y[x])==as.numeric(bag.datos$predicted[x]))
      1
    else 
      0
  }))/length(bag.datos$y))
}

################################################################################

# Calcula el F1 score micro. Función adaptada de la versión macro de:
# https://stackoverflow.com/questions/8499361/easy-way-of-counting-precision-recall-and-f1-score-in-r/8502026
f1_score <- function(predicted, expected, positive.class="1") {
    res <- list()
    
    # predicted <- factor(as.character(predicted), levels=unique(as.character(expected)))
    # expected  <- as.factor(expected)
    cm = as.matrix(table(expected, predicted))
    
    tp <- diag(cm)
    fp <- cm[lower.tri(cm)]
    fn <- cm[upper.tri(cm)]

    res$precision <- tp / (tp + fp)
    res$recall <- tp / (tp + fn)
    
    # res$precision <- sum(tp / (tp + fp))
    # res$precision <- ifelse(is.na(res$precision), 0, res$precision)
    # res$recall <- sum(tp / (tp + fn))
    
    # res$precision <- diag(cm) / colSums(cm)
    # res$recall <- diag(cm) / rowSums(cm)
    
    f1 <-  ifelse(res$precision + res$recall == 0, 0,
                  2 * res$precision * res$recall / (res$precision + res$recall))

    #Assuming that F1 is zero when it's not possible compute it
    f1[is.na(f1)] <- 0

    #Binary F1 or Multi-class macro-averaged F1
    res$f1 <- ifelse(nlevels(expected) == 2, f1[positive.class], mean(f1))

    res
}

################################################################################

# Muestra matriz de confusión y accuracy
compare_pred <- function(predicted, expected) {
  table(expected, predicted) %>% print()
  
  f1_score(predicted, expected) %>% print()
}
```

---------------------------------------------------------------------------------------------------------------------

Cargar conjuntos de datos preprocesados
```{r}
trn_values <- read_csv("data/preprocessed_train.csv")
trn_labels <- read_csv("data/train_labels.csv")
tst_values  <- read_csv("data/preprocessed_test.csv")

names <- colnames(trn_values)
```

```{r}
trn_values
trn_labels
tst_values
```


Cambiar lectura por defecto de read_csv (pasar a factors)
```{r}
#  1:3 -> IDs de posición geográfica
# factores <- c(1,8:15,17:18)
factores <- c("geo_level_1_id",
              # "count_floors_pre_eq",
              # "age",
              # "area_percentage",
              "land_surface_condition",
              "foundation_type",
              "roof_type",
              "ground_floor_type",
              "other_floor_type",
              "position",
              # "count_families",
              "has_superstructure",
              "has_secondary")

for (i in factores) {
  # trn_values[[names[i]]] <- trn_values[[names[i]]] %>% as.factor()
  # tst_values[[names[i]]] <- tst_values[[names[i]]] %>% as.factor()
  trn_values[[i]] <- trn_values[[i]] %>% as.factor()
  tst_values[[i]] <- tst_values[[i]] %>% as.factor()
}

# Las etiquetas
# trn_labels$damage_grade <- trn_labels$damage_grade %>% as.ordered()
trn_labels$damage_grade <- trn_labels$damage_grade %>% as.factor()

# Añadir labels a train
train <- bind_cols(trn_values, trn_labels$damage_grade)
colnames(train)[ncol(train)] <- "labels"
```

```{r}
train
tst_values
```

<!-- Quitar duplicados aquí -->
```{r}
sum(duplicated(train))
train <- unique(train)
sum(duplicated(train))
```

Seleccionando instancias con CNN
```{r}
data <- ubCNN(X=input, Y= output)
newData <- cbind(data$X, data$Y)
```



Separamos un 30% para validar localmente
```{r}
size <- floor(0.7 * nrow(train))

index <- sample(1:nrow(train), size = size)

val <- train[-index,]
train  <- train[index,]
```


```{r}
summary(train)
```

---------------------------------------------------------------------------------------------------------------------
Con tree

Probar con todas las variables
(Se acaban usando "geo_level_1_id"  "foundation_type" "roof_type", tiene sentido)
<!-- Prueba 1: -->
<!-- tree(labels~. -->
<!--                   -has_superstructure -->
<!--                   -geo_level_2_id -->
<!--                   -geo_level_3_id, train) -->
<!-- Demasiado tiempo computacional -->
<!-- Prueba 2 -->
<!-- tree(labels~. -->
<!--                   -has_superstructure -->
<!--                   -geo_level_2_id -->
<!--                   -geo_level_3_id -->
<!--                   -legal_ownership_status -->
<!--                   -plan_configuration, train) -->
<!-- Demasiado tiempo computacional -->
```{r}
# Construir un arbol que clasifica la especie en base al resto de variables
tree.trn <- tree(labels~.-has_superstructure, train)

summary(tree.trn)
```

```{r}
plot(tree.trn)
text(tree.trn)
```

```{r}
tree.trn
```


```{r}
# Aplico el arbol sobre el conjunto de validación
tree.pred <- predict(tree.trn, val, type ="class")
```


```{r}
# Visualizo la matriz de confusion
compare_pred(tree.pred, val$labels)
```


Poda a priori ?
```{r}
# Podar el arbol usando cv
set.seed(2)
cv.trn <- cv.tree(tree.trn, FUN=prune.misclass)

cv.trn
```

Poda a posterior ?
```{r}
# Ahora podamos el arbol con prune.misclass
prune.trn <- prune.misclass(tree.trn, best = 3)
par(mfrow = c(1,1))
plot(prune.trn)
text(prune.trn, pretty =0)
```


```{r}
# Como se comportara este arbol en su capacidad de prediccion
tree.pred <- predict(prune.trn, val, type="class")

compare_pred(tree.pred, val$labels)
```


```{r}
# Ahora podemos modificar el tamanio del arbol modificando best
prune.trn <- prune.misclass(tree.trn, best = 4)
plot(prune.trn)
text(prune.trn, pretty=0)
tree.pred <- predict(prune.trn, val, type="class")

compare_pred(tree.pred, val$labels)
```

Evaluar en test

Probar con diferentes tamaños

Evaluar en test (incluir matriz confusión, precision y recall)

Aplicar CV

---------------------------------------------------------------------------------------------------------------------
Repetir con J48

Evaluar con CV

```{r}
# J48 es la implementacion de C4.5 en Weka, y su uso es similar
# al de la funcion "tree" vista anteriormente en este script.
# Vemos un ejemplo de uso sobre "iris"
modelC4.5 <- J48(labels~.
                  -has_superstructure
                  -geo_level_2_id
                  -geo_level_3_id
                  -legal_ownership_status
                  -plan_configuration, data=train)

modelC4.5.pred <- predict(modelC4.5, val)

modelC4.5$classifier$toSummaryString()

## Fit J48 tree with reduced error pruning
# m5 <- J48(diabetes ~ ., data = PimaIndiansDiabetes,control = Weka_control(R = TRUE))
```

Si no se puede puede que sea por tener muchas hojas
```{r}
plot(modelC4.5)
```

```{r}
# modelC4.5.pred %>% head()
```


```{r}
resul = as.data.frame(cbind(predicted = modelC4.5.pred, y=val$labels))
accuracy(resul)

compare_pred(modelC4.5.pred, val$labels)
```


```{r}
# Si queremos hacer una validacion cruzada usando RWeka
# modelC4.5 = J48(labels~., data=train)
cv_resul <- evaluate_Weka_classifier(modelC4.5, numFolds=10)
cv_resul
```

---------------------------------------------------------------------------------------------------------------------

EDA (AGAIN)

Vemos los ids de geo2 de una misma geo1
```{r}
trn_values %>% filter(geo_level_1_id == 1) %>% select(geo_level_2_id) %>% table() -> x
x <- names(x[x != 0])
x
```

Las geo no siguen un orden

Buscamos intersección con otro geo1
```{r}
trn_values %>% filter(geo_level_1_id == 2) %>% select(geo_level_2_id) %>% table() -> y
y <- names(y[y != 0])
y
intersect(y, x)
```

No intersecan, se pueden quitar geo_2 y geo_3.
Tienen demasiadas categorías diferentes para los árboles


```{r}
df <- bind_cols(trn_values, trn_labels$damage_grade)
colnames(df)[19] <- "labels"
```


Miramos variables con muchas categorías
-plan_configuration
```{r}
df %>% select(plan_configuration) %>% table()

df %>% filter(plan_configuration == "a") %>% select(labels) %>% table()
df %>% filter(plan_configuration == "c") %>% select(labels) %>% table()
df %>% filter(plan_configuration == "f") %>% select(labels) %>% table()
df %>% filter(plan_configuration == "m") %>% select(labels) %>% table()
df %>% filter(plan_configuration == "n") %>% select(labels) %>% table()
df %>% filter(plan_configuration == "o") %>% select(labels) %>% table()
df %>% filter(plan_configuration == "s") %>% select(labels) %>% table()
df %>% filter(plan_configuration == "u") %>% select(labels) %>% table()
```
Todas se pueden quitar, contienen valores de las tres clases (no determinan ninguna)
Por tanto se puede quitar la variable entera

-has_superstructure
-legal_ownership_status
```{r}
df %>% select(legal_ownership_status) %>% table()

df %>% filter(legal_ownership_status == "a") %>% select(labels) %>% table()
df %>% filter(legal_ownership_status == "r") %>% select(labels) %>% table()
df %>% filter(legal_ownership_status == "w") %>% select(labels) %>% table()
```

Idem, se pueden quitar. Además se nota la proporción de las etiquetas desbalanceadas.


Altura y nº de plantas altamente correladas, categorizamos nº de plantas
```{r}
df$count_floors_pre_eq %>% table()
```

Revisamos esas categorías con pocas instancias
```{r}
df %>% filter(count_floors_pre_eq == 5) %>% select(labels) %>% table() %>% prop.table()
df %>% filter(count_floors_pre_eq == 6) %>% select(labels) %>% table() %>% prop.table()
df %>% filter(count_floors_pre_eq == 7) %>% select(labels) %>% table() %>% prop.table()
df %>% filter(count_floors_pre_eq == 8) %>% select(labels) %>% table() %>% prop.table()
df %>% filter(count_floors_pre_eq == 9) %>% select(labels) %>% table() %>% prop.table()
```
Solo una instancia con 9, y las anteriores no siguen el mismo patrón (que todas sufrieran el mismo tipo de daño)
Como tenemos proporciones muy similares, juntamos las +5 con ella

Hay robustez al ruido en los árboles, por ahora no la miramos
---------------------------------------------------------------------------------------------------------------------

Como extra, probar con:
# RWeka tambien proporciona 3 algoritmos mas
# LMT implementa "Logistic Model Trees"
# M5P una version mejorada de C4.5