---
title: "Practica"
author: "Ignacio Vellido"
date: "12/22/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    toc: true
    highlight: github
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)

library(ggplot2)   # Gr?ficos
library(fitdistrplus)  # Ajuste de una distribución -> denscomp 
library(reshape)   # melt
library(ggbiplot)  # biplot
library(tidyverse)   
library(outliers)  # Grubbs
library(MVN)       # mvn: Test de normalidad multivariante  
library(CerioliOutlierDetection)  #MCD Hardin Rocke
library(mvoutlier) # corr.plot 
library(DMwR)      # lof
library(cluster)   # PAM
library(R.matlab)   # Read .mat files
```

```{r include=FALSE}
# M?ster -> Detecci?n de anomal?as
# Juan Carlos Cubero. Universidad de Granada

###########################################################################
# Funciones utilizadas a lo largo del curso
###########################################################################

# rm(list=ls()) 


###########################################################################
# Realiza un plot de todos los registros
# Permite cambiar el color con el que se visualiza un conjunto de registros. 
# Los registros que se muestran con otro color se especifican en el par?metro
# claves.a.mostrar 

plot_2_colores = function (datos, 
                           claves.a.mostrar, 
                           titulo = "",
                           colores = c("black", "red")){
  
  num.datos = nrow(as.matrix(datos))
  seleccionados =  rep(FALSE, num.datos)
  seleccionados[claves.a.mostrar] = TRUE
  colores.a.mostrar = rep(colores[1], num.datos)
  colores.a.mostrar [seleccionados] = colores[2]
  
  plot(datos, col=colores.a.mostrar, main = titulo)
}



###########################################################################
# Funci?n an?loga a son_outliers_IQR, salvo que devuelve un vector
# de claves en vez de un vector de bools

claves_outliers_IQR = function(datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  son.outliers.IQR = son_outliers_IQR(datos, ind.columna, coef)
  return (which(son.outliers.IQR  == TRUE))
}



###########################################################################
# Calcula los outliers IQR con respecto a una columna 
# Devuelve un vector de bools indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a la columna ind.columna
# coef es 1.5 para los outliers normales y hay que pasarle 3 para los outliers extremos

son_outliers_IQR = function (datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  cuartil.primero = quantile(columna.datos)[2]  
  #quantile[1] es el m?nimo y quantile[5] el m?ximo.
  cuartil.tercero = quantile(columna.datos)[4] 
  iqr = cuartil.tercero - cuartil.primero
  extremo.superior.outlier = (iqr * coef) + cuartil.tercero
  extremo.inferior.outlier = cuartil.primero - (iqr * coef)
  son.outliers.IQR  = columna.datos > extremo.superior.outlier |
    columna.datos < extremo.inferior.outlier
  return (son.outliers.IQR)
}


###########################################################################
# Calcula los outliers IQR con respecto a ALGUNA columna
# Devuelve un vector de claves indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a ALGUNA columna
# coef es 1.5 para los outliers normales y  3 para los outliers extremos

claves_outliers_IQR_en_alguna_columna = function(datos, coef = 1.5){
  df.clave.columnas = data.frame()
  claves.outliers =  sapply(1:ncol(datos), 
                               function(x) claves_outliers_IQR(datos, x, coef)
  )
  claves.outliers.en.alguna.columna = unlist(claves.outliers)
  return (claves.outliers.en.alguna.columna)
}




#######################################################################
# Devuelve los nombres de aquellas filas especificadas en el par?metro claves
# filas es un vector de bools 

nombres_filas = function (datos, claves) {
  num.claves = length(claves)
  nombres.filas = row.names(as.data.frame(datos))[claves]
  
  return (nombres.filas)
}




#######################################################################
# funci?n base para diag_caja_outliers_IQR y diag_caja

diag_caja_grafico_base = function(datos, indice.columna){
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  nombre.columna = colnames(datos)[indice.columna]
  ggboxplot = ggplot(data = as.data.frame(datos), 
                     aes(x=factor(""), 
                         y = datos[,indice.columna]) , 
                     environment = environment()) + 
              xlab(nombre.columna) + ylab("") 
  return (ggboxplot)
}

#######################################################################
# Muestra un diagrama de caja
# Calcula los outliers IQR y los muestra como puntos en rojo en un BoxPlot

diag_caja_outliers_IQR = function (datos, ind.columna, coef.IQR = 1.5){
  # Si quisi?semos l?neas horizontales en los l?mites de las cajas
  # habr?a que a?adir 
  # + stat_boxplot(geom = 'errorbar')   
  
   outliers.IQR = son_outliers_IQR(datos, ind.columna, coef = coef.IQR)
   ggboxplot =  diag_caja_grafico_base(datos, ind.columna) + 
                stat_boxplot(coef = coef.IQR) +
                geom_boxplot(coef = coef.IQR, outlier.colour = "red") 
                # Importante: geom_boxplot debe ir despu?s de stat_boxplot
   
   return (ggboxplot)
}



#######################################################################
# Muestra un diagrama de caja
# Tambi?n muestra las etiquetas de los registros indicados en 
# el par?metro claves.a.mostrar 

diag_caja = function (datos, ind.columna, claves.a.mostrar = c()){
  num.filas = nrow(datos)
  num.claves = length(claves.a.mostrar)
  nombres.filas = vector (mode = "character", length = num.filas)
  nombres.filas = rep("", num.filas)
  nombres.claves = nombres_filas(datos, claves.a.mostrar)

  for (i in num.claves)
    nombres.filas[claves.a.mostrar[i]]  = nombres.claves[i]
  

  ggboxplot = diag_caja_grafico_base(datos, ind.columna) + 
    geom_boxplot(outlier.shape = NA) + # Para que no imprima los outliers IQR calculados dentro del mismo geom_boxplot
    geom_text(aes(label = nombres.filas)) 
  
  return (ggboxplot)
}






#######################################################################
# Muestra de forma conjunta todos los diagramas de caja de las variables de datos
# Para ello, normaliza previamente los datos.
# Tambi?n muestra las etiquetas de los registros indicados en claves.a.mostrar
# Requiere reshape

diag_caja_juntos = function (datos, titulo = "", claves.a.mostrar = c()){  
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  
  # Para hacerlo con ggplot, lamentablemente hay que construir antes una tabla 
  # que contenga en cada fila el valor que a cada tupla le da cada variable 
  # -> paquete reshape->melt
  
  # Por ejemplo, si tenemos el siguiente data frame
  
  # datos = data.frame(
  #   A = c(1, 2),
  #   B = c(3, 4)
  # )
  # datos =
  #     A  B
  #     1  3
  #     2  4
  
  # melt(datos) construye esta tabla:
  
  #      variable value
  # 1        A     1
  # 2        A     2
  # 3        B     3
  # 4        B     4
  
  
  nombres.de.filas = nombres_filas (datos, claves.a.mostrar)
  
  datos = scale(datos)
  datos.melted = melt(datos)
  colnames(datos.melted)[2]="Variables"
  colnames(datos.melted)[3]="zscore"
  factor.melted = colnames(datos.melted)[1]
  columna.factor = as.factor(datos.melted[,factor.melted])
  levels(columna.factor)[!levels(columna.factor) %in% nombres.de.filas] = ""  
  
  ggplot(data = datos.melted, 
         aes(x=Variables, y=zscore), 
         environment = environment()) + 
    ggtitle(titulo) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_text(aes(label = columna.factor), size = 3) 
}






#######################################################################
# Muestra un biplot del conjunto de datos
# Se muestran los nombres de los registros indicados en claves.a.mostrar
# El color usado para dichos registros es el segundo del par?metro colores
# El t?tulo para el grupo de dichos registros es el especificado en titulo.grupo.a.mostrar
# El par?metro titulo especifica el t?tulo principal del gr?fico

biplot_2_colores = function (datos, 
                             claves.a.mostrar = c(), 
                             titulo = "",
                             titulo.grupo.a.mostrar = "Outliers",
                             colores = c("black","red")){
  nombres = rownames(datos)
  claves.datos = c(1:nrow(datos))
  son.a.mostrar = claves.datos %in% claves.a.mostrar
  nombres[!son.a.mostrar] = ''

  PCA.model = princomp(scale(datos))
  outlier.shapes = c(".","x") 
  biplot = ggbiplot(PCA.model,
                    obs.scale = 1,
                    var.scale = 1 ,
                    varname.size = 5,
                    groups =  son.a.mostrar,
                    alpha = 1/2) #alpha = 1/10
  biplot = biplot + labs(color = titulo.grupo.a.mostrar)
  biplot = biplot + scale_color_manual(values = colores)
  biplot = biplot + geom_text(label = nombres,
                              stat = "identity",
                              size = 3,
                              hjust=0,
                              vjust=0)
  biplot = biplot + ggtitle(titulo)
}



#######################################################################
# Muestra un biplot de un conjunto de datos diferenciados por color
# El color lo determina la asignaci?n de cada dato a un cluster 
# Las asignaciones de datos a cluster se indican en asignaciones.clustering
# Tambi?n se muestran los outliers cuyas claves vienen indicadas en claves.outliers
 
biplot_outliers_clustering = function(datos, 
                                      titulo = "Outliers por el m?todo de Clustering", 
                                      titulo.color = "Asignaciones Clustering",
                                      titulo.outlier = "Outliers",
                                      asignaciones.clustering,
                                      claves.outliers){
  son.outliers = rep(FALSE, nrow(datos))
  son.outliers[claves.outliers] = TRUE
  
  bip = biplot_colores_formas(datos, 
                              titulo, titulo.color, titulo.outlier,
                              asignaciones.clustering,
                              son.outliers,
                              claves.outliers)
  bip 
}

#######################################################################
# Muestra un biplot del conjunto de datos
# Los datos se muestran diferenciados por color y por forma
# Las asignaciones de cada dato a su color y forma vienen dadas por los vectores
# asignaciones.colores y asignaciones.formas 
# Tambi?n se muestran las etiquetas de los registros indicados
# en el par?metro opcional claves.a.mostrar 

biplot_colores_formas = function (datos, 
                                  titulo, titulo.color = '', titulo.forma = '', 
                                  asignaciones.colores, asignaciones.formas,
                                  claves.a.mostrar = c()){
  PCA.model = princomp(scale(datos))
  
  son.a.mostrar = rep(FALSE, nrow(datos))
  son.a.mostrar[claves.a.mostrar] = TRUE
  nombres.a.mostrar = rownames(datos)
  nombres.a.mostrar[!son.a.mostrar] = ''

  asignaciones.colores = factor(asignaciones.colores)
  asignaciones.formas  = factor(asignaciones.formas)

  
  bip = ggbiplot(PCA.model, obs.scale = 1, var.scale=1 , varname.size = 3, alpha = 0) +              
    geom_point(aes(shape = asignaciones.formas, colour = asignaciones.colores))  +
    labs(shape = titulo.forma) +
    labs(colour = titulo.color) +
    ggtitle(titulo) +
    geom_text(label = nombres.a.mostrar, stat = "identity", size = 3, hjust=0, vjust=0)      
  
  bip
}

#######################################################################
# Calcula las distancias de cada dato al centroide de su cluster
# Las asignaciones de cada dato a su cluster se indican en asignaciones.clustering
# Cada centroide es una fila del data frame datos.centroides.normalizados

distancias_a_centroides = function (datos.normalizados, 
                                    asignaciones.clustering, 
                                    datos.centroides.normalizados){
  
  sqrt(rowSums(   (datos.normalizados 
                   - 
                   datos.centroides.normalizados[asignaciones.clustering,])^2  ))
}


#######################################################################
# Revierte la funci?n de normalizaci?n (z-score)

desnormaliza = function(datos, filas.normalizadas){
  medias        = colMeans(datos)
  desviaciones  = apply(datos, 2, sd , na.rm = TRUE)
  
  filas.desnormalizadas  = sweep(filas.normalizadas, 2, desviaciones, "*")
  filas.desnormalizadas  = sweep(filas.desnormalizadas, 2, medias, "+")
  
  filas.desnormalizadas 
}




top_clustering_outliers = function(datos.norm, 
                                   asignaciones.clustering, 
                                   datos.centroides.norm, 
                                   num.outliers){
  
  dist_centroides = distancias_a_centroides (datos.norm, 
                                             asignaciones.clustering, 
                                             datos.centroides.norm)
  
  claves = order(dist_centroides, decreasing=T)[1:num.outliers]
  
  list(distancias = dist_centroides[claves]  , claves = claves)
}
```

# Dataset y Selección de Variables

Vamos a usar el conjunto de datos ``wine``, un dataset orientado a la clasificación multiclase de vinos en base a 13 atributos. Estos atributos miden diferentes características químicas, siendo las siguientes:

1) Alcohol
2) Malic acid
3) Ash
4) Alcalinity of ash
5) Magnesium
6) Total phenols
7) Flavanoids
8) Nonflavanoid phenols
9) Proanthocyanins
10) Color intensity
11) Hue
12) OD280/OD315 of diluted wines
13) Proline

Todos las variables on numéricas y miden la cantidad encontrada de cada componente.
En nuestro caso ignoraremos la clase de prediccion y las etiquetas de valores anómalos proporcionadas en el dataset. 
(HACER COMPARATIVA AL FINAL CON LAS ETIQUETAS)

Referencias:
http://odds.cs.stonybrook.edu/wine-dataset/
https://archive.ics.uci.edu/ml/datasets/Wine

```{r}
datos <- readMat("./wine.mat") %>% as.data.frame()
nombres <- c("Alcohol", "Malic", "Ash", "Alcalinity", "Mag", "Phenols", "Flavanoids", "Non-flava", "Proantho", "Color", "Hue", "OD280", "Proline", "Anomaly")
colnames(datos) <- nombres

head(datos)
```

```{r}
# Nos guardamos las etiquetas de si es o no un outlier
etiquetas <- datos[,ncol(datos)]

# Y nos quedamos con las variables numéricas
datos.num <- datos[1:ncol(datos)-1]
```


Medidas estadísticas
```{r}
summary(datos.num)
```

Todas contienen valores reales por lo que no eliminamos ninguna más

Finalmente, eliminamos todas aquellas filas que tengan algún valor nulo (NO HABÍA):
```{r}
datos.num <- na.omit(datos.num)
```

# Detección de outliers en una dimensión
## Outliers IQR

Los métodos IQR teóricamente solo se deben aplicar a distribuciones normales, pero también pueden funcionar si la forma de la distribución no es rara (multimodal, uniforme...).

Mostramos histograma de cada variable
```{r}
par(mfrow = c(2,3))
c(1:ncol(datos.num)) %>% sapply(function(x) hist(datos.num[,x], 
                                                 main="", 
                                                 xlab=names(datos.num)[x]))
```

En general la mayoría de variables parecen seguir distribuciones no muy raras, excepto en X.12 que parece ser bimodal.

Comprobamos normalidad con el test de Shapiro
Lo hacemos aunque haya posibles outliers, solo es para hacernos una idea de si las variables que hemos visto que más se parecen son rechazadas por un test estadístico (posiblemente lo harían por outliers IQR)
```{r}
print("Shapiro test, p-values:")
sapply(datos.num, function(x) {
  shapiro.test(x)$p.value}
)
```

Puesto que en este apartado no vamos a usarlas todas, elegimos las que más se asemejan a una normal, o al menos, las que el test de Shapiro no puede asegurar que no son normales.
Por tanto, nos quedamos con X.8, Alcalinity, X.3, X.1 -> Definitivamente, Alcalinity, la que tiene el p-value más alto
```{r}
indice.columna <- 4
columna        <- datos.num[, indice.columna]
nombre.columna <- names(datos.num) [indice.columna]
```

### Obtención de los outliers IQR

Calculamos los cuartiles
```{r}
cuartil.primero <- quantile(columna, .25, names = F)
cuartil.tercero <- quantile(columna, .75, names = F)
iqr <- IQR(columna)
```

```{r}
cat("Q1: ")
cuartil.primero
cat("\nQ3: ")
cuartil.tercero
cat("\nIQR: ")
iqr
```

Calulamos los extremos
```{r}
extremo.superior.outlier.IQR <- cuartil.tercero + 1.5 * iqr
extremo.inferior.outlier.IQR <- cuartil.primero - 1.5 * iqr
extremo.superior.outlier.IQR.extremo <- cuartil.tercero + 3 * iqr
extremo.inferior.outlier.IQR.extremo <- cuartil.primero - 3 * iqr
```

```{r}
extremo.superior.outlier.IQR
extremo.inferior.outlier.IQR
extremo.superior.outlier.IQR.extremo
extremo.inferior.outlier.IQR.extremo
```


Construimos vectores lógicos indicando si cada instancia es o no un outlier (normal o extremo)
```{r}
son.outliers.IQR <- columna < extremo.inferior.outlier.IQR | columna > extremo.superior.outlier.IQR
son.outliers.IQR.extremos <- columna < extremo.inferior.outlier.IQR.extremo | columna > extremo.superior.outlier.IQR.extremo
```

```{r}
head(son.outliers.IQR)
head(son.outliers.IQR.extremos)
sum(son.outliers.IQR)
sum(son.outliers.IQR.extremos)
```

### Índices y valores de los outliers IQR
```{r}
son.outliers.IQR     = son_outliers_IQR(datos.num, indice.columna)
# head(son.outliers.IQR)

claves.outliers.IQR  = claves_outliers_IQR(datos.num, indice.columna)
claves.outliers.IQR

df.outliers.IQR <- datos.num[claves.outliers.IQR,]
df.outliers.IQR
# nombres.outliers.IQR <- row.names(df.outliers.IQR) 
# valores.outliers.IQR <- columna[claves.outliers.IQR]

son.outliers.IQR.extremos    = son_outliers_IQR(datos.num, indice.columna, 3)
# head(son.outliers.IQR.extremos)

claves.outliers.IQR.extremos = claves_outliers_IQR(datos.num, indice.columna, 3)
claves.outliers.IQR.extremos
```

```{r}
# claves.outliers.IQR
# nombres.outliers.IQR
# valores.outliers.IQR
```

### Desviación de los outliers con respecto a la media de la columna

Si partimos de una variable X cuya distribución no es normal, el método de z-score no obtiene una N(0,1), pero si la distribución de X no es demasiado rara, los datos que así obtengamos nos darán información útil sobre si los registros son usuales o no. Para ilustrarlo, apliquemos el método z-score a la variable mpg. Para ello, usamos la función scale:

```{r}
datos.num.norm = scale(datos.num)
head(datos.num.norm)

columna.norm   = datos.num.norm[, indice.columna]
```

Obtenemos los valores normalizados de los outliers
```{r}
valores.outliers.IQR.norm <- columna.norm[claves.outliers.IQR]

valores.outliers.IQR.norm
```

<!-- Son muy grandes, no? Estan por encima del 99% de lo que sería una normal -->

Vamos a ver ahora el comportamiento de los outliers en la columna seleccionada con respecto al resto de columnas. Para ello, basta con seleccionar los datos correspondientes del conjunto de datos normalizado. En nuestro caso, sólo tenemos un outlier IQR en la columna seleccionada. Nos debe salir lo siguiente:

```{r}
datos.num.norm.outliers.IQR <- datos.num.norm[claves.outliers.IQR,]

datos.num.norm.outliers.IQR
```

<!-- ANALIZAR -->

### Gráfico

Mostramos en un gráfico los valores de los registros respecto a diferentes variables.
```{r}
# plot_2_colores(datos.num.norm[], claves.outliers.IQR)
plot_2_colores(datos.num.norm[,c(4,1)], claves.outliers.IQR)
plot_2_colores(datos.num.norm[,c(4,3)], claves.outliers.IQR)
```

<!-- Con las variables X.1 y Alcalinity se nota como se alejan de la distribución -->

<!-- No tenemos extremos -->

### Diagrama de cajas

```{r}
diag_caja_outliers_IQR(datos.num.norm, 4)
```

```{r}
diag_caja(datos.num.norm, 4, claves.outliers.IQR)
```

En todas las variables
```{r}
diag_caja_juntos(datos.num, "Outliers", claves.outliers.IQR)
```
Vemos que solo la instancia 11 no tiene un valor anormal en al menos un par de variales

## Test de hipótesis

Ya hicimos el test de Shapiro anteriormente, pero podemos ver de forma gráfica que la distribución de la variable elegida 
```{r}
ajusteNormal = fitdist(columna , "norm")
denscomp(ajusteNormal,  xlab = nombre.columna)
```

### Test de Grubs

Sabiendo que la distribución se asemeja a una normal, podemos hacer el test de Grubs sobre el valor más alejado de la media, que sabemos que es ??:
```{r}
test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs$p.value
```

<!-- El p-value es > 0.05, por lo que el test no puede rechazar. Así pues, aunque Toyota Corolla tiene un valor alto en mpg, no podemos deducir que realmente sea un outlier desde el punto de vista estadístico. -->

NO HACE FALTA PORQUE SABEMOS QUE LA INSTANCIA QUE ES
```{r}
# valor.posible.outlier = outlier(columna)
# valor.posible.outlier
```

```{r}
# es.posible.outlier = outlier(columna, logical = TRUE)
# clave.posible.outlier = which( es.posible.outlier == TRUE)
# clave.posible.outlier
```

### Test de Normalidad

```{r}
#######################################################################
# Aplica el test de Grubbs sobre la columna ind.col de datos y devuelve una lista con:

# nombre.columna: Nombre de la columna datos[, ind.col]
# clave.mas.alejado.media: Clave del valor O que está más alejado de la media
# valor.mas.alejado.media: Valor de O en datos[, ind.col]
# nombre.mas.alejado.media: Nombre de O en datos
# es.outlier: TRUE/FALSE dependiendo del resultado del test de Grubbs sobre O
# p.value:  p-value calculado por el test de Grubbs
# es.distrib.norm: Resultado de aplicar el test de Normalidad 
#    de Shapiro-Wilks sobre datos[, ind.col]
#    El test de normalidad se aplica sin tener en cuenta el 
#    valor más alejado de la media (el posible outlier O)
#    TRUE si el test no ha podido rechazar
#       -> Sólo podemos concluir que los datos no contradicen una Normal
#    FALSE si el test rechaza 
#       -> Los datos no siguen una Normal

# Requiere el paquete outliers

test_Grubbs = function(datos, ind.col, alpha = 0.05) {
  columna <- datos[,ind.col]
  res <- list()
  
  # Nombre columna
  res$nombre.columna <- colnames(datos)[ind.col]
    
  # Búsqueda del outlier
  es.posible.outlier <- outlier(columna, logical = TRUE)
  
  res$clave.mas.alejado.media <- which(es.posible.outlier == TRUE)
  res$valor.mas.alejado.media <- outlier(columna)
  res$nombre.mas.alejado.media <- rownames(datos)[res$clave.mas.alejado.media]
  
  # Test de Grubbs
  test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)

  res$es.outlier <- ifelse(test.de.Grubbs$p.value <= alpha, TRUE, FALSE)
  res$p.value <- test.de.Grubbs$p.value

  # Test de normalidad
  test.Normalidad <- shapiro.test(columna[-res$clave.mas.alejado.media])
  res$es.distrib.norm <- ifelse(test.Normalidad$p.value > alpha, TRUE, FALSE)
  
  res
}
```

```{r}
test.Grubbs.datos.num = test_Grubbs(datos.num, indice.columna)

test.Grubbs.datos.num
```
Hacemos notar que al eliminar el outlier al calcular el estadístico de normalidad ahora nos rechaza
<!-- POR QUÉ? -->

## Trabajando con varias columnas

### Outliers IQR

Empezamos con los outliers IQR: vamos a calcular los outliers IQR con respecto a cada una de las columnas. El conjunto de ellos nos dará aquellos registros que son outliers con respecto a alguna columna.
```{r}
claves.outliers.IQR.en.alguna.columna =
  claves_outliers_IQR_en_alguna_columna(datos.num, 1.5)

claves.outliers.IQR.en.mas.de.una.columna = 
  unique(
    claves.outliers.IQR.en.alguna.columna[
      duplicated(claves.outliers.IQR.en.alguna.columna)])
claves.outliers.IQR.en.alguna.columna = 
  unique (claves.outliers.IQR.en.alguna.columna)


claves.outliers.IQR.en.mas.de.una.columna
claves.outliers.IQR.en.alguna.columna 
```

Vamos a ver los valores normalizados de algunos de estos outliers:
```{r}
datos.num.norm[claves.outliers.IQR.en.alguna.columna,] %>% head() %>% as.data.frame()
```

De forma gráfica (como hay muchos cogemos unos pocos)
```{r}
diag_caja_juntos(datos.num.norm, "Outliers en alguna columna", claves.outliers.IQR.en.alguna.columna)
# diag_caja_juntos(datos.num.norm, "Outliers en alguna columna", claves.outliers.IQR.en.alguna.columna %>% head())
```

Notamos valores extremadamente altos en Mg y la instancia 11 que parece tener bajas cantidades de múltiples elementos (Alcalinity, Ash, Proantho...)


### Test de Hipótesis

Test de Grubs
```{r}
par(mfrow = c(2,3))
datos.num %>% apply(2, function(col) {
  ajusteNormal = fitdist(col, "norm")
  denscomp (ajusteNormal,  xlab = "")
})
```

Deberíamos quitar la variable 12 pues parece bimodal
La 2, 10, 13
Ante la duda, también la 8?
```{r}
# QUITAR VARIABLE 12
datos.num.reducidos <- datos.num[,-c(2,8,10,12,13)]
```


<!-- ?? -->
```{r}
sapply(1:ncol(datos.num.reducidos), test_Grubbs, datos=datos.num)
```

# Outliers Multivariantes

## Métodos estadísticos basados en la distancia de Mahalanobis

Para que un par de variables siga una distribución normal conjunta ambas deben estar normalmente distribuídas de forma indiviual.
Como el test de normalidad nos deja solo una (Ash), y para poder hacer el apartado, reducidos el nivel de significación a 0.025 para obtener otra variable más (esto no deberíamos aplicarlo en un problema real)
```{r}
# test <- sapply(1:ncol(datos.num), test_Grubbs, datos=datos.num)
test <- sapply(1:ncol(datos.num), test_Grubbs, alpha=0.025, datos=datos.num)
son.col.normales <- apply(test, 2, function(x) {
  x$es.distrib.norm
})
datos.num.distrib.norm = datos.num[,son.col.normales]

son.col.normales
head(datos.num.distrib.norm)
```

Ahora bien, el que las variables sigan una distribución Normal 1-variante no garantiza que el conjunto de ellas siga una distribución Normal multivariante. Es una condición necesaria pero no suficiente. Por lo tanto, tenemos que lanzar un test de Normalidad multivariante:
```{r}
test.MVN = mvn(datos.num.distrib.norm, mvnTest = "energy")
test.MVN$multivariateNormality["MVN"]
test.MVN$multivariateNormality["p value"]
```

El test multivariante no rechaza la hipótesis nula, por lo que no nos asegura que la distribución conjunta de Ash y Non-flava no es normal. Nuestro test indidual nos había dicho que no lo era, aunque recordamos que si no quitámos el valor más alejado de la media no lo negaba (con este valor MVN hizo el test)

No deberíamos por tanto aplicar este método con distancia de Mahalanobis, pues la condición de que las variables se distribuyan de forma normal era necesaria.

### Tests de hipótesis para detectar outliers

Visualmente
```{r}
corr.plot(datos.num[,3], datos.num[,8])
```

Podemos ver que hay 3 puntos fuera de ??, 2 considerablemente más alejados que el tercero.
También apreciamos que el uso o no de un método robusto no nos va a cambiar los resultados ??

Aplicamos test individual y test múltiple, según las hipótesis
```{r}
set.seed(2)

cerioli.individual <- cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha = 0.05)
claves.test.individual <- which(cerioli.individual$outliers)
nombres.test.individual <- nombres_filas(datos.num.distrib.norm, claves.test.individual)

n <- nrow(datos.num)
alpha <- 0.05
cerioli.interseccion <- cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha = 1 - (1 - alpha)^(1/n))
claves.test.interseccion <- which(cerioli.interseccion$outliers)
nombres.test.interseccion <- nombres_filas(datos.num.distrib.norm, claves.test.individual)

claves.test.individual
# nombres.test.individual
claves.test.interseccion
# nombres.test.interseccion
```

Muestre también un gráfico de todas las distancias de Mahlanobis obtenidas para que aprecie cuál es el mayor valor:
```{r}
cerioli.individual$mahdist.rw %>% sort() %>% plot()
```

Se aprecia los 4 puntos vistos anteriormente

```{r}
clave.mayor.dist.Mah <- order(cerioli.individual$mahdist.rw, decreasing = TRUE)[1]
nombre.mayor.dist.Mah <- nombres_filas(datos.num, clave.mayor.dist.Mah)

cerioli.individual$mahdist
clave.mayor.dist.Mah
# nombre.mayor.dist.Mah
```

<!-- Por lo tanto, podemos concluir que el test individual rechazaría la hipótesis de que el registro con clave 31 (Maserati Bora) no es un outlier. Así pues, lo aceptamos como outlier. Algunas consideraciones: -->

<!-- 1. Recuerde que no hemos podido determinar que la distribución subyacente fuese una Normal, por lo que no tenemos garantía estadística de que, efectivamente, dicho registro sea un outlier (de que provenga de una distribución distinta del resto de los datos). -->

<!-- 2. Bajo la premisa de lo dicho anteriormente, el test individual ha etiquetado al Maserati Bora como un outlier multivariante. Recuerde que dicho registro no fue etiquetado como outlier 1-variante en niguna variable por el test de Grubbs ya que tenía un valor muy alto en dos variables (hp y qsec), aunque no lo suficiente para que fuese un outlier. Sin embargo, al tener el mismo coche dos variables con valores muy altos, el test multivariante sí lo puede considerar como un outlier, ya que se suman las contribuciones de ambas variables. -->


## Visualización de datos con un Biplot

```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num, 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

Al contar con un gran número de variables cuesta ver el gráfico.
Lo primero que debemos notar es que el biplot no explica un alto porcentaje de las varianzas, probablemente por la cantidad de variables.

Si redujéramos:
```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num.reducidos[,-(1:3)], 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

También se ve que una gran cantidad de outliers se ubican con valores altos en alguna columna, alejados del centro.
Por tener un conjunto de variables bastante correladas, estos también "parecen" ser altos en otras columnas ??
```{r}
# corrplot::corrplot(cor(datos.num))
# corrplot::corrplot.mixed(cor(datos.num), tl.pos="lt", upper="circle", mar=c(0,0,1,0))
corrplot::corrplot(cor(datos.num), type="upper")
corrplot::corrplot(cor(datos.num.reducidos), type="upper")
```

## Métodos basados en distancias: LOF

MIRAR NÚMERO DE VECINOS
```{r}
num.vecinos.lof = 5
lof.scores = lofactor(datos.num.norm, k = num.vecinos.lof)
claves.lof.ordenados <- order(lof.scores, decreasing = T)
```

```{r}
plot(sort(lof.scores, decreasing = T))
```

Vemos un conjunto de puntos siguiendo un forma bastante continua, aunque podemos apreciar dos grupos: uno con 6 elementos y otro con 9.

Analizamos este primero
```{r}
num.outliers <- 6

claves.outliers.lof <- order(lof.scores, decreasing = T) %>% head(num.outliers)
# nombres.outliers.lof <- nombres_filas(datos.num.norm, claves.outliers.lof)

claves.outliers.lof
# nombres.outliers.lof
```

Mostramos también los valores normalizados de dichos registros:
```{r}
datos.num.norm[claves.outliers.lof, ]
```

ANALIZAR EL POR QUÉ DE LOS SCORES

USAR LOS REDUCIDOS !!!

Analizamos el registro con mayor score de LOF
```{r}
clave.max.outlier.lof = claves.outliers.lof[1]

colores = rep("black", times = nrow(datos.num.norm))
colores[clave.max.outlier.lof] = "red"
pairs(datos.num.norm, pch = 19,  cex = 0.5, col = colores, lower.panel = NULL)
```


<!-- Una vez que tenemos una idea aproximada, vamos a ver de un forma gráfica la interacción de todas las variables (no sólo 2 a 2) Para ello, usamos un biplot. A diferencia de los diagramas de dispersión, el biplot muestra el comportamiento de los datos con respecto a todas las variables. Sin embargo, la información obtenida no es exacta y es proporcional al porcentaje de variación explicado por las componentes principales. En nuestro ejemplo, la suma de la variabilidad explicada por las dos componentes principales es muy alta (casi un 90%) y por tanto la aproximación es muy buena. -->

Para mostrar el biplot ejecutamos el siguiente código:
```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.norm, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```

El porcentaje explicado es muy BAJO !!

```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.reducidos, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```
El porcentaje explicado es muy BAJO !!
Parece tener una combinación inusual en muchas variables

## Métodos basados en Clustering

### Clustering usando k-means
<!-- A falta de más información, fijamos el número de outliers en 5 y el de clusters en 3. Además, como los resultados del método de clustering k-means dependen de la elección inicial de los centroides, fijamos un valor de la semilla con la función set.seed para que, de esta forma, no varíen los resultados de una ejecución a otra. -->
```{r}
num.outliers = 5
num.clusters = 3
set.seed(2)
```

Modelo k-means:
```{r}
modelo.kmeans <- kmeans(datos.num.norm, num.clusters)
asignaciones.clustering.kmeans <- modelo.kmeans$cluster
centroides.normalizados <- modelo.kmeans$centers

head(asignaciones.clustering.kmeans)
centroides.normalizados
```

Variables desnormalizadas:
```{r}
centroides.desnormalizados = desnormaliza(datos.num, centroides.normalizados)
centroides.desnormalizados
```

```{r}
#######################################################################
# Calcula las distancias de los datos a los centroides
# y se queda con los primeros (tantos como indica num.outliers)
# Devuelve una lista con las claves de dichos registros y las
# correspondientes distancias a sus centroides

 
top_clustering_outliers = function(datos.normalizados, 
                                   asignaciones.clustering, 
                                   datos.centroides.normalizados, 
                                   num.outliers){
  distancias <- distancias_a_centroides(datos.normalizados, 
                                        asignaciones.clustering,
                                        datos.centroides.normalizados)
  claves <- distancias %>% order(decreasing = T) %>% head(num.outliers)
  
  res <- list()
  res$distancias <- distancias %>% sort(decreasing = T) %>% head(num.outliers)
  res$claves <- claves
  res
}
```

Calculamos outliers
```{r}
top.outliers.kmeans = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.kmeans, 
                                              centroides.normalizados, 
                                              num.outliers)
claves.outliers.kmeans = top.outliers.kmeans$claves 
# nombres.outliers.kmeans = nombres_filas(datos.num, claves.outliers.kmeans)
distancias.outliers.centroides = top.outliers.kmeans$distancias

claves.outliers.kmeans
# nombres.outliers.kmeans
distancias.outliers.centroides
```

Biplot outliers y clusters:
```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers k-means",
                           asignaciones.clustering = asignaciones.clustering.kmeans,
                           claves.outliers = claves.outliers.kmeans)
```

Diagrama de cajas
```{r}
diag_caja_juntos(datos.num, "Outliers k-means", claves.outliers.kmeans)
```

ANALIZAR

MIRAR SI SON LOS MISMOS QUE CON LOS MÉTODOS ANTERIORES

### Clustering usando medoides

```{r}
set.seed(2)
matriz.distancias = dist(datos.num.norm)
modelo.pam        = pam(matriz.distancias , k = num.clusters)
```

```{r}
asignaciones.clustering.pam = modelo.pam$clustering
# nombres.medoides = modelo.pam$medoids    
medoides = datos.num[nombres.medoides, ] %>% as.data.frame()
medoides.normalizados = datos.num.norm[nombres.medoides, ] %>% as.data.frame()

# nombres.medoides
medoides
medoides.normalizados
```

Calculamos los top.outliers
```{r}
top.outliers.pam = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.pam, 
                                              medoides.normalizados, 
                                              num.outliers)
claves.outliers.pam = top.outliers.pam$claves 
# nombres.outliers.pam = nombres_filas(datos.num, claves.outliers.pam)

claves.outliers.pam
# nombres.outliers.pam
```

```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers PAM",
                           asignaciones.clustering = asignaciones.clustering.pam,
                           claves.outliers = claves.outliers.pam)
```

### Análisis de los outliers multivariantes puros

- El outlier tiene un valor extremo en alguna variable. Es decir, en una única variable, dicho registro presenta un valor extremo y por tanto esa variable contribuye de forma decisiva en el cómputo global. Puede ser el caso del Merc 230 que era un outlier en qsec.

  También sería el caso del grupo de vehículos Lincoln Continental, Chrysler Imperial, Cadillac Fletwood que eran outliers no sólo en una variable sino en varias (coches muy pesados, con mucha cilindrada y que consumen mucho)

  Estos outliers se sitúan en la periferia del biplot.

- El outlier tiene valores relativamente extremos en más de una variable, aunque no llega a ser un outlier en ninguna de ellas. El efecto sumado de dichas variables hace que el registro sea etiquetado como outlier. Era el caso del Ford Pantera L y Honda Civic.

  Al igual que los outliers anteriores, éstos también suelen situarse en la periferia del biplot.

- El outlier no tiene valores extremos en ninguna variable pero, sin embargo, presenta una combinación inusual de valores de dos o más variables.

  Estos outliers suelen estar en la zona interior del biplot.

- El outlier tiene alguna otra característica que depende del método de detección aplicado. Por ejemplo, los métodos basados en distancia detectan aquellos valores que están aislados del resto de valores. Concretamente, el método LOF tiene en cuenta la densidad relativa de los vecinos próximos. Realmente, podría considerarse que este tipo de outliers aislados son un tipo particular de los anteriores (registros con combinaciones inusuales de variables)

```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.outliers.lof, claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.outliers.lof
claves.outliers.lof.no.IQR
# nombres.outliers.lof.no.IQR
```

<!-- Sería interesante intentar extraer alguna otra información adicional del conjunto de datos, aumentando el número de outliers a estudiar. Si recuerda la gráfica de los scores LOF, había un grupo de 3 registros con mayores valores de score, pero también había otro grupo de 8 registros con scores notablemente superiores al resto. Vamos por tanto a analizar de nuevo los resultados del método LOF aumentando el número total de outliers a 11. Para ello, basta con que seleccione los 11 primeros registros del vector claves.lof.ordenados y calcular de nuevo los que son outliers LOF pero no 1-variantes. Le debe salir lo siguiente (el vector claves.outliers.IQR.en.alguna.columna no cambia): -->
```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.lof.ordenados %>% head(11), claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.lof.ordenados %>% head(11)
claves.outliers.lof.no.IQR
# nombres.outliers.lof.no.IQR
```

Mostramos el biplot de los outliers puros:
```{r}
biplot <- biplot_2_colores(datos.num, 
                           titulo = "Outliers LOF (excluídos los que son IQR)",
                           claves.a.mostrar = claves.outliers.lof.no.IQR)
biplot
```

<!-- Puede apreciar que el Hornet 4 Drive es un outlier muy similar a Valiant, aunque este último tenía un mayor score. El resto de outliers, exceptuando a Ferrari Dino presentan valores extremos en varias variables. Por ejemplo, Camaro Z28 y Duster 360 tienen un valor muy alto en hp y muy bajo en qsec por lo que posiblemente el efecto sumado de ambas variables haya contribuido a que tengan un alto score. Nos fijamos por tanto en el Ferrari Dino que no parece que tenga un valor extremo en ninguna variable (se sitúa en la zona central del biplot). Veamos los datos normalizados: -->
```{r}
datos.num.norm[claves.outliers.lof.no.IQR, ] %>% as.data.frame()
```


# Análisis de resultados












