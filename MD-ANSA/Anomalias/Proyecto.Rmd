---
title: "Practica"
author: "Ignacio Vellido"
date: "12/22/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    toc: true
    highlight: github
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)

library(ggplot2)   # Gr?ficos
library(fitdistrplus)  # Ajuste de una distribuciÃ³n -> denscomp 
library(reshape)   # melt
library(ggbiplot)  # biplot
library(tidyverse)   
library(outliers)  # Grubbs
library(MVN)       # mvn: Test de normalidad multivariante  
library(CerioliOutlierDetection)  #MCD Hardin Rocke
library(mvoutlier) # corr.plot 
library(DMwR)      # lof
library(cluster)   # PAM
library(R.matlab)   # Read .mat files
```

```{r include=FALSE}
# M?ster -> Detecci?n de anomal?as
# Juan Carlos Cubero. Universidad de Granada

###########################################################################
# Funciones utilizadas a lo largo del curso
###########################################################################

# rm(list=ls()) 


###########################################################################
# Realiza un plot de todos los registros
# Permite cambiar el color con el que se visualiza un conjunto de registros. 
# Los registros que se muestran con otro color se especifican en el par?metro
# claves.a.mostrar 

plot_2_colores = function (datos, 
                           claves.a.mostrar, 
                           titulo = "",
                           colores = c("black", "red")){
  
  num.datos = nrow(as.matrix(datos))
  seleccionados =  rep(FALSE, num.datos)
  seleccionados[claves.a.mostrar] = TRUE
  colores.a.mostrar = rep(colores[1], num.datos)
  colores.a.mostrar [seleccionados] = colores[2]
  
  plot(datos, col=colores.a.mostrar, main = titulo)
}



###########################################################################
# Funci?n an?loga a son_outliers_IQR, salvo que devuelve un vector
# de claves en vez de un vector de bools

claves_outliers_IQR = function(datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  son.outliers.IQR = son_outliers_IQR(datos, ind.columna, coef)
  return (which(son.outliers.IQR  == TRUE))
}



###########################################################################
# Calcula los outliers IQR con respecto a una columna 
# Devuelve un vector de bools indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a la columna ind.columna
# coef es 1.5 para los outliers normales y hay que pasarle 3 para los outliers extremos

son_outliers_IQR = function (datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  cuartil.primero = quantile(columna.datos)[2]  
  #quantile[1] es el m?nimo y quantile[5] el m?ximo.
  cuartil.tercero = quantile(columna.datos)[4] 
  iqr = cuartil.tercero - cuartil.primero
  extremo.superior.outlier = (iqr * coef) + cuartil.tercero
  extremo.inferior.outlier = cuartil.primero - (iqr * coef)
  son.outliers.IQR  = columna.datos > extremo.superior.outlier |
    columna.datos < extremo.inferior.outlier
  return (son.outliers.IQR)
}


###########################################################################
# Calcula los outliers IQR con respecto a ALGUNA columna
# Devuelve un vector de claves indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a ALGUNA columna
# coef es 1.5 para los outliers normales y  3 para los outliers extremos

claves_outliers_IQR_en_alguna_columna = function(datos, coef = 1.5){
  df.clave.columnas = data.frame()
  claves.outliers =  sapply(1:ncol(datos), 
                               function(x) claves_outliers_IQR(datos, x, coef)
  )
  claves.outliers.en.alguna.columna = unlist(claves.outliers)
  return (claves.outliers.en.alguna.columna)
}




#######################################################################
# Devuelve los nombres de aquellas filas especificadas en el par?metro claves
# filas es un vector de bools 

nombres_filas = function (datos, claves) {
  num.claves = length(claves)
  nombres.filas = row.names(as.data.frame(datos))[claves]
  
  return (nombres.filas)
}




#######################################################################
# funci?n base para diag_caja_outliers_IQR y diag_caja

diag_caja_grafico_base = function(datos, indice.columna){
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  nombre.columna = colnames(datos)[indice.columna]
  ggboxplot = ggplot(data = as.data.frame(datos), 
                     aes(x=factor(""), 
                         y = datos[,indice.columna]) , 
                     environment = environment()) + 
              xlab(nombre.columna) + ylab("") 
  return (ggboxplot)
}

#######################################################################
# Muestra un diagrama de caja
# Calcula los outliers IQR y los muestra como puntos en rojo en un BoxPlot

diag_caja_outliers_IQR = function (datos, ind.columna, coef.IQR = 1.5){
  # Si quisi?semos l?neas horizontales en los l?mites de las cajas
  # habr?a que a?adir 
  # + stat_boxplot(geom = 'errorbar')   
  
   outliers.IQR = son_outliers_IQR(datos, ind.columna, coef = coef.IQR)
   ggboxplot =  diag_caja_grafico_base(datos, ind.columna) + 
                stat_boxplot(coef = coef.IQR) +
                geom_boxplot(coef = coef.IQR, outlier.colour = "red") 
                # Importante: geom_boxplot debe ir despu?s de stat_boxplot
   
   return (ggboxplot)
}



#######################################################################
# Muestra un diagrama de caja
# Tambi?n muestra las etiquetas de los registros indicados en 
# el par?metro claves.a.mostrar 

diag_caja = function (datos, ind.columna, claves.a.mostrar = c()){
  num.filas = nrow(datos)
  num.claves = length(claves.a.mostrar)
  nombres.filas = vector (mode = "character", length = num.filas)
  nombres.filas = rep("", num.filas)
  nombres.claves = nombres_filas(datos, claves.a.mostrar)

  for (i in num.claves)
    nombres.filas[claves.a.mostrar[i]]  = nombres.claves[i]
  

  ggboxplot = diag_caja_grafico_base(datos, ind.columna) + 
    geom_boxplot(outlier.shape = NA) + # Para que no imprima los outliers IQR calculados dentro del mismo geom_boxplot
    geom_text(aes(label = nombres.filas)) 
  
  return (ggboxplot)
}






#######################################################################
# Muestra de forma conjunta todos los diagramas de caja de las variables de datos
# Para ello, normaliza previamente los datos.
# Tambi?n muestra las etiquetas de los registros indicados en claves.a.mostrar
# Requiere reshape

diag_caja_juntos = function (datos, titulo = "", claves.a.mostrar = c()){  
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  
  # Para hacerlo con ggplot, lamentablemente hay que construir antes una tabla 
  # que contenga en cada fila el valor que a cada tupla le da cada variable 
  # -> paquete reshape->melt
  
  # Por ejemplo, si tenemos el siguiente data frame
  
  # datos = data.frame(
  #   A = c(1, 2),
  #   B = c(3, 4)
  # )
  # datos =
  #     A  B
  #     1  3
  #     2  4
  
  # melt(datos) construye esta tabla:
  
  #      variable value
  # 1        A     1
  # 2        A     2
  # 3        B     3
  # 4        B     4
  
  
  nombres.de.filas = nombres_filas (datos, claves.a.mostrar)
  
  datos = scale(datos)
  datos.melted = melt(datos)
  colnames(datos.melted)[2]="Variables"
  colnames(datos.melted)[3]="zscore"
  factor.melted = colnames(datos.melted)[1]
  columna.factor = as.factor(datos.melted[,factor.melted])
  levels(columna.factor)[!levels(columna.factor) %in% nombres.de.filas] = ""  
  
  ggplot(data = datos.melted, 
         aes(x=Variables, y=zscore), 
         environment = environment()) + 
    ggtitle(titulo) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_text(aes(label = columna.factor), size = 3) 
}






#######################################################################
# Muestra un biplot del conjunto de datos
# Se muestran los nombres de los registros indicados en claves.a.mostrar
# El color usado para dichos registros es el segundo del par?metro colores
# El t?tulo para el grupo de dichos registros es el especificado en titulo.grupo.a.mostrar
# El par?metro titulo especifica el t?tulo principal del gr?fico

biplot_2_colores = function (datos, 
                             claves.a.mostrar = c(), 
                             titulo = "",
                             titulo.grupo.a.mostrar = "Outliers",
                             colores = c("black","red")){
  nombres = rownames(datos)
  claves.datos = c(1:nrow(datos))
  son.a.mostrar = claves.datos %in% claves.a.mostrar
  nombres[!son.a.mostrar] = ''

  PCA.model = princomp(scale(datos))
  outlier.shapes = c(".","x") 
  biplot = ggbiplot(PCA.model,
                    obs.scale = 1,
                    var.scale = 1 ,
                    varname.size = 5,
                    groups =  son.a.mostrar,
                    alpha = 1/2) #alpha = 1/10
  biplot = biplot + labs(color = titulo.grupo.a.mostrar)
  biplot = biplot + scale_color_manual(values = colores)
  biplot = biplot + geom_text(label = nombres,
                              stat = "identity",
                              size = 3,
                              hjust=0,
                              vjust=0)
  biplot = biplot + ggtitle(titulo)
}



#######################################################################
# Muestra un biplot de un conjunto de datos diferenciados por color
# El color lo determina la asignaci?n de cada dato a un cluster 
# Las asignaciones de datos a cluster se indican en asignaciones.clustering
# Tambi?n se muestran los outliers cuyas claves vienen indicadas en claves.outliers
 
biplot_outliers_clustering = function(datos, 
                                      titulo = "Outliers por el m?todo de Clustering", 
                                      titulo.color = "Asignaciones Clustering",
                                      titulo.outlier = "Outliers",
                                      asignaciones.clustering,
                                      claves.outliers){
  son.outliers = rep(FALSE, nrow(datos))
  son.outliers[claves.outliers] = TRUE
  
  bip = biplot_colores_formas(datos, 
                              titulo, titulo.color, titulo.outlier,
                              asignaciones.clustering,
                              son.outliers,
                              claves.outliers)
  bip 
}

#######################################################################
# Muestra un biplot del conjunto de datos
# Los datos se muestran diferenciados por color y por forma
# Las asignaciones de cada dato a su color y forma vienen dadas por los vectores
# asignaciones.colores y asignaciones.formas 
# Tambi?n se muestran las etiquetas de los registros indicados
# en el par?metro opcional claves.a.mostrar 

biplot_colores_formas = function (datos, 
                                  titulo, titulo.color = '', titulo.forma = '', 
                                  asignaciones.colores, asignaciones.formas,
                                  claves.a.mostrar = c()){
  PCA.model = princomp(scale(datos))
  
  son.a.mostrar = rep(FALSE, nrow(datos))
  son.a.mostrar[claves.a.mostrar] = TRUE
  nombres.a.mostrar = rownames(datos)
  nombres.a.mostrar[!son.a.mostrar] = ''

  asignaciones.colores = factor(asignaciones.colores)
  asignaciones.formas  = factor(asignaciones.formas)

  
  bip = ggbiplot(PCA.model, obs.scale = 1, var.scale=1 , varname.size = 3, alpha = 0) +              
    geom_point(aes(shape = asignaciones.formas, colour = asignaciones.colores))  +
    labs(shape = titulo.forma) +
    labs(colour = titulo.color) +
    ggtitle(titulo) +
    geom_text(label = nombres.a.mostrar, stat = "identity", size = 3, hjust=0, vjust=0)      
  
  bip
}

#######################################################################
# Calcula las distancias de cada dato al centroide de su cluster
# Las asignaciones de cada dato a su cluster se indican en asignaciones.clustering
# Cada centroide es una fila del data frame datos.centroides.normalizados

distancias_a_centroides = function (datos.normalizados, 
                                    asignaciones.clustering, 
                                    datos.centroides.normalizados){
  
  sqrt(rowSums(   (datos.normalizados 
                   - 
                   datos.centroides.normalizados[asignaciones.clustering,])^2  ))
}


#######################################################################
# Revierte la funci?n de normalizaci?n (z-score)

desnormaliza = function(datos, filas.normalizadas){
  medias        = colMeans(datos)
  desviaciones  = apply(datos, 2, sd , na.rm = TRUE)
  
  filas.desnormalizadas  = sweep(filas.normalizadas, 2, desviaciones, "*")
  filas.desnormalizadas  = sweep(filas.desnormalizadas, 2, medias, "+")
  
  filas.desnormalizadas 
}




top_clustering_outliers = function(datos.norm, 
                                   asignaciones.clustering, 
                                   datos.centroides.norm, 
                                   num.outliers){
  
  dist_centroides = distancias_a_centroides (datos.norm, 
                                             asignaciones.clustering, 
                                             datos.centroides.norm)
  
  claves = order(dist_centroides, decreasing=T)[1:num.outliers]
  
  list(distancias = dist_centroides[claves]  , claves = claves)
}
```

# Dataset y SelecciÃ³n de Variables

Vamos a usar el conjunto de datos ``wine``, un dataset orientado a la clasificaciÃ³n multiclase de vinos en base a 13 atributos. Estos atributos miden diferentes caracterÃ­sticas quÃ­micas, siendo las siguientes:

1) Alcohol
2) Malic acid
3) Ash
4) Alcalinity of ash
5) Magnesium
6) Total phenols
7) Flavanoids
8) Nonflavanoid phenols
9) Proanthocyanins
10) Color intensity
11) Hue
12) OD280/OD315 of diluted wines
13) Proline

Todos las variables on numÃ©ricas y miden la cantidad encontrada de cada componente.
En nuestro caso ignoraremos la clase de prediccion y las etiquetas de valores anÃ³malos proporcionadas en el dataset. 
(HACER COMPARATIVA AL FINAL CON LAS ETIQUETAS)

Referencias:
http://odds.cs.stonybrook.edu/wine-dataset/
https://archive.ics.uci.edu/ml/datasets/Wine

```{r}
datos <- readMat("./wine.mat") %>% as.data.frame()
nombres <- c("Alcohol", "Malic", "Ash", "Alcalinity", "Mag", "Phenols", "Flavanoids", "Non-flava", "Proantho", "Color", "Hue", "OD280", "Proline", "Anomaly")
colnames(datos) <- nombres

head(datos)
```

```{r}
# Nos guardamos las etiquetas de si es o no un outlier
etiquetas <- datos[,ncol(datos)]

# Y nos quedamos con las variables numÃ©ricas
datos.num <- datos[1:ncol(datos)-1]
```


Medidas estadÃ­sticas
```{r}
summary(datos.num)
```

Todas contienen valores reales por lo que no eliminamos ninguna mÃ¡s

Finalmente, eliminamos todas aquellas filas que tengan algÃºn valor nulo (NO HABÃA):
```{r}
datos.num <- na.omit(datos.num)
```

# DetecciÃ³n de outliers en una dimensiÃ³n
## Outliers IQR

Los mÃ©todos IQR teÃ³ricamente solo se deben aplicar a distribuciones normales, pero tambiÃ©n pueden funcionar si la forma de la distribuciÃ³n no es rara (multimodal, uniforme...).

Mostramos histograma de cada variable
```{r}
par(mfrow = c(2,3))
c(1:ncol(datos.num)) %>% sapply(function(x) hist(datos.num[,x], 
                                                 main="", 
                                                 xlab=names(datos.num)[x]))
```

En general la mayorÃ­a de variables parecen seguir distribuciones no muy raras, excepto en X.12 que parece ser bimodal.

Comprobamos normalidad con el test de Shapiro
Lo hacemos aunque haya posibles outliers, solo es para hacernos una idea de si las variables que hemos visto que mÃ¡s se parecen son rechazadas por un test estadÃ­stico (posiblemente lo harÃ­an por outliers IQR)
```{r}
print("Shapiro test, p-values:")
sapply(datos.num, function(x) {
  shapiro.test(x)$p.value}
)
```

Puesto que en este apartado no vamos a usarlas todas, elegimos las que mÃ¡s se asemejan a una normal, o al menos, las que el test de Shapiro no puede asegurar que no son normales.
Por tanto, nos quedamos con X.8, Alcalinity, X.3, X.1 -> Definitivamente, Alcalinity, la que tiene el p-value mÃ¡s alto
```{r}
indice.columna <- 4
columna        <- datos.num[, indice.columna]
nombre.columna <- names(datos.num) [indice.columna]
```

### ObtenciÃ³n de los outliers IQR

Calculamos los cuartiles
```{r}
cuartil.primero <- quantile(columna, .25, names = F)
cuartil.tercero <- quantile(columna, .75, names = F)
iqr <- IQR(columna)
```

```{r}
cat("Q1: ")
cuartil.primero
cat("\nQ3: ")
cuartil.tercero
cat("\nIQR: ")
iqr
```

Calulamos los extremos
```{r}
extremo.superior.outlier.IQR <- cuartil.tercero + 1.5 * iqr
extremo.inferior.outlier.IQR <- cuartil.primero - 1.5 * iqr
extremo.superior.outlier.IQR.extremo <- cuartil.tercero + 3 * iqr
extremo.inferior.outlier.IQR.extremo <- cuartil.primero - 3 * iqr
```

```{r}
extremo.superior.outlier.IQR
extremo.inferior.outlier.IQR
extremo.superior.outlier.IQR.extremo
extremo.inferior.outlier.IQR.extremo
```


Construimos vectores lÃ³gicos indicando si cada instancia es o no un outlier (normal o extremo)
```{r}
son.outliers.IQR <- columna < extremo.inferior.outlier.IQR | columna > extremo.superior.outlier.IQR
son.outliers.IQR.extremos <- columna < extremo.inferior.outlier.IQR.extremo | columna > extremo.superior.outlier.IQR.extremo
```

```{r}
head(son.outliers.IQR)
head(son.outliers.IQR.extremos)
sum(son.outliers.IQR)
sum(son.outliers.IQR.extremos)
```

### Ãndices y valores de los outliers IQR
```{r}
son.outliers.IQR     = son_outliers_IQR(datos.num, indice.columna)
# head(son.outliers.IQR)

claves.outliers.IQR  = claves_outliers_IQR(datos.num, indice.columna)
claves.outliers.IQR

df.outliers.IQR <- datos.num[claves.outliers.IQR,]
df.outliers.IQR
# nombres.outliers.IQR <- row.names(df.outliers.IQR) 
# valores.outliers.IQR <- columna[claves.outliers.IQR]

son.outliers.IQR.extremos    = son_outliers_IQR(datos.num, indice.columna, 3)
# head(son.outliers.IQR.extremos)

claves.outliers.IQR.extremos = claves_outliers_IQR(datos.num, indice.columna, 3)
claves.outliers.IQR.extremos
```

```{r}
# claves.outliers.IQR
# nombres.outliers.IQR
# valores.outliers.IQR
```

### DesviaciÃ³n de los outliers con respecto a la media de la columna

Si partimos de una variable X cuya distribuciÃ³n no es normal, el mÃ©todo de z-score no obtiene una N(0,1), pero si la distribuciÃ³n de X no es demasiado rara, los datos que asÃ­ obtengamos nos darÃ¡n informaciÃ³n Ãºtil sobre si los registros son usuales o no. Para ilustrarlo, apliquemos el mÃ©todo z-score a la variable mpg. Para ello, usamos la funciÃ³n scale:

```{r}
datos.num.norm = scale(datos.num)
head(datos.num.norm)

columna.norm   = datos.num.norm[, indice.columna]
```

Obtenemos los valores normalizados de los outliers
```{r}
valores.outliers.IQR.norm <- columna.norm[claves.outliers.IQR]

valores.outliers.IQR.norm
```

<!-- Son muy grandes, no? Estan por encima del 99% de lo que serÃ­a una normal -->

Vamos a ver ahora el comportamiento de los outliers en la columna seleccionada con respecto al resto de columnas. Para ello, basta con seleccionar los datos correspondientes del conjunto de datos normalizado. En nuestro caso, sÃ³lo tenemos un outlier IQR en la columna seleccionada. Nos debe salir lo siguiente:

```{r}
datos.num.norm.outliers.IQR <- datos.num.norm[claves.outliers.IQR,]

datos.num.norm.outliers.IQR
```

<!-- ANALIZAR -->

### GrÃ¡fico

Mostramos en un grÃ¡fico los valores de los registros respecto a diferentes variables.
```{r}
# plot_2_colores(datos.num.norm[], claves.outliers.IQR)
plot_2_colores(datos.num.norm[,c(4,1)], claves.outliers.IQR)
plot_2_colores(datos.num.norm[,c(4,3)], claves.outliers.IQR)
```

<!-- Con las variables X.1 y Alcalinity se nota como se alejan de la distribuciÃ³n -->

<!-- No tenemos extremos -->

### Diagrama de cajas

```{r}
diag_caja_outliers_IQR(datos.num.norm, 4)
```

```{r}
diag_caja(datos.num.norm, 4, claves.outliers.IQR)
```

En todas las variables
```{r}
diag_caja_juntos(datos.num, "Outliers", claves.outliers.IQR)
```
Vemos que solo la instancia 11 no tiene un valor anormal en al menos un par de variales

## Test de hipÃ³tesis

Ya hicimos el test de Shapiro anteriormente, pero podemos ver de forma grÃ¡fica que la distribuciÃ³n de la variable elegida 
```{r}
ajusteNormal = fitdist(columna , "norm")
denscomp(ajusteNormal,  xlab = nombre.columna)
```

### Test de Grubs

Sabiendo que la distribuciÃ³n se asemeja a una normal, podemos hacer el test de Grubs sobre el valor mÃ¡s alejado de la media, que sabemos que es ??:
```{r}
test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs$p.value
```

<!-- El p-value es > 0.05, por lo que el test no puede rechazar. AsÃ­ pues, aunque Toyota Corolla tiene un valor alto en mpg, no podemos deducir que realmente sea un outlier desde el punto de vista estadÃ­stico. -->

NO HACE FALTA PORQUE SABEMOS QUE LA INSTANCIA QUE ES
```{r}
# valor.posible.outlier = outlier(columna)
# valor.posible.outlier
```

```{r}
# es.posible.outlier = outlier(columna, logical = TRUE)
# clave.posible.outlier = which( es.posible.outlier == TRUE)
# clave.posible.outlier
```

### Test de Normalidad

```{r}
#######################################################################
# Aplica el test de Grubbs sobre la columna ind.col de datos y devuelve una lista con:

# nombre.columna: Nombre de la columna datos[, ind.col]
# clave.mas.alejado.media: Clave del valor O que estÃ¡ mÃ¡s alejado de la media
# valor.mas.alejado.media: Valor de O en datos[, ind.col]
# nombre.mas.alejado.media: Nombre de O en datos
# es.outlier: TRUE/FALSE dependiendo del resultado del test de Grubbs sobre O
# p.value:  p-value calculado por el test de Grubbs
# es.distrib.norm: Resultado de aplicar el test de Normalidad 
#    de Shapiro-Wilks sobre datos[, ind.col]
#    El test de normalidad se aplica sin tener en cuenta el 
#    valor mÃ¡s alejado de la media (el posible outlier O)
#    TRUE si el test no ha podido rechazar
#       -> SÃ³lo podemos concluir que los datos no contradicen una Normal
#    FALSE si el test rechaza 
#       -> Los datos no siguen una Normal

# Requiere el paquete outliers

test_Grubbs = function(datos, ind.col, alpha = 0.05) {
  columna <- datos[,ind.col]
  res <- list()
  
  # Nombre columna
  res$nombre.columna <- colnames(datos)[ind.col]
    
  # BÃºsqueda del outlier
  es.posible.outlier <- outlier(columna, logical = TRUE)
  
  res$clave.mas.alejado.media <- which(es.posible.outlier == TRUE)
  res$valor.mas.alejado.media <- outlier(columna)
  res$nombre.mas.alejado.media <- rownames(datos)[res$clave.mas.alejado.media]
  
  # Test de Grubbs
  test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)

  res$es.outlier <- ifelse(test.de.Grubbs$p.value <= alpha, TRUE, FALSE)
  res$p.value <- test.de.Grubbs$p.value

  # Test de normalidad
  test.Normalidad <- shapiro.test(columna[-res$clave.mas.alejado.media])
  res$es.distrib.norm <- ifelse(test.Normalidad$p.value > alpha, TRUE, FALSE)
  
  res
}
```

```{r}
test.Grubbs.datos.num = test_Grubbs(datos.num, indice.columna)

test.Grubbs.datos.num
```
Hacemos notar que al eliminar el outlier al calcular el estadÃ­stico de normalidad ahora nos rechaza
<!-- POR QUÃ? -->

## Trabajando con varias columnas

### Outliers IQR

Empezamos con los outliers IQR: vamos a calcular los outliers IQR con respecto a cada una de las columnas. El conjunto de ellos nos darÃ¡ aquellos registros que son outliers con respecto a alguna columna.
```{r}
claves.outliers.IQR.en.alguna.columna =
  claves_outliers_IQR_en_alguna_columna(datos.num, 1.5)

claves.outliers.IQR.en.mas.de.una.columna = 
  unique(
    claves.outliers.IQR.en.alguna.columna[
      duplicated(claves.outliers.IQR.en.alguna.columna)])
claves.outliers.IQR.en.alguna.columna = 
  unique (claves.outliers.IQR.en.alguna.columna)


claves.outliers.IQR.en.mas.de.una.columna
claves.outliers.IQR.en.alguna.columna 
```

Vamos a ver los valores normalizados de algunos de estos outliers:
```{r}
datos.num.norm[claves.outliers.IQR.en.alguna.columna,] %>% head() %>% as.data.frame()
```

De forma grÃ¡fica (como hay muchos cogemos unos pocos)
```{r}
diag_caja_juntos(datos.num.norm, "Outliers en alguna columna", claves.outliers.IQR.en.alguna.columna)
# diag_caja_juntos(datos.num.norm, "Outliers en alguna columna", claves.outliers.IQR.en.alguna.columna %>% head())
```

Notamos valores extremadamente altos en Mg y la instancia 11 que parece tener bajas cantidades de mÃºltiples elementos (Alcalinity, Ash, Proantho...)


### Test de HipÃ³tesis

Test de Grubs
```{r}
par(mfrow = c(2,3))
datos.num %>% apply(2, function(col) {
  ajusteNormal = fitdist(col, "norm")
  denscomp (ajusteNormal,  xlab = "")
})
```

DeberÃ­amos quitar la variable 12 pues parece bimodal
La 2, 10, 13
Ante la duda, tambiÃ©n la 8?
```{r}
# QUITAR VARIABLE 12
datos.num.reducidos <- datos.num[,-c(2,8,10,12,13)]
```


<!-- ?? -->
```{r}
sapply(1:ncol(datos.num.reducidos), test_Grubbs, datos=datos.num)
```

# Outliers Multivariantes

## MÃ©todos estadÃ­sticos basados en la distancia de Mahalanobis

Para que un par de variables siga una distribuciÃ³n normal conjunta ambas deben estar normalmente distribuÃ­das de forma indiviual.
Como el test de normalidad nos deja solo una (Ash), y para poder hacer el apartado, reducidos el nivel de significaciÃ³n a 0.025 para obtener otra variable mÃ¡s (esto no deberÃ­amos aplicarlo en un problema real)
```{r}
# test <- sapply(1:ncol(datos.num), test_Grubbs, datos=datos.num)
test <- sapply(1:ncol(datos.num), test_Grubbs, alpha=0.025, datos=datos.num)
son.col.normales <- apply(test, 2, function(x) {
  x$es.distrib.norm
})
datos.num.distrib.norm = datos.num[,son.col.normales]

son.col.normales
head(datos.num.distrib.norm)
```

Ahora bien, el que las variables sigan una distribuciÃ³n Normal 1-variante no garantiza que el conjunto de ellas siga una distribuciÃ³n Normal multivariante. Es una condiciÃ³n necesaria pero no suficiente. Por lo tanto, tenemos que lanzar un test de Normalidad multivariante:
```{r}
test.MVN = mvn(datos.num.distrib.norm, mvnTest = "energy")
test.MVN$multivariateNormality["MVN"]
test.MVN$multivariateNormality["p value"]
```

El test multivariante no rechaza la hipÃ³tesis nula, por lo que no nos asegura que la distribuciÃ³n conjunta de Ash y Non-flava no es normal. Nuestro test indidual nos habÃ­a dicho que no lo era, aunque recordamos que si no quitÃ¡mos el valor mÃ¡s alejado de la media no lo negaba (con este valor MVN hizo el test)

No deberÃ­amos por tanto aplicar este mÃ©todo con distancia de Mahalanobis, pues la condiciÃ³n de que las variables se distribuyan de forma normal era necesaria.

### Tests de hipÃ³tesis para detectar outliers

Visualmente
```{r}
corr.plot(datos.num[,3], datos.num[,8])
```

Podemos ver que hay 3 puntos fuera de ??, 2 considerablemente mÃ¡s alejados que el tercero.
TambiÃ©n apreciamos que el uso o no de un mÃ©todo robusto no nos va a cambiar los resultados ??

Aplicamos test individual y test mÃºltiple, segÃºn las hipÃ³tesis
```{r}
set.seed(2)

cerioli.individual <- cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha = 0.05)
claves.test.individual <- which(cerioli.individual$outliers)
nombres.test.individual <- nombres_filas(datos.num.distrib.norm, claves.test.individual)

n <- nrow(datos.num)
alpha <- 0.05
cerioli.interseccion <- cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha = 1 - (1 - alpha)^(1/n))
claves.test.interseccion <- which(cerioli.interseccion$outliers)
nombres.test.interseccion <- nombres_filas(datos.num.distrib.norm, claves.test.individual)

claves.test.individual
# nombres.test.individual
claves.test.interseccion
# nombres.test.interseccion
```

Muestre tambiÃ©n un grÃ¡fico de todas las distancias de Mahlanobis obtenidas para que aprecie cuÃ¡l es el mayor valor:
```{r}
cerioli.individual$mahdist.rw %>% sort() %>% plot()
```

Se aprecia los 4 puntos vistos anteriormente

```{r}
clave.mayor.dist.Mah <- order(cerioli.individual$mahdist.rw, decreasing = TRUE)[1]
nombre.mayor.dist.Mah <- nombres_filas(datos.num, clave.mayor.dist.Mah)

cerioli.individual$mahdist
clave.mayor.dist.Mah
# nombre.mayor.dist.Mah
```

<!-- Por lo tanto, podemos concluir que el test individual rechazarÃ­a la hipÃ³tesis de que el registro con clave 31 (Maserati Bora) no es un outlier. AsÃ­ pues, lo aceptamos como outlier. Algunas consideraciones: -->

<!-- 1. Recuerde que no hemos podido determinar que la distribuciÃ³n subyacente fuese una Normal, por lo que no tenemos garantÃ­a estadÃ­stica de que, efectivamente, dicho registro sea un outlier (de que provenga de una distribuciÃ³n distinta del resto de los datos). -->

<!-- 2. Bajo la premisa de lo dicho anteriormente, el test individual ha etiquetado al Maserati Bora como un outlier multivariante. Recuerde que dicho registro no fue etiquetado como outlier 1-variante en niguna variable por el test de Grubbs ya que tenÃ­a un valor muy alto en dos variables (hp y qsec), aunque no lo suficiente para que fuese un outlier. Sin embargo, al tener el mismo coche dos variables con valores muy altos, el test multivariante sÃ­ lo puede considerar como un outlier, ya que se suman las contribuciones de ambas variables. -->


## VisualizaciÃ³n de datos con un Biplot

```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num, 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

Al contar con un gran nÃºmero de variables cuesta ver el grÃ¡fico.
Lo primero que debemos notar es que el biplot no explica un alto porcentaje de las varianzas, probablemente por la cantidad de variables.

Si redujÃ©ramos:
```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num.reducidos[,-(1:3)], 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

TambiÃ©n se ve que una gran cantidad de outliers se ubican con valores altos en alguna columna, alejados del centro.
Por tener un conjunto de variables bastante correladas, estos tambiÃ©n "parecen" ser altos en otras columnas ??
```{r}
# corrplot::corrplot(cor(datos.num))
# corrplot::corrplot.mixed(cor(datos.num), tl.pos="lt", upper="circle", mar=c(0,0,1,0))
corrplot::corrplot(cor(datos.num), type="upper")
corrplot::corrplot(cor(datos.num.reducidos), type="upper")
```

## MÃ©todos basados en distancias: LOF

MIRAR NÃMERO DE VECINOS
```{r}
num.vecinos.lof = 5
lof.scores = lofactor(datos.num.norm, k = num.vecinos.lof)
claves.lof.ordenados <- order(lof.scores, decreasing = T)
```

```{r}
plot(sort(lof.scores, decreasing = T))
```

Vemos un conjunto de puntos siguiendo un forma bastante continua, aunque podemos apreciar dos grupos: uno con 6 elementos y otro con 9.

Analizamos este primero
```{r}
num.outliers <- 6

claves.outliers.lof <- order(lof.scores, decreasing = T) %>% head(num.outliers)
# nombres.outliers.lof <- nombres_filas(datos.num.norm, claves.outliers.lof)

claves.outliers.lof
# nombres.outliers.lof
```

Mostramos tambiÃ©n los valores normalizados de dichos registros:
```{r}
datos.num.norm[claves.outliers.lof, ]
```

ANALIZAR EL POR QUÃ DE LOS SCORES

USAR LOS REDUCIDOS !!!

Analizamos el registro con mayor score de LOF
```{r}
clave.max.outlier.lof = claves.outliers.lof[1]

colores = rep("black", times = nrow(datos.num.norm))
colores[clave.max.outlier.lof] = "red"
pairs(datos.num.norm, pch = 19,  cex = 0.5, col = colores, lower.panel = NULL)
```


<!-- Una vez que tenemos una idea aproximada, vamos a ver de un forma grÃ¡fica la interacciÃ³n de todas las variables (no sÃ³lo 2 a 2) Para ello, usamos un biplot. A diferencia de los diagramas de dispersiÃ³n, el biplot muestra el comportamiento de los datos con respecto a todas las variables. Sin embargo, la informaciÃ³n obtenida no es exacta y es proporcional al porcentaje de variaciÃ³n explicado por las componentes principales. En nuestro ejemplo, la suma de la variabilidad explicada por las dos componentes principales es muy alta (casi un 90%) y por tanto la aproximaciÃ³n es muy buena. -->

Para mostrar el biplot ejecutamos el siguiente cÃ³digo:
```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.norm, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```

El porcentaje explicado es muy BAJO !!

```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.reducidos, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```
El porcentaje explicado es muy BAJO !!
Parece tener una combinaciÃ³n inusual en muchas variables

## MÃ©todos basados en Clustering

### Clustering usando k-means
<!-- A falta de mÃ¡s informaciÃ³n, fijamos el nÃºmero de outliers en 5 y el de clusters en 3. AdemÃ¡s, como los resultados del mÃ©todo de clustering k-means dependen de la elecciÃ³n inicial de los centroides, fijamos un valor de la semilla con la funciÃ³n set.seed para que, de esta forma, no varÃ­en los resultados de una ejecuciÃ³n a otra. -->
```{r}
num.outliers = 5
num.clusters = 3
set.seed(2)
```

Modelo k-means:
```{r}
modelo.kmeans <- kmeans(datos.num.norm, num.clusters)
asignaciones.clustering.kmeans <- modelo.kmeans$cluster
centroides.normalizados <- modelo.kmeans$centers

head(asignaciones.clustering.kmeans)
centroides.normalizados
```

Variables desnormalizadas:
```{r}
centroides.desnormalizados = desnormaliza(datos.num, centroides.normalizados)
centroides.desnormalizados
```

```{r}
#######################################################################
# Calcula las distancias de los datos a los centroides
# y se queda con los primeros (tantos como indica num.outliers)
# Devuelve una lista con las claves de dichos registros y las
# correspondientes distancias a sus centroides

 
top_clustering_outliers = function(datos.normalizados, 
                                   asignaciones.clustering, 
                                   datos.centroides.normalizados, 
                                   num.outliers){
  distancias <- distancias_a_centroides(datos.normalizados, 
                                        asignaciones.clustering,
                                        datos.centroides.normalizados)
  claves <- distancias %>% order(decreasing = T) %>% head(num.outliers)
  
  res <- list()
  res$distancias <- distancias %>% sort(decreasing = T) %>% head(num.outliers)
  res$claves <- claves
  res
}
```

Calculamos outliers
```{r}
top.outliers.kmeans = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.kmeans, 
                                              centroides.normalizados, 
                                              num.outliers)
claves.outliers.kmeans = top.outliers.kmeans$claves 
# nombres.outliers.kmeans = nombres_filas(datos.num, claves.outliers.kmeans)
distancias.outliers.centroides = top.outliers.kmeans$distancias

claves.outliers.kmeans
# nombres.outliers.kmeans
distancias.outliers.centroides
```

Biplot outliers y clusters:
```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers k-means",
                           asignaciones.clustering = asignaciones.clustering.kmeans,
                           claves.outliers = claves.outliers.kmeans)
```

Diagrama de cajas
```{r}
diag_caja_juntos(datos.num, "Outliers k-means", claves.outliers.kmeans)
```

ANALIZAR

MIRAR SI SON LOS MISMOS QUE CON LOS MÃTODOS ANTERIORES

### Clustering usando medoides

```{r}
set.seed(2)
matriz.distancias = dist(datos.num.norm)
modelo.pam        = pam(matriz.distancias , k = num.clusters)
```

```{r}
asignaciones.clustering.pam = modelo.pam$clustering
# nombres.medoides = modelo.pam$medoids    
medoides = datos.num[nombres.medoides, ] %>% as.data.frame()
medoides.normalizados = datos.num.norm[nombres.medoides, ] %>% as.data.frame()

# nombres.medoides
medoides
medoides.normalizados
```

Calculamos los top.outliers
```{r}
top.outliers.pam = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.pam, 
                                              medoides.normalizados, 
                                              num.outliers)
claves.outliers.pam = top.outliers.pam$claves 
# nombres.outliers.pam = nombres_filas(datos.num, claves.outliers.pam)

claves.outliers.pam
# nombres.outliers.pam
```

```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers PAM",
                           asignaciones.clustering = asignaciones.clustering.pam,
                           claves.outliers = claves.outliers.pam)
```

### AnÃ¡lisis de los outliers multivariantes puros

- El outlier tiene un valor extremo en alguna variable. Es decir, en una Ãºnica variable, dicho registro presenta un valor extremo y por tanto esa variable contribuye de forma decisiva en el cÃ³mputo global. Puede ser el caso del Merc 230 que era un outlier en qsec.

  TambiÃ©n serÃ­a el caso del grupo de vehÃ­culos Lincoln Continental, Chrysler Imperial, Cadillac Fletwood que eran outliers no sÃ³lo en una variable sino en varias (coches muy pesados, con mucha cilindrada y que consumen mucho)

  Estos outliers se sitÃºan en la periferia del biplot.

- El outlier tiene valores relativamente extremos en mÃ¡s de una variable, aunque no llega a ser un outlier en ninguna de ellas. El efecto sumado de dichas variables hace que el registro sea etiquetado como outlier. Era el caso del Ford Pantera L y Honda Civic.

  Al igual que los outliers anteriores, Ã©stos tambiÃ©n suelen situarse en la periferia del biplot.

- El outlier no tiene valores extremos en ninguna variable pero, sin embargo, presenta una combinaciÃ³n inusual de valores de dos o mÃ¡s variables.

  Estos outliers suelen estar en la zona interior del biplot.

- El outlier tiene alguna otra caracterÃ­stica que depende del mÃ©todo de detecciÃ³n aplicado. Por ejemplo, los mÃ©todos basados en distancia detectan aquellos valores que estÃ¡n aislados del resto de valores. Concretamente, el mÃ©todo LOF tiene en cuenta la densidad relativa de los vecinos prÃ³ximos. Realmente, podrÃ­a considerarse que este tipo de outliers aislados son un tipo particular de los anteriores (registros con combinaciones inusuales de variables)

```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.outliers.lof, claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.outliers.lof
claves.outliers.lof.no.IQR
# nombres.outliers.lof.no.IQR
```

<!-- SerÃ­a interesante intentar extraer alguna otra informaciÃ³n adicional del conjunto de datos, aumentando el nÃºmero de outliers a estudiar. Si recuerda la grÃ¡fica de los scores LOF, habÃ­a un grupo de 3 registros con mayores valores de score, pero tambiÃ©n habÃ­a otro grupo de 8 registros con scores notablemente superiores al resto. Vamos por tanto a analizar de nuevo los resultados del mÃ©todo LOF aumentando el nÃºmero total de outliers a 11. Para ello, basta con que seleccione los 11 primeros registros del vector claves.lof.ordenados y calcular de nuevo los que son outliers LOF pero no 1-variantes. Le debe salir lo siguiente (el vector claves.outliers.IQR.en.alguna.columna no cambia): -->
```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.lof.ordenados %>% head(11), claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.lof.ordenados %>% head(11)
claves.outliers.lof.no.IQR
# nombres.outliers.lof.no.IQR
```

Mostramos el biplot de los outliers puros:
```{r}
biplot <- biplot_2_colores(datos.num, 
                           titulo = "Outliers LOF (excluÃ­dos los que son IQR)",
                           claves.a.mostrar = claves.outliers.lof.no.IQR)
biplot
```

<!-- Puede apreciar que el Hornet 4 Drive es un outlier muy similar a Valiant, aunque este Ãºltimo tenÃ­a un mayor score. El resto de outliers, exceptuando a Ferrari Dino presentan valores extremos en varias variables. Por ejemplo, Camaro Z28 y Duster 360 tienen un valor muy alto en hp y muy bajo en qsec por lo que posiblemente el efecto sumado de ambas variables haya contribuido a que tengan un alto score. Nos fijamos por tanto en el Ferrari Dino que no parece que tenga un valor extremo en ninguna variable (se sitÃºa en la zona central del biplot). Veamos los datos normalizados: -->
```{r}
datos.num.norm[claves.outliers.lof.no.IQR, ] %>% as.data.frame()
```


# AnÃ¡lisis de resultados












