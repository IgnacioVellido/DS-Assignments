---
title: "Parte práctica: Detección de Anomalías"
author: "Ignacio Vellido Expósito"
date: "22/12/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    toc: true
    highlight: github
    df_print: paged
    number_sections: true
---

<style>
.entry-content {
    width: 95%;
    max-width: unset;
}
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

```{r setup, include=FALSE}
options(width = 90)
knitr::opts_chunk$set(echo = FALSE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE, size="small")

library(ggplot2)   # Gr?ficos
library(fitdistrplus)  # Ajuste de una distribución -> denscomp 
library(reshape)   # melt
library(ggbiplot)  # biplot
library(tidyverse)   
library(outliers)  # Grubbs
library(MVN)       # mvn: Test de normalidad multivariante  
library(CerioliOutlierDetection)  #MCD Hardin Rocke
library(mvoutlier) # corr.plot 
library(DMwR)      # lof
library(cluster)   # PAM
library(R.matlab)   # Read .mat files
```

```{r include=FALSE}
# M?ster -> Detecci?n de anomal?as
# Juan Carlos Cubero. Universidad de Granada

###########################################################################
# Funciones utilizadas a lo largo del curso
###########################################################################

# rm(list=ls()) 


###########################################################################
# Realiza un plot de todos los registros
# Permite cambiar el color con el que se visualiza un conjunto de registros. 
# Los registros que se muestran con otro color se especifican en el par?metro
# claves.a.mostrar 

plot_2_colores = function (datos, 
                           claves.a.mostrar, 
                           titulo = "",
                           colores = c("black", "red")){
  
  num.datos = nrow(as.matrix(datos))
  seleccionados =  rep(FALSE, num.datos)
  seleccionados[claves.a.mostrar] = TRUE
  colores.a.mostrar = rep(colores[1], num.datos)
  colores.a.mostrar [seleccionados] = colores[2]
  
  plot(datos, col=colores.a.mostrar, main = titulo)
}



###########################################################################
# Funci?n an?loga a son_outliers_IQR, salvo que devuelve un vector
# de claves en vez de un vector de bools

claves_outliers_IQR = function(datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  son.outliers.IQR = son_outliers_IQR(datos, ind.columna, coef)
  return (which(son.outliers.IQR  == TRUE))
}



###########################################################################
# Calcula los outliers IQR con respecto a una columna 
# Devuelve un vector de bools indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a la columna ind.columna
# coef es 1.5 para los outliers normales y hay que pasarle 3 para los outliers extremos

son_outliers_IQR = function (datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  cuartil.primero = quantile(columna.datos)[2]  
  #quantile[1] es el m?nimo y quantile[5] el m?ximo.
  cuartil.tercero = quantile(columna.datos)[4] 
  iqr = cuartil.tercero - cuartil.primero
  extremo.superior.outlier = (iqr * coef) + cuartil.tercero
  extremo.inferior.outlier = cuartil.primero - (iqr * coef)
  son.outliers.IQR  = columna.datos > extremo.superior.outlier |
    columna.datos < extremo.inferior.outlier
  return (son.outliers.IQR)
}


###########################################################################
# Calcula los outliers IQR con respecto a ALGUNA columna
# Devuelve un vector de claves indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a ALGUNA columna
# coef es 1.5 para los outliers normales y  3 para los outliers extremos

claves_outliers_IQR_en_alguna_columna = function(datos, coef = 1.5){
  df.clave.columnas = data.frame()
  claves.outliers =  sapply(1:ncol(datos), 
                               function(x) claves_outliers_IQR(datos, x, coef)
  )
  claves.outliers.en.alguna.columna = unlist(claves.outliers)
  return (claves.outliers.en.alguna.columna)
}




#######################################################################
# Devuelve los nombres de aquellas filas especificadas en el par?metro claves
# filas es un vector de bools 

nombres_filas = function (datos, claves) {
  num.claves = length(claves)
  nombres.filas = row.names(as.data.frame(datos))[claves]
  
  return (nombres.filas)
}




#######################################################################
# funci?n base para diag_caja_outliers_IQR y diag_caja

diag_caja_grafico_base = function(datos, indice.columna){
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  nombre.columna = colnames(datos)[indice.columna]
  ggboxplot = ggplot(data = as.data.frame(datos), 
                     aes(x=factor(""), 
                         y = datos[,indice.columna]) , 
                     environment = environment()) + 
              xlab(nombre.columna) + ylab("") 
  return (ggboxplot)
}

#######################################################################
# Muestra un diagrama de caja
# Calcula los outliers IQR y los muestra como puntos en rojo en un BoxPlot

diag_caja_outliers_IQR = function (datos, ind.columna, coef.IQR = 1.5){
  # Si quisi?semos l?neas horizontales en los l?mites de las cajas
  # habr?a que a?adir 
  # + stat_boxplot(geom = 'errorbar')   
  
   outliers.IQR = son_outliers_IQR(datos, ind.columna, coef = coef.IQR)
   ggboxplot =  diag_caja_grafico_base(datos, ind.columna) + 
                stat_boxplot(coef = coef.IQR) +
                geom_boxplot(coef = coef.IQR, outlier.colour = "red") 
                # Importante: geom_boxplot debe ir despu?s de stat_boxplot
   
   return (ggboxplot)
}



#######################################################################
# Muestra un diagrama de caja
# Tambi?n muestra las etiquetas de los registros indicados en 
# el par?metro claves.a.mostrar 

diag_caja = function (datos, ind.columna, claves.a.mostrar = c()){
  num.filas = nrow(datos)
  num.claves = length(claves.a.mostrar)
  nombres.filas = vector (mode = "character", length = num.filas)
  nombres.filas = rep("", num.filas)
  nombres.claves = nombres_filas(datos, claves.a.mostrar)

  for (i in 1:num.claves)
    nombres.filas[claves.a.mostrar[i]]  = nombres.claves[i]
  

  ggboxplot = diag_caja_grafico_base(datos, ind.columna) + 
    geom_boxplot(outlier.shape = NA) + # Para que no imprima los outliers IQR calculados dentro del mismo geom_boxplot
    geom_text(aes(label = nombres.filas)) 
  
  return (ggboxplot)
}






#######################################################################
# Muestra de forma conjunta todos los diagramas de caja de las variables de datos
# Para ello, normaliza previamente los datos.
# Tambi?n muestra las etiquetas de los registros indicados en claves.a.mostrar
# Requiere reshape

diag_caja_juntos = function (datos, titulo = "", claves.a.mostrar = c()){  
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  
  # Para hacerlo con ggplot, lamentablemente hay que construir antes una tabla 
  # que contenga en cada fila el valor que a cada tupla le da cada variable 
  # -> paquete reshape->melt
  
  # Por ejemplo, si tenemos el siguiente data frame
  
  # datos = data.frame(
  #   A = c(1, 2),
  #   B = c(3, 4)
  # )
  # datos =
  #     A  B
  #     1  3
  #     2  4
  
  # melt(datos) construye esta tabla:
  
  #      variable value
  # 1        A     1
  # 2        A     2
  # 3        B     3
  # 4        B     4
  
  
  nombres.de.filas = nombres_filas (datos, claves.a.mostrar)
  
  datos = scale(datos)
  datos.melted = melt(datos)
  colnames(datos.melted)[2]="Variables"
  colnames(datos.melted)[3]="zscore"
  factor.melted = colnames(datos.melted)[1]
  columna.factor = as.factor(datos.melted[,factor.melted])
  levels(columna.factor)[!levels(columna.factor) %in% nombres.de.filas] = ""  
  
  ggplot(data = datos.melted, 
         aes(x=Variables, y=zscore), 
         environment = environment()) + 
    ggtitle(titulo) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_text(aes(label = columna.factor), size = 3) 
}






#######################################################################
# Muestra un biplot del conjunto de datos
# Se muestran los nombres de los registros indicados en claves.a.mostrar
# El color usado para dichos registros es el segundo del par?metro colores
# El t?tulo para el grupo de dichos registros es el especificado en titulo.grupo.a.mostrar
# El par?metro titulo especifica el t?tulo principal del gr?fico

biplot_2_colores = function (datos, 
                             claves.a.mostrar = c(), 
                             titulo = "",
                             titulo.grupo.a.mostrar = "Outliers",
                             colores = c("black","red")){
  nombres = rownames(datos)
  claves.datos = c(1:nrow(datos))
  son.a.mostrar = claves.datos %in% claves.a.mostrar
  nombres[!son.a.mostrar] = ''

  PCA.model = princomp(scale(datos))
  outlier.shapes = c(".","x") 
  biplot = ggbiplot(PCA.model,
                    obs.scale = 1,
                    var.scale = 1 ,
                    varname.size = 5,
                    groups =  son.a.mostrar,
                    alpha = 1/2) #alpha = 1/10
  biplot = biplot + labs(color = titulo.grupo.a.mostrar)
  biplot = biplot + scale_color_manual(values = colores)
  biplot = biplot + geom_text(label = nombres,
                              stat = "identity",
                              size = 3,
                              hjust=0,
                              vjust=0)
  biplot = biplot + ggtitle(titulo)
}



#######################################################################
# Muestra un biplot de un conjunto de datos diferenciados por color
# El color lo determina la asignaci?n de cada dato a un cluster 
# Las asignaciones de datos a cluster se indican en asignaciones.clustering
# Tambi?n se muestran los outliers cuyas claves vienen indicadas en claves.outliers
 
biplot_outliers_clustering = function(datos, 
                                      titulo = "Outliers por el m?todo de Clustering", 
                                      titulo.color = "Asignaciones Clustering",
                                      titulo.outlier = "Outliers",
                                      asignaciones.clustering,
                                      claves.outliers){
  son.outliers = rep(FALSE, nrow(datos))
  son.outliers[claves.outliers] = TRUE
  
  bip = biplot_colores_formas(datos, 
                              titulo, titulo.color, titulo.outlier,
                              asignaciones.clustering,
                              son.outliers,
                              claves.outliers)
  bip 
}

#######################################################################
# Muestra un biplot del conjunto de datos
# Los datos se muestran diferenciados por color y por forma
# Las asignaciones de cada dato a su color y forma vienen dadas por los vectores
# asignaciones.colores y asignaciones.formas 
# Tambi?n se muestran las etiquetas de los registros indicados
# en el par?metro opcional claves.a.mostrar 

biplot_colores_formas = function (datos, 
                                  titulo, titulo.color = '', titulo.forma = '', 
                                  asignaciones.colores, asignaciones.formas,
                                  claves.a.mostrar = c()){
  PCA.model = princomp(scale(datos))
  
  son.a.mostrar = rep(FALSE, nrow(datos))
  son.a.mostrar[claves.a.mostrar] = TRUE
  nombres.a.mostrar = rownames(datos)
  nombres.a.mostrar[!son.a.mostrar] = ''

  asignaciones.colores = factor(asignaciones.colores)
  asignaciones.formas  = factor(asignaciones.formas)

  
  bip = ggbiplot(PCA.model, obs.scale = 1, var.scale=1 , varname.size = 3, alpha = 0) +              
    geom_point(aes(shape = asignaciones.formas, colour = asignaciones.colores))  +
    labs(shape = titulo.forma) +
    labs(colour = titulo.color) +
    ggtitle(titulo) +
    geom_text(label = nombres.a.mostrar, stat = "identity", size = 3, hjust=0, vjust=0)      
  
  bip
}

#######################################################################
# Calcula las distancias de cada dato al centroide de su cluster
# Las asignaciones de cada dato a su cluster se indican en asignaciones.clustering
# Cada centroide es una fila del data frame datos.centroides.normalizados

distancias_a_centroides = function (datos.normalizados, 
                                    asignaciones.clustering, 
                                    datos.centroides.normalizados){
  
  sqrt(rowSums(   (datos.normalizados 
                   - 
                   datos.centroides.normalizados[asignaciones.clustering,])^2  ))
}


#######################################################################
# Revierte la funci?n de normalizaci?n (z-score)

desnormaliza = function(datos, filas.normalizadas){
  medias        = colMeans(datos)
  desviaciones  = apply(datos, 2, sd , na.rm = TRUE)
  
  filas.desnormalizadas  = sweep(filas.normalizadas, 2, desviaciones, "*")
  filas.desnormalizadas  = sweep(filas.desnormalizadas, 2, medias, "+")
  
  filas.desnormalizadas 
}




top_clustering_outliers = function(datos.norm, 
                                   asignaciones.clustering, 
                                   datos.centroides.norm, 
                                   num.outliers){
  
  dist_centroides = distancias_a_centroides (datos.norm, 
                                             asignaciones.clustering, 
                                             datos.centroides.norm)
  
  claves = order(dist_centroides, decreasing=T)[1:num.outliers]
  
  list(distancias = dist_centroides[claves]  , claves = claves)
}
```

# Introducción

Vamos a usar el conjunto de datos ``wine``, un dataset orientado a la clasificación multiclase de diferentes vinos en base a 13 atributos. Estos atributos miden las cantidades presentes de diferentes componentes químicos:

1) Alcohol
2) Malic acid
3) Ash
4) Alcalinity of ash
5) Magnesium
6) Total phenols
7) Flavanoids
8) Nonflavanoid phenols
9) Proanthocyanins
10) Color intensity
11) Hue
12) OD280/OD315 of diluted wines
13) Proline

Todos las variables son numéricas con valores reales.
En nuestro caso ignoraremos la clase de prediccion y las etiquetas de valores anómalos proporcionadas en el dataset.

Referencias:

http://odds.cs.stonybrook.edu/wine-dataset/

https://archive.ics.uci.edu/ml/datasets/Wine

***
__NOTA__: Es necesario mencionar que cada vino distinto (cada instancia) únicamente viene identificada por su posición en la tabla. Es decir, no tenemos un nombre o algún identificador para referenciarla.
Por tanto, cuando hablemos de instancias concretas durante la práctica se mencionará únicamente la "clave" del vino en cuestión y no su nombre.
Esto implica que en los apartados que tenían la etiqueta ``# COMPLETAR`` se incluirá el cálculo de los nombres pero aparecerá su salida comentada.

***

# Selección de Variables

El conjunto de datos inicial tiene la siguiente forma:
```{r}
datos <- readMat("./wine.mat") %>% as.data.frame()
nombres <- c("Alcohol", "Malic", "Ash", "Alcalinity", "Mag", "Phenols", "Flavanoids", "Non-flava", "Proantho", "Color", "Hue", "OD280", "Proline", "Anomaly")
colnames(datos) <- nombres

head(datos)
```

```{r}
# Nos guardamos las etiquetas de si es o no un outlier
etiquetas <- datos[,ncol(datos)]

# Y nos quedamos con las variables numéricas
datos.num <- datos[1:ncol(datos)-1]
```


Y las medidas estadísticas:
```{r}
summary(datos.num)
```

Aunque contamos con reales, mostramos también la ocurrencia de cada valor:
```{r}
datos.num %>% apply(2, table)
```


```{r}
datos.num <- na.omit(datos.num)
```


Vemos que cada columna contiene variedad suficiente y sin valores nulos, por lo que no eliminamos ninguna.

# Detección de outliers en una dimensión
## Outliers IQR

Los métodos IQR teóricamente solo se deben aplicar a distribuciones normales, pero también pueden funcionar si la forma de la distribución no es rara (multimodal, uniforme...).

De cara a elegir aquellas variables, mostramos un histograma de cada una:
```{r  echo=TRUE}
par(mfrow = c(2,3))
c(1:ncol(datos.num)) %>% sapply(function(x) hist(datos.num[,x], 
                                                 main="", 
                                                 xlab=names(datos.num)[x])) -> c
```

En general la mayoría de variables parecen seguir distribuciones no muy raras, excepto ``OD280`` que parece ser bimodal.

De cara a elegir una de estas variables, y únicamente por tener un criterio más de desempate, aplicamos un test de normalidad a cada una.
<!-- INCLUIR -->
```{r echo=TRUE}
print("Shapiro test, p-values:")
sapply(datos.num, function(x) {
  shapiro.test(x)$p.value}
)
```

Vemos que existen varias a las que el test no rechaza. De aquellas, elegimos la que obtiene mayor p-value (``Alcalinity``), que vimos en el histograma que se asemeja bastante a una normal.

```{r}
indice.columna <- 4
columna        <- datos.num[, indice.columna]
nombre.columna <- names(datos.num) [indice.columna]
```

### Obtención de los outliers IQR

Calculamos los cuartiles:
<!-- INCLUIR -->
```{r echo=TRUE}
cuartil.primero <- quantile(columna, .25, names = F)
cuartil.tercero <- quantile(columna, .75, names = F)
iqr <- IQR(columna)
```

```{r}
cat("Q1: ")
cuartil.primero
cat("\nQ3: ")
cuartil.tercero
cat("\nIQR: ")
iqr
```

Y calculamos los extremos:
<!-- INCLUIR -->
```{r echo=TRUE}
extremo.superior.outlier.IQR <- cuartil.tercero + 1.5 * iqr
extremo.inferior.outlier.IQR <- cuartil.primero - 1.5 * iqr
extremo.superior.outlier.IQR.extremo <- cuartil.tercero + 3 * iqr
extremo.inferior.outlier.IQR.extremo <- cuartil.primero - 3 * iqr
```

```{r}
cat("Extremo superior: ")
extremo.superior.outlier.IQR
cat("Extremo inferior: ")
extremo.inferior.outlier.IQR
cat("Extremo superior extremo: ")
extremo.superior.outlier.IQR.extremo
cat("Extremo inferior extremo: ")
extremo.inferior.outlier.IQR.extremo
```


Construimos vectores lógicos indicando si cada instancia es o no un outlier (normal o extremo):
<!-- INCLUIR -->
```{r echo=TRUE}
son.outliers.IQR <- columna < extremo.inferior.outlier.IQR | columna > extremo.superior.outlier.IQR
son.outliers.IQR.extremos <- columna < extremo.inferior.outlier.IQR.extremo | columna > extremo.superior.outlier.IQR.extremo
```

```{r echo=T}
head(son.outliers.IQR)
head(son.outliers.IQR.extremos)
sum(son.outliers.IQR)
sum(son.outliers.IQR.extremos)
```

### Índices y valores de los outliers IQR

Obtenemos ahora los índices de las instancias que se detectan como outliers con el método IQR
<!-- INCLUIR -->
```{r echo=TRUE}
claves.outliers.IQR <- which(son.outliers.IQR)
df.outliers.IQR <- datos.num[claves.outliers.IQR,]
nombres.outliers.IQR <- row.names(df.outliers.IQR) 
valores.outliers.IQR <- columna[claves.outliers.IQR]

claves.outliers.IQR.extremos <- which(son.outliers.IQR.extremos)
df.outliers.IQR.extremos <- datos.num[claves.outliers.IQR.extremos,]
nombres.outliers.IQR.extremos <- row.names(df.outliers.IQR.extremos) 
valores.outliers.IQR.extremos <- columna[claves.outliers.IQR.extremos]
```

```{r echo=T, results="default"}
claves.outliers.IQR
df.outliers.IQR
# nombres.outliers.IQR
valores.outliers.IQR
```

```{r echo=TRUE, results="default"}
claves.outliers.IQR.extremos
df.outliers.IQR.extremos
# nombres.outliers.IQR.extremos
valores.outliers.IQR.extremos
```

### Cómputo de los outliers IQR con funciones

La misma información anterior podemos calcularlas con las funciones proporcionadas:
```{r}
son.outliers.IQR     = son_outliers_IQR(datos.num, indice.columna)
# head(son.outliers.IQR)

claves.outliers.IQR  = claves_outliers_IQR(datos.num, indice.columna)
claves.outliers.IQR

df.outliers.IQR <- datos.num[claves.outliers.IQR,]
df.outliers.IQR
# nombres.outliers.IQR <- row.names(df.outliers.IQR) 
# valores.outliers.IQR <- columna[claves.outliers.IQR]

son.outliers.IQR.extremos    = son_outliers_IQR(datos.num, indice.columna, 3)
# head(son.outliers.IQR.extremos)

claves.outliers.IQR.extremos = claves_outliers_IQR(datos.num, indice.columna, 3)
claves.outliers.IQR.extremos
```

```{r}
# claves.outliers.IQR
# nombres.outliers.IQR
# valores.outliers.IQR
```

### Desviación de los outliers con respecto a la media de la columna

Calcularemos ahora la desviación de cada outlier respecto a la media de su columna.

Para ello, aplicaremos el método z-score para aproximar la distribución a una N(0,1) y observaremos los valores de cada outlier en la variable seleccionada (``Alcalinity``).
Dataset normalizado:
```{r}
datos.num.norm = scale(datos.num)
head(datos.num.norm)

columna.norm   = datos.num.norm[, indice.columna]
```

Mostramos primeramente un gráfico de la variable:
```{r}
ajusteNormal = fitdist(columna , "norm")
denscomp (ajusteNormal,  xlab = nombre.columna)
```


Obtenemos los valores normalizados de los outliers:
<!-- INCLUIR -->
```{r echo=TRUE}
valores.outliers.IQR.norm <- columna.norm[claves.outliers.IQR]
names(valores.outliers.IQR.norm) <- claves.outliers.IQR

valores.outliers.IQR.norm
```

Apreciamos que las instancias 11 y 25 toman valores mucho más alejados que 73 y 79.
Aún así, ninguno llega a considerarse extremo en una N(0,1) puesto que no son más grandes que ±4.69.
Hacemos notar también que si la variable Alcalinity se aproximara perfectamente a una N(0,1) las instancias 73 y 79 no se considerarían outliers, por estar por debajo del umbral ±2.68.

Mostramos ahora el comportamiento de estos outliers con el resto de columnas (normalizadas):
<!-- INCLUIR -->
```{r echo=TRUE}
datos.num.norm.outliers.IQR <- datos.num.norm[claves.outliers.IQR,]
rownames(datos.num.norm.outliers.IQR) <- claves.outliers.IQR

datos.num.norm.outliers.IQR %>% as.data.frame()
```

Vemos que las instancias 11 y 73 tienen adicionalmente un valor extremo en la columnas ``Ash``, la 25 obtiene otro en ``Mag`` y la 79 es la única que parece tener un valor excesivamente grande en una sola columna (no lo podemos afirmar de forma estadística puesto que no sabemos si el resto de variables provienen de una normal).

### Gráficos

Mostramos en un gráfico los valores de los registros respecto a diferentes variables, concretamente Alcalinity-Alcohol, Alcalinity-Ash y Alcalinity-Mag.
<!-- INCLUIR -->
```{r echo=TRUE}
plot_2_colores(datos.num.norm[,c(4,1)], claves.outliers.IQR)
plot_2_colores(datos.num.norm[,c(4,3)], claves.outliers.IQR)
plot_2_colores(datos.num.norm[,c(4,5)], claves.outliers.IQR)
```

Tal y como habíamos visto, en las tres gráficas los outliers se ubican alejados de la nube de puntos, 3 de ellos por arriba en el Alcalinity y 1 por debajo.

No tenemos valores extremos, pero si los tuviéramos los mostraríamos con la sentencia:
<!-- INCLUIR -->
```{r echo=TRUE}
plot_2_colores(datos.num.norm[,c(4,1)], claves.outliers.IQR.extremos)
```

### Diagrama de cajas

Mostramos diagramas de cajas con los outliers destacados:
<!-- INCLUIR -->
```{r echo=TRUE}
diag_caja_outliers_IQR(datos.num.norm, 4)
```
<!-- INCLUIR -->
```{r echo=TRUE}
# La función original tiene un fallo, solo muestra la última clave.
# Para corregirlo, se debe cambiar la línea:
# for (i in num.claves)
# Por:
# for (i in 1:num.claves)
diag_caja(datos.num.norm, 4, claves.outliers.IQR)
```

No se muestran los extremos por no tener ninguno.

Mostramos ahora los outlilers en todas las variables:
```{r}
diag_caja_juntos(datos.num, "Outliers", claves.outliers.IQR)
```
Como habíamos analizado antes, solo la instancia 79 parece ser un outliers en una sola columna.
Para el resto, vemos que más o menos habíamos acertado, solo la 11 parece también estar un tanto alejada de su media en la columna ``Proantho``.
<!-- REVISAR ESTA ÚLTIMA FRASE, VER SI EL SALIRSE DE LA RALLA ERA OUTLIER (JURARÍA QUE SÍ) -->

## Test de hipótesis

Haciendo uso de un test de hipótesis determinaremos si el valor más alejado de la media se puede considerar un outlier.

### Comprobación de la hipótesis de Normalidad

Puesto que vamos a aplicar el test de Grubs, tenemos que comprobar previamente que la distribución que sigue la variable __sin el outlier__ sigue una distribución normal (por tanto, el test de Shapiro que aplicamos anteriormente no nos vale).

Primeramente, podemos ver de forma gráfica la distribución de la variable elegida:
```{r}
ajusteNormal = fitdist(columna , "norm")
denscomp(ajusteNormal,  xlab = nombre.columna)
```

Los valores extremos pueden ser problemáticos para que la distribución sea normal, pero visualmente apreciamos un buen ajuste con la curva.

### Test de Grubs

Más adelante se aplicará el test de normalidad sobre la variable sin el outlier.
Antes, basándonos en el gráfico anterior y suponiendo que la distribución es normal, podemos hacer el test de Grubs sobre el valor más alejado de la media, obteniendo un p-value de:
```{r echo=T}
test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs$p.value
```

Mostramos adicionalmente el valor de la instancia:
```{r echo=T}
valor.posible.outlier = outlier(columna)
valor.posible.outlier
```
Y su clave:
```{r echo=T}
es.posible.outlier = outlier(columna, logical = TRUE)
clave.posible.outlier = which( es.posible.outlier == TRUE)
clave.posible.outlier
```

Tenemos un p-value > 0.05, por lo que el test no rechaza la hipótesis de que el valor más alejado de la media (en este caso la instancia 11) no es un outlier. Aunque tenga un valor bajo para la variable Alcalinity no podemos concluir con garantía estadística que es una anomalía.

### Test de Normalidad

Pasamos los tests de Normalidad al conjunto de datos eliminando previamente el outlier (instancia 11):
<!-- INCLUIR -->
```{r echo=TRUE}
datos.sin.outlier = columna[-clave.posible.outlier]
cat("Datos sin outlier:\n")
datos.sin.outlier

shapiro.test(datos.sin.outlier)

goodness_fit = gofstat(ajusteNormal)
goodness_fit$adtest
```

El test de Anderson-Darling no se aplica por tener pocos datos. Para el test de Shapiro, esta vez sin el outlier, vemos que ahora se rechaza (p-value < 0.05) por lo que afirma con significación que la columna no sigue una distribución normal. Tal y como habíamos visto anteriormente, el test con el outlier no rechazaba, por lo que este modificaba la media lo suficiente para hacer creer al estadístico que la distribución era normal.
No deberíamos por tanto aplicar el test de Grubs sobre la columna Alcalinity, pues esta normalidad es una condición necesaria.

Pese a ello, proseguimos con el apartado.

Construímos una función que nos calcula toda la información necesaria para aplicar un test de Grubs sobre una columna.
<!-- INCLUIR -->
```{r echo=TRUE}
#######################################################################
# Aplica el test de Grubbs sobre la columna ind.col de datos y devuelve una lista con:

# nombre.columna: Nombre de la columna datos[, ind.col]
# clave.mas.alejado.media: Clave del valor O que está más alejado de la media
# valor.mas.alejado.media: Valor de O en datos[, ind.col]
# nombre.mas.alejado.media: Nombre de O en datos
# es.outlier: TRUE/FALSE dependiendo del resultado del test de Grubbs sobre O
# p.value:  p-value calculado por el test de Grubbs
# es.distrib.norm: Resultado de aplicar el test de Normalidad 
#    de Shapiro-Wilks sobre datos[, ind.col]
#    El test de normalidad se aplica sin tener en cuenta el 
#    valor más alejado de la media (el posible outlier O)
#    TRUE si el test no ha podido rechazar
#       -> Sólo podemos concluir que los datos no contradicen una Normal
#    FALSE si el test rechaza 
#       -> Los datos no siguen una Normal

# Requiere el paquete outliers

test_Grubbs = function(datos, ind.col, alpha = 0.05) {
  columna <- datos[,ind.col]
  res <- list()
  
  # Nombre columna
  res$nombre.columna <- colnames(datos)[ind.col]
    
  # Búsqueda del outlier
  es.posible.outlier <- outlier(columna, logical = TRUE)
  
  res$clave.mas.alejado.media <- which(es.posible.outlier == TRUE)
  res$valor.mas.alejado.media <- outlier(columna)
  res$nombre.mas.alejado.media <- rownames(datos)[res$clave.mas.alejado.media]
  
  # Test de Grubbs
  test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)

  res$es.outlier <- ifelse(test.de.Grubbs$p.value <= alpha, TRUE, FALSE)
  res$p.value <- test.de.Grubbs$p.value

  # Test de normalidad
  test.Normalidad <- shapiro.test(columna[-res$clave.mas.alejado.media])
  res$es.distrib.norm <- ifelse(test.Normalidad$p.value > alpha, TRUE, FALSE)
  
  res
}
```


Aplicamos la función a nuestro dataset, obteniendo:
```{r}
test.Grubbs.datos.num = test_Grubbs(datos.num, indice.columna)

test.Grubbs.datos.num
```

## Trabajando con varias columnas

Aplicamos los métodos anteriores simultáneamente sobre varias columnas.

### Outliers IQR

Calculamos los outliers IQR
```{r}
claves.outliers.IQR.en.alguna.columna =
  claves_outliers_IQR_en_alguna_columna(datos.num, 1.5)

claves.outliers.IQR.en.mas.de.una.columna = 
  unique(
    claves.outliers.IQR.en.alguna.columna[
      duplicated(claves.outliers.IQR.en.alguna.columna)])
claves.outliers.IQR.en.alguna.columna = 
  unique (claves.outliers.IQR.en.alguna.columna)

print("Claves outliers IQR en mas de una columna:")
claves.outliers.IQR.en.mas.de.una.columna
print("Claves outliers IQR en alguna columna:")
claves.outliers.IQR.en.alguna.columna 
```

Los valores normalizados de algunos de estos outliers son:
<!-- INCLUIR -->
```{r echo=TRUE}
datos.num.norm[claves.outliers.IQR.en.alguna.columna,] %>% head() %>% as.data.frame()
```

De forma gráfica (puesto que hay una gran cantidad de puntos los separamos en dos gráficos):
```{r echo=T}
diag_caja_juntos(datos.num.norm, "Outliers en alguna columna-1", claves.outliers.IQR.en.alguna.columna[c(T,F)])
diag_caja_juntos(datos.num.norm, "Outliers en alguna columna-2", claves.outliers.IQR.en.alguna.columna[c(F,T)])
```

Notamos valores extremadamente altos en Mag y Proline, y solo unas pocas variables sin outliers.
También vemos que muchos outliers lo son en más de una columna, pero ninguno parece que en más de tres.

Además, apreciamos en Mag (cantidad de Magnesio) valores extremádamente altos, sobre todo en la instancia 47.
Una búsqueda rápida por internet nos dice que la cantidad magnesio en los vinos es baja, cosa que hace aún más llamativa esta instancia.
```{r}
datos.num[47,]

summary(datos.num$Mag)
```

De los valores altos en Proline (2,4,9,10) vemos que todos tienen valores similares en el resto de variables.
La Proline es el aminoácido más abundante en los vinos y parece variar en base al tipo de fermentación que ha sufrido el vino.
```{r}
datos.num[c(2,4,9,10),]
```

Referencia:

https://ojs.openagrar.de/index.php/VITIS/article/view/7459/6882

### Test de Hipótesis

Aplicaremos el test de Grubs sobre las columnas que tenemos, pero antes mostramos los gráficos de las distribuciones:
```{r}
par(mfrow = c(2,3))
# datos.num %>% apply(2, function(col) {
#   ajusteNormal = fitdist(col, "norm")
#   denscomp (ajusteNormal,  xlab = "")
# })
c(1:ncol(datos.num)) %>% sapply(function(x) {
 ajusteNormal = fitdist(datos.num[,x], "norm")
  denscomp (ajusteNormal,  xlab = names(datos.num)[x])
}) -> c
```

Quitamos la variable OD280 pues parece bimodal, pero mantenemos el resto por no ser demasiado "raras".
<!-- La 2, 10, 13 -->
<!-- Ante la duda, también la 8? -->
```{r}
# datos.num.reducidos <- datos.num[,-c(2,8,10,12,13)]
datos.num.reducidos <- datos.num[,-12]
```

Aplicamos el test
<!-- INCLUIR -->
```{r echo=TRUE}
sapply(1:ncol(datos.num.reducidos), test_Grubbs, datos=datos.num)
```

Lo primero que vemos es que tras quitar el valor más alejado de la media en cada columna solo una (Ash) no es rechazada por el test de Shapiro. Por tanto es la única que no contradice la hipótesis de normalidad, pues del resto se afirma con garantía estadística que no lo son.

Del único outlier que debemos mirar con el test de Grubs, vemos que es la instancia 11, que conocíamos que contenía valores extremadamente bajos de los componentes.
El p-value supera por poco nuestro nivel de significación, por lo que el test no lo rechaza con garantía estadística.

# Outliers Multivariantes

## Métodos estadísticos basados en la distancia de Mahalanobis

### Hipótesis de Normalidad

Para que un par de variables siga una distribución normal conjunta ambas deben estar normalmente distribuídas de forma individual.
Como el test de normalidad nos deja solo una (Ash), reducidos el nivel de significación a 0.025 para obtener otra variable más (esto no deberíamos aplicarlo en un problema real)
<!-- INCLUIR -->
```{r echo=TRUE, results="default"}
test <- sapply(1:ncol(datos.num), test_Grubbs, datos=datos.num)
son.col.normales <- apply(test, 2, function(x) {
  x$es.distrib.norm
})
# Alpha = 0.05
son.col.normales

test <- sapply(1:ncol(datos.num), test_Grubbs, alpha=0.025, datos=datos.num)
son.col.normales <- apply(test, 2, function(x) {
  x$es.distrib.norm
})
datos.num.distrib.norm = datos.num[,son.col.normales]

# Alpha = 0.025
son.col.normales
head(datos.num.distrib.norm)
```

Aplicamos un test de Normalidad multivariante para comprobar la condición:
```{r, results="default"}
test.MVN = mvn(datos.num.distrib.norm, mvnTest = "energy")
test.MVN$multivariateNormality["MVN"]
test.MVN$multivariateNormality["p value"]
```

El test multivariante no rechaza la hipótesis nula, por lo que no nos asegura que la distribución conjunta de Ash y Non-flava no es normal. Nuestro test indidual nos había dicho que no lo era, aunque recordamos que si no quitámos el valor más alejado de la media no lo negaba (y con este valor MVN hizo el test).

No deberíamos por tanto aplicar este método con distancia de Mahalanobis si queremos garantía estadística, pues la condición de que las variables se distribuyan de forma normal es necesaria.

### Tests de hipótesis para detectar outliers

Mostramos los outliers con y sin método robusto
```{r}
corr.plot(datos.num[,3], datos.num[,8])
```

Podemos ver que hay tres puntos fuera de las elipses, dos considerablemente más alejados que el tercero.
También apreciamos que el uso o no de un método robusto no nos cambia significativamente la forma de los resultados.

Aplicamos un test individual y un test múltiple, ajustando el parámetro alpha de forma que corrijamos el FWER:
<!-- INCLUIR -->
```{r echo=TRUE}
set.seed(2)

cerioli.individual <- cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha = 0.05)
claves.test.individual <- which(cerioli.individual$outliers)
nombres.test.individual <- nombres_filas(datos.num.distrib.norm, claves.test.individual)

n <- nrow(datos.num)
alpha <- 0.05
cerioli.interseccion <- cerioli2010.fsrmcd.test(datos.num.distrib.norm, signif.alpha = 1 - (1 - alpha)^(1/n))
claves.test.interseccion <- which(cerioli.interseccion$outliers)
nombres.test.interseccion <- nombres_filas(datos.num.distrib.norm, claves.test.individual)

claves.test.individual
# nombres.test.individual
claves.test.interseccion
# nombres.test.interseccion
```

El test de intersección no nos devuelve ningún outlier, pero el indivual nos saca cuatro puntos. Todos estos fueron detectados como outliers por el método IQR en alguna columan, siendo 11 y 73 en más de una.
```{r}
# claves.outliers.IQR.en.mas.de.una.columna
```


Si se cumpliera el requisito de distribución normal multivariante, tendríamos garantía estadística en la instancia con mayor distancia Mahalanobis.

Mostramos las distancias de forma ordenada:
<!-- INCLUIR -->
```{r echo=TRUE}
cerioli.individual$mahdist.rw %>% sort() %>% plot()
```

Apreciando los cuatro puntos vistos anteriormente y un segundo conjunto de puntos un poco por debajo. Este conjunto probablemente esté algo alejado de la nube de puntos, pero no sean outliers.

Si mostramos las claves y las distancias, obtenemos:
<!-- INCLUIR -->
```{r echo=TRUE}
clave.mayor.dist.Mah <- order(cerioli.individual$mahdist.rw, decreasing = TRUE)[1]
nombre.mayor.dist.Mah <- nombres_filas(datos.num, clave.mayor.dist.Mah)

cerioli.individual$mahdist
clave.mayor.dist.Mah
# nombre.mayor.dist.Mah
```

Si cumpliéramos el requisito de normalidad, concluiríamos con garantía estadística que el registro 11 rechaza la hipótesis y es un outlier. 
<!-- Este punto ya fue detectado como outlier tanto en una como en varias columnas. -->

## Visualización de datos con un Biplot

Mostramos visualmente información resumida del conjunto de datos en dos variables, junto a los outliers detectados mediante alguna columna con IQR:
```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num, 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

Lo primero que debemos notar es que el biplot no explica un alto porcentaje de las varianzas, probablemente por la cantidad de variables, que haga difícil explicar la variabilidad de los datos en solo dos componentes. Esto hace que nuestra confianza en los valores mostrados en el biplot no sea muy alta, pues existe una alta variabilidad no mostrada en el gráfico.
Además, vemos que una gran cantidad de outliers se ubican con valores altos en alguna columna, alejados del centro. Solo la instancia 11 se aprecia con valores muy inferiores en un conjunto de variables (Ash, Alcohol, Mag...) pero con valores normales en el resto.

También apreciamos que las variables están agrupadas en tres grupos, con correlaciones positivas y negativas entre ellas.
<!-- INCLUIR -->
```{r echo=TRUE}
corrplot::corrplot(cor(datos.num), type="upper")
```

Podemos reducir las variables seleccionadas de cara a obtener una varianza explicada mayor en el biplot:
<!-- INCLUIR -->
```{r echo=TRUE}
biplot.outliers.IQR = biplot_2_colores(datos.num[,-c(1,3,4,5,10,13)], 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

No tenemos la información expresada por las variables que hemos quitado, pero en base a estas obtenemos una variabilidad explicada del 14.8+54.8=69.6%, y podemos intentar interpretar de nuevo los resultados.

Primero vemos que la mayoría de outliers se ubican en la parte izquierda del biplot. Al tener variables correladas, se aprecia que muchos tienen alta cantidad de Phenols, Flavanoilds... mientras contienen bajo grado de Non-Flavanoids. 
Por otro lado tenemos un par de instancias en la esquina superior izquierda con nuevamente valores altos en varias columnas.

Volvemos a fijarnos en la instancia 11, que en las variables seleccionadas contiene valores mayormente normales. Era en las columnas Alcalinity y Ash donde contenía valores anormalmente bajos. Aún así vemos que la combinación de las variables seleccionadas es algo inusual, pero no demasiado, pues existe algunos puntos cercanos a su área.

Concluímos diciendo que se aprecia enormemente en este dataset que la información proporcionada por un biplot debe ser una ayuda al científico, y no el único método en el que basarse para la detección de outliers.

## Métodos basados en distancias: LOF

La selección de un buen número de vecinos es vital para los métodos basados en vecinos más cercanos. Un valor demasiado grande puede hacernos detectar como outliers nubes con bajo número de puntos, mientras que uno demasiado alto puede ocultar outliers como tales.
El método LOF basado en densidad paliará en parte estos problemas, pero sigue siendo necesaria una selección cuidadosa de este valor.

En base a las gráficas vistas a lo largo de la práctica, concretamente las de la sección [Gráficos], vemos a la mayoría de puntos generalmente agrupados en la misma nube, no apreciando multiples agrupaciones diferentes (debemos tener cuidado con esto puesto que así no tenemos en cuenta los outliers con valores anómalos en más de dos columnas). En base a este razonamiento, elegimos un número de vecinos medianamente alto, de 7.
```{r}
num.vecinos.lof = 7
lof.scores = lofactor(datos.num.norm, k = num.vecinos.lof)
claves.lof.ordenados <- order(lof.scores, decreasing = T)
```

<!-- INCLUIR -->
```{r echo=TRUE}
plot(sort(lof.scores, decreasing = T))
```

Vemos un valor con un alto grado de anomalía y dos conjuntos que lo siguen, el primero formado por seis instancias y el segundo por dos.

Analizamos este punto y el primer conjunto:
<!-- INCLUIR -->
```{r echo=TRUE}
claves.outliers.lof <- order(lof.scores, decreasing = T) %>% head(num.vecinos.lof)
# nombres.outliers.lof <- nombres_filas(datos.num.norm, claves.outliers.lof)

claves.outliers.lof
# nombres.outliers.lof
```

Mostramos también los valores normalizados de dichos registros:
```{r}
datos.num.norm[claves.outliers.lof, ] %>% as.data.frame()
```

Lo que más nos llama la atención primeramente es que de este conjunto de instancias la 11, la cuál nos ha aparecido múltiples veces a lo largo de la memoria, es la que menor grado LOF de anomalía tiene.

La que más obtiene es la 73, que también había sido detectada por los métodos anteriores. Visualizando sus datos normalizados, vemos que solo tiene valores alejados en dos columnas (tres si consideramos Alcalinity).
El resto de instancias mayoritariamente solo tienen un valor extremo en una columna, lo que ya les otorga un alto score de LOF.

Analizamos el registro con mayor score de LOF (73) en diagramas de dispersión (puesto que la figura es excesivamente grande con todas las variables, seleccionamos aquellas en las que contine valores extremos)
```{r}
clave.max.outlier.lof = claves.outliers.lof[1]

colores = rep("black", times = nrow(datos.num.norm[,-c(2,8,9,10,11,12,13)]))
colores[clave.max.outlier.lof] = "red"
pairs(datos.num.norm[,-c(2,8,9,10,11,12,13)], pch = 19,  cex = 0.5, col = colores, lower.panel = NULL)
```

Las correlaciones las vimos anteriormente, pero se hace notoria la relación Phenols-Flavanoids, con la instancia 73 más baja de lo que sería esperado si siguieramos una línea de regresión sobre esos datos.
Con Ash-Alcalinity ocurre algo similar, pero el punto se ubica alejado del resto, sin contradecir la tendencia.

También vemos que los valores de esta instancia se ven extremadamente altos en Ash y Flavanoids, situándolo en las esquinas cuando se muestran estas variables.
En el resto, en la mayoría de casos el punto está algo alejado del centro de la distribución pero no de manera extrema.

Volvemos a mostrar los biplots pero en este caso fijándonos en esta instancia:
```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.norm, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof

biplot.max.outlier.lof = biplot_2_colores(datos.num.norm[,-c(1,3,4,5,10,13)], clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```

Como habíamos mencionado, contiene inusualmente valores altos en varias variables, pero se sigue ubicando en una zona relativamente densidad.

Otros puntos que se sitúan por esa zona y con scores altos en LOF es el 25 y el 47. Los mostramos:
```{r}
datos.num.norm[claves.outliers.lof[-c(2,3,5,7)], ] %>% as.data.frame()
```

En los dos primeros notamos un alto grado de Alcalinity, y en los dos últimos de Magnesio, pero no se aprecia una característica extrema común que los relacione a todos.

## Métodos basados en Clustering

### Clustering usando K-means

Para esta parte, al no tener clústers predefinidos, seleccionamos un número de 3 y la cantidad de outliers a 5.
```{r}
num.outliers = 5
num.clusters = 3
set.seed(2)
```

Construímos el modelo K-means:
<!-- INCLUIR -->
```{r echo=TRUE, results="default"}
modelo.kmeans <- kmeans(datos.num.norm, num.clusters)
asignaciones.clustering.kmeans <- modelo.kmeans$cluster
centroides.normalizados <- modelo.kmeans$centers

head(asignaciones.clustering.kmeans)
centroides.normalizados %>% as.data.frame()
```

Y mostramos los valores de los centroides desnormalizados:
```{r}
centroides.desnormalizados = desnormaliza(datos.num, centroides.normalizados)
centroides.desnormalizados %>% as.data.frame()
```

Función para calcular los outliers:
<!-- INCLUIR -->
```{r echo=T}
#######################################################################
# Calcula las distancias de los datos a los centroides
# y se queda con los primeros (tantos como indica num.outliers)
# Devuelve una lista con las claves de dichos registros y las
# correspondientes distancias a sus centroides
#######################################################################
top_clustering_outliers = function(datos.normalizados, 
                                   asignaciones.clustering, 
                                   datos.centroides.normalizados, 
                                   num.outliers){
  distancias <- distancias_a_centroides(datos.normalizados, 
                                        asignaciones.clustering,
                                        datos.centroides.normalizados)
  claves <- distancias %>% order(decreasing = T) %>% head(num.outliers)
  
  res <- list()
  res$distancias <- distancias %>% sort(decreasing = T) %>% head(num.outliers)
  res$claves <- claves
  res
}
```

Y obtenemos los outliers:
```{r}
top.outliers.kmeans = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.kmeans, 
                                              centroides.normalizados, 
                                              num.outliers)
claves.outliers.kmeans = top.outliers.kmeans$claves 
# nombres.outliers.kmeans = nombres_filas(datos.num, claves.outliers.kmeans)
distancias.outliers.centroides = top.outliers.kmeans$distancias

claves.outliers.kmeans
# nombres.outliers.kmeans
distancias.outliers.centroides
```

Instancias que también habíamos obtenido con otros métodos.

Mostramos en un biplot estos outliers y los clusters:
```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers k-means - k=3",
                           asignaciones.clustering = asignaciones.clustering.kmeans,
                           claves.outliers = claves.outliers.kmeans)
```

No mostramos el biplot con el número reducido de columnas pues la separación en clústers ha hecho uso de todas las variables.

Apreciamos que se detecta un solo outlier en los grupos 1 y 3, estando estos bastante alejados de la nube de puntos a la que pertenecen.
Respecto a los del grupo 2, este es un clúster bastante disperso en este biplot (recordamos que la varianza explicada es baja), y los outliers están alejados de su centroide.

<!-- ---------------------------------------- -->

Sabemos que un outlier detectado como tal en una partición grande de clústers es más probable que lo sea. Probamos por tanto a repetir el proceso incrementando el número de clústers a 5.

Centroides normalizados:
```{r}
num.clusters <- 5
modelo.kmeans <- kmeans(datos.num.norm, num.clusters)
asignaciones.clustering.kmeans <- modelo.kmeans$cluster
centroides.normalizados <- modelo.kmeans$centers

# head(asignaciones.clustering.kmeans)
centroides.normalizados %>% as.data.frame()
```

Centroides desnormalizados:
```{r}
centroides.desnormalizados = desnormaliza(datos.num, centroides.normalizados)
centroides.desnormalizados %>% as.data.frame()
```

Obtenemos los outliers:
```{r}
top.outliers.kmeans = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.kmeans, 
                                              centroides.normalizados, 
                                              num.outliers)
claves.outliers.kmeans = top.outliers.kmeans$claves 
# nombres.outliers.kmeans = nombres_filas(datos.num, claves.outliers.kmeans)
distancias.outliers.centroides = top.outliers.kmeans$distancias

claves.outliers.kmeans
# nombres.outliers.kmeans
distancias.outliers.centroides
```

```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers k-means - k=5",
                           asignaciones.clustering = asignaciones.clustering.kmeans,
                           claves.outliers = claves.outliers.kmeans)
```

Vemos que:

- Coinciden los outliers 73, 11 y 21. Recordamos que estas instancias tenían valores muy extremos en al menos una de las variables.
- Se ha mantenido un clúster con la misma agrupación que con k=3, y los otros dos se han dividido en dos partes.
- El outlier 110, que hubiera pertenecido al clúster 5, al haberse este particionado ya no lo tenemos como uno de los 5 puntos con mayor distancia, puesto que el centroide se ha acercado a su posición.
- Tenemos también dos nuevos puntos (25 y 48):
  - El 25 lo vemos cerca de lo que sería su centroide. Es probable que el biplot no nos muestre la información que lo convierta en un valor anómalo.
  - Respecto al punto 48, lo tenemos muy cerca del centro del biplot. No es el único punto en esta zona, aunque está ligeramente desplazado hacia arriba. Es posible que tenga una combinación anómala, o que una vez más la información del biplot sea insuficiente.

<!-- ---------------------------------------- -->

Mostramos los outliers obtenidos con un k=5 en un diagrama de cajas:
```{r}
diag_caja_juntos(datos.num, "Outliers k-means", claves.outliers.kmeans)
```

Vemos que:

- La instancia 73 tiene valores extremos en tres columnas.
- La 25 y la 11 en dos.
- La 21 y la 48 solo tienen un valor extremo en una columna, la de Magnesio.

Hacemos notar que la información de este biplot no era muy buena porque vemos que la flecha del Magnesio se ubica en el centro apuntando hacia arriba y las instancias 21 y 48, que tienen una alta cantidad de este componente, se encuentran paralelamente cerca del centro del biplot.

Apreciamos también que en general el orden de las distancias de los outliers se corresponde con el número de columnas en las que son valores extremos estas instancias. Solo 25 tiene una distancia menor a 21, pero podemos apreciar que 21 tiene valores no extremos, pero alejados del centro de la distribución en la mayoría de columnas, lo que haga aumentar su grado de anomalía.

### Clustering usando medoides

Seguiremos usando 5 clústers para el método PAM.
```{r}
set.seed(2)
matriz.distancias = dist(datos.num.norm)
modelo.pam        = pam(matriz.distancias , k = num.clusters)
```

Obtenemos los medoides:
```{r}
asignaciones.clustering.pam = modelo.pam$clustering
nombres.medoides = modelo.pam$medoids
medoides = datos.num[nombres.medoides, ] %>% as.data.frame()
medoides.normalizados = datos.num.norm[nombres.medoides, ] %>% as.data.frame()

cat("Claves medoides: ")
nombres.medoides
medoides
```

Y sus valores normalizados:
```{r}
medoides.normalizados
```

Calculamos los cinco posibles outliers:
<!-- INCLUIR -->
```{r echo=TRUE}
top.outliers.pam = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.pam, 
                                              medoides.normalizados, 
                                              num.outliers)
claves.outliers.pam = top.outliers.pam$claves 
# nombres.outliers.pam = nombres_filas(datos.num, claves.outliers.pam)

claves.outliers.pam
# nombres.outliers.pam
```
Y creamos un biplot:
<!-- INCLUIR -->
```{r echo=TRUE}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers PAM",
                           asignaciones.clustering = asignaciones.clustering.pam,
                           claves.outliers = claves.outliers.pam)
```

Lo primero que notamos es un conjunto de clústers totalmente diferente a los del método K-means. En este caso tenemos una única agrupación a la derecha y tres clústers en la parte centro-inferior izquierda.
Esto ha hecho que le punto 11, que ha sido detectado como outliers por todos los métodos anteriores, ya no lo sea.

En este caso los outlier se ubican en la parte izquierda, no muy alejados de sus nubes de puntos pero si con valores extremos en al menos alguna de las componentes principales del biplot.
De estos outliers, habíamos obtenido con k-means las instancias 73, 25 y 21, y con un valor de k=3 también la 62.

Mostramos un diagrama de cajas con estos puntos:
```{r}
diag_caja_juntos(datos.num, "Outliers PAM", claves.outliers.pam)
```

Todos tienen valores extremos en más de una columna excepto la 62, que únicamente tiene una gran acumulación de Proantho.

### Análisis de los outliers multivariantes puros

Calculamos aquellos outliers que no lo son para una variable separadamente
<!-- INCLUIR -->
```{r echo=TRUE, results="default"}
claves.outliers.lof.no.IQR <- setdiff(claves.outliers.lof, claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

# nombres.outliers.lof.no.IQR
claves.outliers.IQR.en.alguna.columna
claves.outliers.lof
claves.outliers.lof.no.IQR
```

Vemos que no obtenemos ningún outlier puramente multivariable, todos son extremos en alguna de ellas.

En el método LOF vimos que había 3 conjuntos de puntos con scores altos, con un total de 9 instancias.
Volvemos a calcular los outliers LOF aumentando el umbral a 9 puntos, y buscamos los multivariables puros:
```{r, echo=T, results="default"}
claves.outliers.lof.no.IQR <- setdiff(claves.lof.ordenados %>% head(9), claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

# nombres.outliers.lof.no.IQR
claves.outliers.IQR.en.alguna.columna
claves.lof.ordenados %>% head(11)
claves.outliers.lof.no.IQR
```

Mostramos el biplot de los outliers puros:
```{r}
biplot <- biplot_2_colores(datos.num, 
                           titulo = "Outliers LOF (excluídos los que son IQR)",
                           claves.a.mostrar = claves.outliers.lof.no.IQR)
biplot
```

Ahora sí obtenemos un registro, el número 23.

Mostramos sus valores normalizados:
```{r}
datos.num.norm[claves.outliers.lof.no.IQR, ] %>% as.data.frame()
```

Y vemos como no contiene ninguna cantidad extrema de ningún componente:
```{r}
diag_caja_juntos(datos.num, "Outliers multivariables puros", claves.outliers.lof.no.IQR)
```

A partir del diagrama de cajas apreciamos que aunque no tienen un valor extremo en ninguna de las variables, en la mayoría contienen una cantidad un tanto alejada del centro de la distribución. Este hecho sea lo que probablemente haga aumentar el score de LOF y presente un alto grado de anomalía.

# Análisis de resultados

__Conjunto de datos__

Se ha trabajado con el dataset _wine_, que codifica la cantidad de diversos componentes en varios vinos italianos.
Todas las variables son numéricas y con suficiente variabilidad, por lo que no se ha eliminado ninguna.

Hemos aplicado la normalización por z-score en aquellos métodos que así lo han requerido.

__Outliers en una variable__

- __Método IQR__

No hemos encontrado outliers extremos pero sí hay cuatro outliers no extremos.

La instancia 11 es un vino con bajas cantidades de la mayoría de componentes, siendo extremo IQR en múltiples de ellas.

El resto de outliers solo se disparan por arriba, con la 73 contando con valores altos de Ash y Alcalinity.
La 25 y 79 tienen valores altos de Alcalinity pero no extremadamente alejados de sus medias.

Existen al menos alguna instancia con valores altos en prácticamente todas las variables, aunque estos se acumulan más en las columnas de Magnesio y Proline.

Aunque sean vinos del mismo país, las diferentes formas de cultivo en las regiones y los diferentes terrenos hagan que los vinos acumulen una mayor cantidad de ciertos componentes.
También es posible que el tiempo de fermetación afecte a las cantidades, con mayor grado en Ash y Alcohol.

No se aprecia ninguna característica común que agrupe a varios vinos.

- __Test de Hipótesis__

Para el test de normalidad se elimina la variable OD280 por tener una forma asemejada a una bimodal.
El test de Shapiro-Wilks nos rechaza con significación todas las columnas menos Ash.

Contamos con un p-value alto (0.0127) para el outlier de esta columna (instancia 11), por lo que no podemos afirmar con garantía estadística que lo sea.

__Outliers multivariantes__

- __Visualización con biplot__

Contamos con un número alto de variables y el biplot no nos saca una variabilidad explicada alta (51.3%), eliminando un conjunto alto de ellas conseguimos un porcentaje de 69.8%, pero la representación deja de ser fidedigna puesto que se han eliminado columnas usadas para calcular los outliers.

- __Métodos estadísticos usando la distancia de Mahalanobis__

Se han lanzado los métodos para obtener información pese a que no existía ninguna distribución Normal multivariante. Se ha reducido el valor de significación para elegir una columna más, aplicando finalmente los métodos con la distribución conjunta Ash-NonFlava.

El test de intersección no devuelve ningún outlier, mientras que el test individual devuelve cuatro outliers, todos detectados previamente con el método IQR. 
La instancia con mayor distancia de Mahalanobis era la 11, que como hemos comentado contiene valores extremadamente bajos en múltiples componentes. Al no cumplir los requisitos de normalidad, este método no puede afirmar con garantía estadística que sea un outlier.

- __LOF__

El gráfico de scores mostró tres grupos diferentes: un valor muy alto, un conjunto de seis puntos y uno de dos.
De los siete puntos con mayor grado de anomalía, todos tenían un valor extremo en al menos una columna, siendo la causa de este score alto.

En base al método LOF se detecta un outlier multivariante puro en el tercer grupo de puntos. Esta instancia, la 23, contienen valores altos ligeramente alejados de las media en la mayoría de las columnas, ubicándose en torno al centro de la distribución solo en la variable Color.

- __Métodos basados en clustering__

Con el método K-means en tres particiones en vinos detectados previamente. Una segunda ejecución ampliando el número de clúster a 5 nos ha cambiado las instancias con mayor distancia a su centroide, manteniéndose la 73, 11 y 21.

Hemos notado cierta tendencia viendo que el orden de estos puntos se correspone con el número de columnas en las que son valores extremos. Solo una instancia interesante, la 25, no contiene valores anómalos pero si un tanto superiores a la media en cada uno de los componentes.

Haciendo uso de medoides solo se han detectado instancias ya descubiertas.