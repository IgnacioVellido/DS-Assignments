---
title: "Practica"
author: "Ignacio Vellido"
date: "12/9/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    toc: true
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)

library(ggplot2)   # Gr?ficos
library(fitdistrplus)  # Ajuste de una distribución -> denscomp 
library(reshape)   # melt
library(ggbiplot)  # biplot
library(tidyverse)   
library(outliers)  # Grubbs
library(MVN)       # mvn: Test de normalidad multivariante  
library(CerioliOutlierDetection)  #MCD Hardin Rocke
library(mvoutlier) # corr.plot 
library(DMwR)      # lof
library(cluster)   # PAM
```

```{r include=FALSE}
# M?ster -> Detecci?n de anomal?as
# Juan Carlos Cubero. Universidad de Granada

###########################################################################
# Funciones utilizadas a lo largo del curso
###########################################################################

# rm(list=ls()) 


###########################################################################
# Realiza un plot de todos los registros
# Permite cambiar el color con el que se visualiza un conjunto de registros. 
# Los registros que se muestran con otro color se especifican en el par?metro
# claves.a.mostrar 

plot_2_colores = function (datos, 
                           claves.a.mostrar, 
                           titulo = "",
                           colores = c("black", "red")){
  
  num.datos = nrow(as.matrix(datos))
  seleccionados =  rep(FALSE, num.datos)
  seleccionados[claves.a.mostrar] = TRUE
  colores.a.mostrar = rep(colores[1], num.datos)
  colores.a.mostrar [seleccionados] = colores[2]
  
  plot(datos, col=colores.a.mostrar, main = titulo)
}



###########################################################################
# Funci?n an?loga a son_outliers_IQR, salvo que devuelve un vector
# de claves en vez de un vector de bools

claves_outliers_IQR = function(datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  son.outliers.IQR = son_outliers_IQR(datos, ind.columna, coef)
  return (which(son.outliers.IQR  == TRUE))
}



###########################################################################
# Calcula los outliers IQR con respecto a una columna 
# Devuelve un vector de bools indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a la columna ind.columna
# coef es 1.5 para los outliers normales y hay que pasarle 3 para los outliers extremos

son_outliers_IQR = function (datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  cuartil.primero = quantile(columna.datos)[2]  
  #quantile[1] es el m?nimo y quantile[5] el m?ximo.
  cuartil.tercero = quantile(columna.datos)[4] 
  iqr = cuartil.tercero - cuartil.primero
  extremo.superior.outlier = (iqr * coef) + cuartil.tercero
  extremo.inferior.outlier = cuartil.primero - (iqr * coef)
  son.outliers.IQR  = columna.datos > extremo.superior.outlier |
    columna.datos < extremo.inferior.outlier
  return (son.outliers.IQR)
}


###########################################################################
# Calcula los outliers IQR con respecto a ALGUNA columna
# Devuelve un vector de claves indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a ALGUNA columna
# coef es 1.5 para los outliers normales y  3 para los outliers extremos

claves_outliers_IQR_en_alguna_columna = function(datos, coef = 1.5){
  df.clave.columnas = data.frame()
  claves.outliers =  sapply(1:ncol(datos), 
                               function(x) claves_outliers_IQR(datos, x, coef)
  )
  claves.outliers.en.alguna.columna = unlist(claves.outliers)
  return (claves.outliers.en.alguna.columna)
}




#######################################################################
# Devuelve los nombres de aquellas filas especificadas en el par?metro claves
# filas es un vector de bools 

nombres_filas = function (datos, claves) {
  num.claves = length(claves)
  nombres.filas = row.names(as.data.frame(datos))[claves]
  
  return (nombres.filas)
}




#######################################################################
# funci?n base para diag_caja_outliers_IQR y diag_caja

diag_caja_grafico_base = function(datos, indice.columna){
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  nombre.columna = colnames(datos)[indice.columna]
  ggboxplot = ggplot(data = as.data.frame(datos), 
                     aes(x=factor(""), 
                         y = datos[,indice.columna]) , 
                     environment = environment()) + 
              xlab(nombre.columna) + ylab("") 
  return (ggboxplot)
}

#######################################################################
# Muestra un diagrama de caja
# Calcula los outliers IQR y los muestra como puntos en rojo en un BoxPlot

diag_caja_outliers_IQR = function (datos, ind.columna, coef.IQR = 1.5){
  # Si quisi?semos l?neas horizontales en los l?mites de las cajas
  # habr?a que a?adir 
  # + stat_boxplot(geom = 'errorbar')   
  
   outliers.IQR = son_outliers_IQR(datos, ind.columna, coef = coef.IQR)
   ggboxplot =  diag_caja_grafico_base(datos, ind.columna) + 
                stat_boxplot(coef = coef.IQR) +
                geom_boxplot(coef = coef.IQR, outlier.colour = "red") 
                # Importante: geom_boxplot debe ir despu?s de stat_boxplot
   
   return (ggboxplot)
}



#######################################################################
# Muestra un diagrama de caja
# Tambi?n muestra las etiquetas de los registros indicados en 
# el par?metro claves.a.mostrar 

diag_caja = function (datos, ind.columna, claves.a.mostrar = c()){
  num.filas = nrow(datos)
  num.claves = length(claves.a.mostrar)
  nombres.filas = vector (mode = "character", length = num.filas)
  nombres.filas = rep("", num.filas)
  nombres.claves = nombres_filas(datos, claves.a.mostrar)

  for (i in num.claves)
    nombres.filas[claves.a.mostrar[i]]  = nombres.claves[i]
  

  ggboxplot = diag_caja_grafico_base(datos, ind.columna) + 
    geom_boxplot(outlier.shape = NA) + # Para que no imprima los outliers IQR calculados dentro del mismo geom_boxplot
    geom_text(aes(label = nombres.filas)) 
  
  return (ggboxplot)
}






#######################################################################
# Muestra de forma conjunta todos los diagramas de caja de las variables de datos
# Para ello, normaliza previamente los datos.
# Tambi?n muestra las etiquetas de los registros indicados en claves.a.mostrar
# Requiere reshape

diag_caja_juntos = function (datos, titulo = "", claves.a.mostrar = c()){  
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  
  # Para hacerlo con ggplot, lamentablemente hay que construir antes una tabla 
  # que contenga en cada fila el valor que a cada tupla le da cada variable 
  # -> paquete reshape->melt
  
  # Por ejemplo, si tenemos el siguiente data frame
  
  # datos = data.frame(
  #   A = c(1, 2),
  #   B = c(3, 4)
  # )
  # datos =
  #     A  B
  #     1  3
  #     2  4
  
  # melt(datos) construye esta tabla:
  
  #      variable value
  # 1        A     1
  # 2        A     2
  # 3        B     3
  # 4        B     4
  
  
  nombres.de.filas = nombres_filas (datos, claves.a.mostrar)
  
  datos = scale(datos)
  datos.melted = melt(datos)
  colnames(datos.melted)[2]="Variables"
  colnames(datos.melted)[3]="zscore"
  factor.melted = colnames(datos.melted)[1]
  columna.factor = as.factor(datos.melted[,factor.melted])
  levels(columna.factor)[!levels(columna.factor) %in% nombres.de.filas] = ""  
  
  ggplot(data = datos.melted, 
         aes(x=Variables, y=zscore), 
         environment = environment()) + 
    ggtitle(titulo) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_text(aes(label = columna.factor), size = 3) 
}






#######################################################################
# Muestra un biplot del conjunto de datos
# Se muestran los nombres de los registros indicados en claves.a.mostrar
# El color usado para dichos registros es el segundo del par?metro colores
# El t?tulo para el grupo de dichos registros es el especificado en titulo.grupo.a.mostrar
# El par?metro titulo especifica el t?tulo principal del gr?fico

biplot_2_colores = function (datos, 
                             claves.a.mostrar = c(), 
                             titulo = "",
                             titulo.grupo.a.mostrar = "Outliers",
                             colores = c("black","red")){
  nombres = rownames(datos)
  claves.datos = c(1:nrow(datos))
  son.a.mostrar = claves.datos %in% claves.a.mostrar
  nombres[!son.a.mostrar] = ''

  PCA.model = princomp(scale(datos))
  outlier.shapes = c(".","x") 
  biplot = ggbiplot(PCA.model,
                    obs.scale = 1,
                    var.scale = 1 ,
                    varname.size = 5,
                    groups =  son.a.mostrar,
                    alpha = 1/2) #alpha = 1/10
  biplot = biplot + labs(color = titulo.grupo.a.mostrar)
  biplot = biplot + scale_color_manual(values = colores)
  biplot = biplot + geom_text(label = nombres,
                              stat = "identity",
                              size = 3,
                              hjust=0,
                              vjust=0)
  biplot = biplot + ggtitle(titulo)
}



#######################################################################
# Muestra un biplot de un conjunto de datos diferenciados por color
# El color lo determina la asignaci?n de cada dato a un cluster 
# Las asignaciones de datos a cluster se indican en asignaciones.clustering
# Tambi?n se muestran los outliers cuyas claves vienen indicadas en claves.outliers
 
biplot_outliers_clustering = function(datos, 
                                      titulo = "Outliers por el m?todo de Clustering", 
                                      titulo.color = "Asignaciones Clustering",
                                      titulo.outlier = "Outliers",
                                      asignaciones.clustering,
                                      claves.outliers){
  son.outliers = rep(FALSE, nrow(datos))
  son.outliers[claves.outliers] = TRUE
  
  bip = biplot_colores_formas(datos, 
                              titulo, titulo.color, titulo.outlier,
                              asignaciones.clustering,
                              son.outliers,
                              claves.outliers)
  bip 
}

#######################################################################
# Muestra un biplot del conjunto de datos
# Los datos se muestran diferenciados por color y por forma
# Las asignaciones de cada dato a su color y forma vienen dadas por los vectores
# asignaciones.colores y asignaciones.formas 
# Tambi?n se muestran las etiquetas de los registros indicados
# en el par?metro opcional claves.a.mostrar 

biplot_colores_formas = function (datos, 
                                  titulo, titulo.color = '', titulo.forma = '', 
                                  asignaciones.colores, asignaciones.formas,
                                  claves.a.mostrar = c()){
  PCA.model = princomp(scale(datos))
  
  son.a.mostrar = rep(FALSE, nrow(datos))
  son.a.mostrar[claves.a.mostrar] = TRUE
  nombres.a.mostrar = rownames(datos)
  nombres.a.mostrar[!son.a.mostrar] = ''

  asignaciones.colores = factor(asignaciones.colores)
  asignaciones.formas  = factor(asignaciones.formas)

  
  bip = ggbiplot(PCA.model, obs.scale = 1, var.scale=1 , varname.size = 3, alpha = 0) +              
    geom_point(aes(shape = asignaciones.formas, colour = asignaciones.colores))  +
    labs(shape = titulo.forma) +
    labs(colour = titulo.color) +
    ggtitle(titulo) +
    geom_text(label = nombres.a.mostrar, stat = "identity", size = 3, hjust=0, vjust=0)      
  
  bip
}

#######################################################################
# Calcula las distancias de cada dato al centroide de su cluster
# Las asignaciones de cada dato a su cluster se indican en asignaciones.clustering
# Cada centroide es una fila del data frame datos.centroides.normalizados

distancias_a_centroides = function (datos.normalizados, 
                                    asignaciones.clustering, 
                                    datos.centroides.normalizados){
  
  sqrt(rowSums(   (datos.normalizados 
                   - 
                   datos.centroides.normalizados[asignaciones.clustering,])^2  ))
}


#######################################################################
# Revierte la funci?n de normalizaci?n (z-score)

desnormaliza = function(datos, filas.normalizadas){
  medias        = colMeans(datos)
  desviaciones  = apply(datos, 2, sd , na.rm = TRUE)
  
  filas.desnormalizadas  = sweep(filas.normalizadas, 2, desviaciones, "*")
  filas.desnormalizadas  = sweep(filas.desnormalizadas, 2, medias, "+")
  
  filas.desnormalizadas 
}




top_clustering_outliers = function(datos.norm, 
                                   asignaciones.clustering, 
                                   datos.centroides.norm, 
                                   num.outliers){
  
  dist_centroides = distancias_a_centroides (datos.norm, 
                                             asignaciones.clustering, 
                                             datos.centroides.norm)
  
  claves = order(dist_centroides, decreasing=T)[1:num.outliers]
  
  list(distancias = dist_centroides[claves]  , claves = claves)
}
```

# Dataset y Selección de Variables

En este guion usaremos el conjunto de datos __mtcars__ disponible directamente en R. Contiene los datos de un serie de vehículos. Puede encontrar en Internet una descripción completa de dicho dataset. Nosotros describimos aquí las columnas que serán el objetivo de nuestro estudio.

- Variables relativas a las características físicas

  - disp (displacement) Nos indica la cilindrada en pulgadas cúbicas. En España, lo normal es referirnos a la cilindrada en centímetros cúbicos.

  - hp (horse power) Es la potencia del motor

  - drat (Rear axle ratio) Es la relación del eje trasero. Un valor bajo nos permite unos desarrollos mayores con bajo consumo: es lo habitual en turismos. Un valor alto hace que el coche consuma más, pero permite enviar más fuerza, como por ejemplo en un todo terreno.

  - wt (weight) Peso del vehículo

- Variables relativas al rendimiento

  - mpg (miles per gallon) Nos indica el consumo del coche. Cuanto mayor sea, más combustible consume.

  - qsec (1/4 mile time) Mide el tiempo necesario para recorrer un cuarto de milla. Es una medida inversa a la aceleración: cuanto más acelere un coche, menor será el valor de qsec .

Para trabajar con dicho conjunto, vamos a construir los siguientes objetos:

  - datos: frame de datos que contendrá mtcars
  - datos.num: frame obtenido a partir de datos utilizando sólo las columnas de tipo numérico.
  - indice.columna: Índice de la columna de datos con la que se quiera trabajar.
  - columna: Contendrá la columna de datos correspondiente a indice.columna.
  - nombre.columna: Nombre de la columna correspondiente a indice.columna.

Trabajaremos únicamente sobre las variables numéricas. Por lo tanto procedemos de la siguiente forma:

1. Cargamos el conjunto de datos. En nuestro caso usaremos el conjunto de datos mtcars
2. Seleccionamos sólo las variables numéricas. Para ello usamos la función is.numeric. Aplicada sobre una columna, nos dice si 3. todos sus valores son numéricos. Por ejemplo, para ver si la tercera columna es numérica, pondríamos is.numeric(datos[, 3])
4. Vemos los valores que toman dichas variables y eliminamos aquellas que sean ordinales o con pocos valores distintos
5. Eliminamos también aquellos registros que tienen algún valor nulo en alguna columna. En aquellos casos en los que tenga sentido hacerlo, se puede aplicar un procedimiento de imputación de valores en vez de eliminar dichos registros.

Cargamos el conjunto de datos
```{r}
datos = mtcars
head(datos)
```

Construímos un dataframe con las columnas numéricas
```{r}
columnas.num = sapply(c(1:ncol(datos)) , function(x) is.numeric(datos[, x]))
columnas.num

datos.num = datos[, columnas.num]
```

Vemos información sobre cada variable
```{r}
head(datos)

# Medidas estadísticas
summary(datos)

# Ocurrencias
apply(mpg, 2, table)
```

Puede apreciar que las variables cyl, vs, am, gear, carb tienen muy pocos valores distintos por lo que las eliminamos del estudio.
```{r}
datos.num  = datos.num[,-c(2 , 8:11)]  
head(datos.num)
```

Finalmente, eliminamos todas aquellas filas que tengan algún valor nulo:
```{r}
datos.num = na.omit(datos.num)
```

# Detección de outliers en una dimensión
## Outliers IQR

Los métodos IQR teóricamente solo se deben aplicar a distribuciones normales, pero también pueden funcionar si la forma de la distribución no es rara (multimodal, uniforme...).

Mostramos histograma de cada variable
```{r}
par(mfrow = c(2,3))
c(1:ncol(datos.num)) %>% sapply(function(x) hist(datos.num[,x], 
                                                 main="", 
                                                 xlab=names(datos.num)[x]))
```

Ninguna variable sigue una distribución __rara__ (quizás la variable _disp_ que parece uniforme), así que mantenemos todas las columnas.

A falta de más información, seleccionamos cualquiera de ellas (posteriormente trabajaremos con todas) Por ejemplo, seleccionamos _mpg_ (ya que junto a qsec, drat y hp son las que más se asemejan a una Normal) Establecemos las siguientes variables para reutilizarlas a lo largo de este apartado.

```{r}
indice.columna = 1
columna        = datos.num[, indice.columna]
nombre.columna = names(datos.num) [indice.columna]
```

### Obtención de los outliers IQR

1. En primer lugar debe calcular las siguiente variables:

  - cuartil.primero: Es el primer cuartil
  - cuartil.tercero: Es el tercer cuartil
  - iqr : Distancia intercuartil IQR
  
  Para ello, usamos las siguientes funciones:

  - quantile(columna, x) para obtener los cuartiles: x=0.25 para el primer cuartil, 0.5 para la mediana y 0.75 para el tercero.
  - IQR para obtener la distancia intercuartil (o bien reste directamente el cuartil tercero y el primero)
  
```{r}
cuartil.primero <- quantile(columna, .25, names = F)
cuartil.tercero <- quantile(columna, .75, names = F)
iqr <- IQR(columna)
```

```{r}
cat("Q1: ")
cuartil.primero
cat("\nQ3: ")
cuartil.tercero
cat("\nIQR: ")
iqr
```
  
2. A continuación debe calcular los extremos que delimitan los outliers:

  - extremo.superior.outlier.IQR se calcula como el cuartil tercero más 1.5 veces la distancia intercuartil.
  - extremo.inferior.outlier.IQR se calcula como el cuartil primero menos 1.5 veces 1.5 la distancia intercuartil.
  - extremo.superior.outlier.IQR.extremo se calcula como el cuartil tercero más 3 veces la distancia intercuartil.
  - extremo.inferior.outlier.IQR.extremo se calcula como el cuartil primero menos 3 veces la distancia intercuartil.

```{r}
extremo.superior.outlier.IQR <- cuartil.tercero + 1.5 * iqr
extremo.inferior.outlier.IQR <- cuartil.primero - 1.5 * iqr
extremo.superior.outlier.IQR.extremo <- cuartil.tercero + 3 * iqr
extremo.inferior.outlier.IQR.extremo <- cuartil.primero - 3 * iqr
```

```{r}
extremo.superior.outlier.IQR
extremo.inferior.outlier.IQR
extremo.superior.outlier.IQR.extremo
extremo.inferior.outlier.IQR.extremo
```


3. Finalmente, construya sendos vectores de valores lógicos TRUE/FALSE que nos dicen si cada registro es o no un outlier con respecto a la columna fijada:

  - son.outliers.IQR
  - son.outliers.IQR.extremos

Para ello, basta comparar con el operador relacional > o el operador relacional < la columna con alguno de los valores extremos anteriores (El operador lógico que debe usar es |)

```{r}
son.outliers.IQR <- columna < extremo.inferior.outlier.IQR | columna > extremo.superior.outlier.IQR
son.outliers.IQR.extremos <- columna < extremo.inferior.outlier.IQR.extremo | columna > extremo.superior.outlier.IQR.extremo
```

```{r}
head(son.outliers.IQR)
head(son.outliers.IQR.extremos)
sum(son.outliers.IQR)
sum(son.outliers.IQR.extremos)
```

### Índices y valores de los outliers IQR
```{r}
claves.outliers.IQR <- which(son.outliers.IQR)
df.outliers.IQR <- datos.num[claves.outliers.IQR,]
nombres.outliers.IQR <- row.names(df.outliers.IQR) 
valores.outliers.IQR <- columna[claves.outliers.IQR]

claves.outliers.IQR.extremos <- which(son.outliers.IQR.extremos)
df.outliers.IQR.extremos <- datos.num[claves.outliers.IQR.extremos,]
nombres.outliers.IQR.extremos <- row.names(df.outliers.IQR.extremos) 
valores.outliers.IQR.extremos <- columna[claves.outliers.IQR.extremos]
```

```{r}
claves.outliers.IQR
df.outliers.IQR
nombres.outliers.IQR
valores.outliers.IQR
```


```{r}
claves.outliers.IQR.extremos
df.outliers.IQR.extremos
nombres.outliers.IQR.extremos
valores.outliers.IQR.extremos
```

### Cómputo de los outliers IQR con funciones

ELIMINAR LO DE ARRIBA PARA LA MEMORIA (o incluírlo como subapartado)

```{r}
son.outliers.IQR     = son_outliers_IQR(datos.num, indice.columna)
head(son.outliers.IQR)

claves.outliers.IQR  = claves_outliers_IQR(datos.num, indice.columna)
claves.outliers.IQR

son.outliers.IQR.extremos    = son_outliers_IQR(datos.num, indice.columna, 3)
head(son.outliers.IQR.extremos)

claves.outliers.IQR.extremos = claves_outliers_IQR(datos.num, indice.columna, 3)
claves.outliers.IQR.extremos
```

### Desviación de los outliers con respecto a la media de la columna

Si partimos de una variable X cuya distribución no es normal, el método de z-score no obtiene una N(0,1), pero si la distribución de X no es demasiado rara, los datos que así obtengamos nos darán información útil sobre si los registros son usuales o no. Para ilustrarlo, apliquemos el método z-score a la variable mpg. Para ello, usamos la función scale:

```{r}
datos.num.norm = scale(datos.num)
head(datos.num.norm)

columna.norm   = datos.num.norm[, indice.columna]
```
Para ver los valores normalizados de los outliers, construya la la variable valores.outliers.IQR.norm. Para ello, debe usar la variable columna.norm junto con son.outliers.IQR (o bien claves.outliers.IQR). Le debe salir lo siguiente:
```{r}
valores.outliers.IQR.norm <- columna.norm[claves.outliers.IQR]

valores.outliers.IQR.norm
```

Vamos a ver ahora el comportamiento de los outliers en la columna seleccionada con respecto al resto de columnas. Para ello, basta con seleccionar los datos correspondientes del conjunto de datos normalizado. En nuestro caso, sólo tenemos un outlier IQR en la columna seleccionada. Nos debe salir lo siguiente:

```{r}
datos.num.norm.outliers.IQR <- datos.num.norm[claves.outliers.IQR,]

datos.num.norm.outliers.IQR
```
Podemos apreciar que el Toyota Corolla no tiene valores excesivamente grandes o pequeños en el resto de columnas (distintas de mpg)

### Gráfico

Mostramos en un gráfico los valores de los registros. Usaremos el color rojo para mostrar lo outliers. Para ello, llame a la siguiente función:
```{r}
plot_2_colores(datos.num.norm, claves.outliers.IQR)
```

```{r}
plot_2_colores(datos.num.norm, claves.outliers.IQR.extremos)
```

### Diagrama de cajas

Otro análisis exploratorio de los datos nos lo da los diagramas de cajas. Vamos a usar la función geom_boxplot definida en el paquete ggplot. En vez de usarla directamente, llamamos a la siguiente función (que llama internamente a geom_boxplot), disponible en el fichero

```{r}
diag_caja_outliers_IQR(datos.num.norm, 1)
```

Esta función se ha construido para mostrar un diagrama de cajas genérico. El diagrama también muestra las etiquetas de los registros cuyos índices se indican en el parámetro claves.a.mostrar. En nuestro caso, le pasamos como parámetro el vector que ya había construido anteriormente con los índices de los outliers IQR, es decir, el vector claves.outliers.IQR ( pero podría pasarle cualquier otro vector de índices). Nos debe salir lo siguiente:

```{r}
diag_caja(datos.num.norm, 1, claves.outliers.IQR)
```

Al igual que hicimos en el apartado anterior, vamos a analizar los valores que un outlier (con respecto a una columna seleccionada) toma en el resto de columnas. Para ello, vamos a mostrar de forma conjunta los diagramas de cajas de varias variables. Llamamos a la función diag_caja_juntos, disponible en el fichero OutliersFunciones_byCubero.R
```{r}
diag_caja_juntos(datos.num, "Outliers", claves.outliers.IQR)
```

Tal y como habíamos analizado anteriormente, el Toyota Corolla (que es un outlier IQR con respecto a mpg) no tiene valores anormales en el resto de columnas (aunque tal vez con la excepción de la variable disp).

## Test de hipótesis

En este apartado vamos a determinar con un test de hipótesis si el valor más alejado de la media puede considerarse como un outlier.
Así pues, la hipótesis nula es la siguiente:


H0:El valor más alejado de la media no es un outlier

O siendo más correctos:

H0:El valor más alejado de la media proviene de la misma distribución que el resto de datos

El método IQR que hemos visto anteriormente es un método que suele aplicarse con la única restricción de que el histograma de la variable no sea demasiado raro. Sin embargo, un test de hipótesis es un método de decisión cuya finalidad es rechazar una hipótesis con suficientes garantías, desde un punto de vista estadístico. Por tanto, debemos ser más cautelosos con las restricciones exigidas para aplicar el método. En nuestro caso, vamos a aplicar el test de Grubbs.

### Comprobación de la hipótesis de Normalidad

El test de Grubbs establece como hipótesis nula que el valor más alejado de la media (llamémosle O) no es un outlier. Por tanto, si el test rechaza, tendremos garantía estadística de que es un outlier. Ahora bien:

El test asume que los datos deben seguir una distribución Normal. Esta hipótesis se refiere al conjunto de datos sin tener en cuenta O. Por tanto, si el test de Grubbs decide rechazar y se acepta que O es un outlier, el siguiente paso que debemos dar es comprobar que los datos que quedan siguen una distribución Normal. Esto lo haremos aplicando un test específico de ajuste de distribuciones.

Si no se rechaza, sólo podremos decir que no hay evidencia de que O provenga de otra distribución distinta al resto de los datos.

En primer lugar pasamos a comprobar de una forma informal que los datos siguen una distribución Normal. Lo vamos a hacer visualmente analizando el histograma. Por simplicidad incluimos el posible outlier en el gráfico. Posteriormente, aplicaremos un test de hipótesis específico de ajuste de distribuciones (sin tener en cuenta el outlier), tal y como hemos indicado anteriormente.

Para ver la curva Normal que mejor se ajusta al histograma, usamos la función denscompdel paquete fitdistrplus y observamos que, efectivamente, podemos suponer que la distribución subyacente es una Normal.

```{r}
ajusteNormal = fitdist(columna , "norm")
denscomp (ajusteNormal,  xlab = nombre.columna)
```

### Test de Grubs

Una vez que hemos visto que los datos siguen una distribución no demasiado alejada de la Normal, procedemos a aplicar el test de Grubbs.
```{r}
test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs$p.value
```

El p-value es > 0.05, por lo que el test no puede rechazar. Así pues, aunque Toyota Corolla tiene un valor alto en mpg, no podemos deducir que realmente sea un outlier desde el punto de vista estadístico.

En el caso de que el estudio de los outliers lo hubiésemos empezado directamente con el test de Grubbs, la anterior función sólo nos dice si el valor más alejado de la media puede considerarse un outlier. ¿Pero a qué valor corresponde? Para responder esta pregunta, usamos la función outlier del paquete outliers. Es importante enfatizar que la función outlier no realiza ningún test. Simplemente nos da información referente a las diferencias de cada valor con respecto a la media. 

Para conocer el valor que toma el registro que más se aleja de la media, basta pasar como parámetro la columna de datos a la función outlier:
```{r}
valor.posible.outlier = outlier(columna)
valor.posible.outlier
```

Efectivamente, el valor del Toyota Corolla en la columna mpg es 33.9. Para obtener el identificador de dicho registro, pasamos como parámetro adicional a la función outlier el valor logical = TRUE: ésto nos devuelve un vector de bools en el que todos son FALSE excepto el valor que está más alejado de la media. Basta pues, usar which para ver su identificador:

```{r}
es.posible.outlier = outlier(columna, logical = TRUE)
clave.posible.outlier = which( es.posible.outlier == TRUE)
clave.posible.outlier
```

Lo que nos devuelve la clave 20 (la clave de Toyota Corolla).

### Test de Normalidad

Vamos a comprobar que los datos que quedan después de eliminar el outlier detectado por el test de Grubss siguen una distribución Normal. En el conjunto mtcars no hay ningún registro de ninguna variable que el test de Grubbs etiquete como outlier. Por lo tanto, para ilustrar el proceso que vamos a seguir, vamos a usar un conjunto de datos sintético. En el trabajo final que usted debe desarrollar (en su caso) use la misma variable que hubiese seleccionado al principio. Hágalo aunque el test de Grubbs no haya detectado ningún outlier, para así comprobar si la variable se distribuye según una distribución Normal.

El test de hipótesis que se plantea es el siguiente:

```
H0:La distribución subyacente de la variable es una Normal
```

Observe que el tipo de hipótesis nula es diferente que el del test de outliers. En este caso, la hipótesis nula es una afirmación (en el caso del test de Grubbs era una negación). Por lo tanto, si se rechaza, podemos afirmar que los datos no vienen de una Normal. En el caso de que no se pueda rechazar, sólo podremos afirmar que los datos no contradicen la hipótesis nula y por tanto, podremos asumir (pero sin garantía estadística) que se satisface el requisito de Normalidad de los datos.

Hay varios tests de hipótesis para comprobar el ajuste de una distribución (por orden de importancia):

- El test de Shapiro-Wilks (shapiro.test) es un test específico para la distribución Normal. Es el preferible cuando hay pocos datos (menos de 50)

- El test de Anderson-Darling es un test para cualquier distribución. Requiere que se conozcan los parámetros de la distribución, aunque suele utilizarse con las estimaciones de éstos. Está disponible a través de gofstat$adtest del paquete fitdistrplus

- El test de Kolomogorov-Smirnov (shapiro.test) es otro test genérico aplicable a cualquier distribución (sólo compara la mayor diferencia observada entre los datos y la media). Requiere conocer los parámetros de la distribución. En el caso de la Normal, se usa la variante de Lilliefors (lillie.test del paquete nortest) que no requiere el conocimiento de éstos.

Vamos a trabajar con los dos primeros tests. Construimos un dataset artificial

```{r}
datos.artificiales = c(45,56,54,34,32,45,67,45,67,65,140)
```

y lanzamos el mismo proceso anterior para detectar el posible outlier O:
```{r}
test.de.Grubbs = grubbs.test(datos.artificiales, two.sided = TRUE)
test.de.Grubbs$p.value

valor.posible.outlier = outlier(datos.artificiales)
valor.posible.outlier

es.posible.outlier = outlier(datos.artificiales, logical = TRUE)
es.posible.outlier

clave.posible.outlier = which(es.posible.outlier == TRUE)
clave.posible.outlier
```

Pasamos los tests de Normalidad al conjunto de datos eliminando previamente el outlier O:
```{r}
datos.artificiales.sin.outlier = datos.artificiales[-clave.posible.outlier]
datos.artificiales.sin.outlier

shapiro.test(datos.artificiales)

goodness_fit = gofstat(ajusteNormal)
goodness_fit$adtest
```

El test de Anderson-Darling no se ha podido aplicar porque hay pocos datos. El test de Shapiro no puede rechazar la hipótesis nula de Normalidad (p-value > 0.05) Así pues, podemos asumir que los datos no contradicen que la distribución subyacente sea una Normal.

En resumen, podemos concluir que los valores presentes en datos.artificiales son compatibles con una distribución Normal y que el registro 11 con un valor de 140 es el que más se aleja de la media y puede considerarse un outlier con garantía estadística según el test de Grubbs.

Construya una función con el nombre test_Grubbs que devuelva una lista con los cómputos anteriores. También debe lanzar el test de Normalidad sobre la columna elegida (una vez eliminado el posible outlier). Concretamente, la función pedida debe tener la siguiente cabecera:

```{r}
#######################################################################
# Aplica el test de Grubbs sobre la columna ind.col de datos y devuelve una lista con:

# nombre.columna: Nombre de la columna datos[, ind.col]
# clave.mas.alejado.media: Clave del valor O que está más alejado de la media
# valor.mas.alejado.media: Valor de O en datos[, ind.col]
# nombre.mas.alejado.media: Nombre de O en datos
# es.outlier: TRUE/FALSE dependiendo del resultado del test de Grubbs sobre O
# p.value:  p-value calculado por el test de Grubbs
# es.distrib.norm: Resultado de aplicar el test de Normalidad 
#    de Shapiro-Wilks sobre datos[, ind.col]
#    El test de normalidad se aplica sin tener en cuenta el 
#    valor más alejado de la media (el posible outlier O)
#    TRUE si el test no ha podido rechazar
#       -> Sólo podemos concluir que los datos no contradicen una Normal
#    FALSE si el test rechaza 
#       -> Los datos no siguen una Normal

# Requiere el paquete outliers

test_Grubbs = function(datos, ind.col, alpha = 0.05) {
  columna <- datos[,ind.col]
  res <- list()
  
  # Nombre columna
  res$nombre.columna <- colnames(datos)[ind.col]
    
  # Búsqueda del outlier
  es.posible.outlier <- outlier(columna, logical = TRUE)
  
  res$clave.mas.alejado.media <- which(es.posible.outlier == TRUE)
  res$valor.mas.alejado.media <- outlier(columna)
  res$nombre.mas.alejado.media <- rownames(datos)[res$clave.mas.alejado.media]
  
  # Test de Grubbs
  test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)

  res$es.outlier <- ifelse(test.de.Grubbs$p.value <= alpha, TRUE, FALSE)
  res$p.value <- test.de.Grubbs$p.value

  # Test de normalidad
  test.Normalidad <- shapiro.test(columna[-res$clave.mas.alejado.media])
  res$es.distrib.norm <- ifelse(test.Normalidad$p.value > alpha, TRUE, FALSE)
  
  res
}
  
df.datos.artificiales = as.data.frame(datos.artificiales)

test.Grubbs.datos.artificiales = test_Grubbs(df.datos.artificiales, 1)

test.Grubbs.datos.artificiales
```

```{r}
test.Grubbs.datos.num = test_Grubbs(datos.num, indice.columna)

test.Grubbs.datos.num
```

## Trabajando con varias columnas

### Outliers IQR

Empezamos con los outliers IQR: vamos a calcular los outliers IQR con respecto a cada una de las columnas. El conjunto de ellos nos dará aquellos registros que son outliers con respecto a alguna columna.
Para ello, llamamos a la siguiente función (disponible en OutliersFunciones_byCubero.R) y guardamos el resultado en la variable claves.outliers.IQR.en.alguna.columna (mire el código de la función para ver cómo utiliza sapply)

```{r}
claves.outliers.IQR.en.alguna.columna =
  claves_outliers_IQR_en_alguna_columna(datos.num, 1.5)

claves.outliers.IQR.en.alguna.columna
```

En este ejemplo, no hay registros duplicados pero podría haberlos. En ese caso, construimos sendas variables claves.outliers.IQR.en.mas.de.una.columna con todos aquellos registros que aparecen más de una vez y modificamos la variable claves.outliers.IQR.en.alguna.columna para que no aparezcan registros repetidos:

```{r}
claves.outliers.IQR.en.mas.de.una.columna = 
  unique(
    claves.outliers.IQR.en.alguna.columna[
      duplicated(claves.outliers.IQR.en.alguna.columna)])
claves.outliers.IQR.en.alguna.columna = 
  unique (claves.outliers.IQR.en.alguna.columna)


claves.outliers.IQR.en.mas.de.una.columna
claves.outliers.IQR.en.alguna.columna 
nombres_filas(datos.num, claves.outliers.IQR.en.mas.de.una.columna)
nombres_filas(datos.num, claves.outliers.IQR.en.alguna.columna)
```

Vamos a ver los valores normalizados de estos outliers. Nos debe salir lo siguiente:
```{r}
datos.num.norm[claves.outliers.IQR.en.alguna.columna,]
```

Vamos a ver esta misma información de forma gráfica. Para ello, utilice el vector claves.outliers.IQR.en.alguna.columna para pasarlo como parámetro a la función diag_caja_juntos. De esta forma, obtendremos los diagramas de cajas de todas las variables y se mostrarán los valores que toman los outliers con respecto a alguna columna. Hágalo con los outliers normales y con los extremos. En nuestro ejemplo, como no hay outliers extremos, mostraremos los resultados con los outliers normales. Debe salir lo siguiente:
```{r}
diag_caja_juntos(datos.num.norm, "Outliers en alguna columna", claves.outliers.IQR.en.alguna.columna)
```

Vemos, por ejemplo, que el Toyota Corolla se dispara (por arriba) en mpg pero no tanto en el resto de columnas. Parece por tanto un coche bastante equilibrado que consume muy poco.

Por otra parte, el Maserati Bora se dispara en hp (por arriba) y algo menos en qsec (por abajo): es un coche muy potente lo que le permite obtener una aceleración muy alta. Además, tiene un consumo (mpg) bastante moderado para ser un coche de esas características.

Es llamativo el caso del Merc 230 que tenga una aceleración tan baja, la menor de todos los coches. Habría que determinar si se trata de un error en la toma de datos o simplemente los ingenieros diseñaron el vehículo con esas características.

También es llamativo el bloque de coches Lincoln Continental, Chrysler Imperial, Cadillac Fletwood. Son coches muy pesados, con mucha cilindrada y que consumen mucho. Los típicos coches americanos.

### Test de Hipótesis

Vamos a ejecutar el test de Grubbs sobre las columnas de datos.num. En primer lugar, analizamos los histogramas de las variables para ver aquellas que se ajustan a una distribución Normal. Podemos usar los gráficos que generamos en el apartado Datasets y Selección de Variables o bien generarlos con las funciones fitdist y denscomp tal y como hicimos en el apartado Comprobación de la Hipótesis de Normalidad (tendrá que recorrer todas las columnas con sapply). Si lo hace de esta segunda forma, le debe salir lo siguiente:
```{r}
par(mfrow = c(2,3))
datos.num %>% apply(2, function(columna) {
  ajusteNormal = fitdist(columna , "norm")
  denscomp (ajusteNormal,  xlab = nombre.columna)
})
```

Tal y como vimos en el apartado Datasets y Selección de Variables, la variable disp es la que más se aleja de una Normal. En cualquier caso, la mantenemos por ahora. Pasamos el test de Grubbs a todas las columnas. Para ello, utilice sapply (también podría haber usado apply, pero los resultados no se muestran de una forma tan compacta). Debe obtener lo siguiente:
```{r}
sapply(1:ncol(datos.num), test_Grubbs, datos=datos.num)
```

En primer lugar, analizamos el test de Normalidad de Shapiro-Wilks. Recordemos que la función test_Grubbs la habíamos construido de forma que aplicaba el test después de haber eliminado el posible outlier de la columna correspondiente. El test rechaza en las variables disp (como ya habíamos supuesto) y drat. Así pues, podemos afirmar que dichas variables no siguen una distribución Normal. En cuanto al resto de variables, el test no puede rechazar por lo que concluimos que dichas variables puede considerarse que siguen una Normal. Recuerde que no tenemos ninguna garantía estadística ya que el test no ha rechazado y realmente lo único que podemos afirmar es que los datos no contradicen la hipótesis de Normalidad.

Por otra parte, vemos que ninguno de los outliers IQR pueden considerarse realmente outliers con garantía estadística. Los candidatos que han estado más cerca de considerarse outliers según el test de Grubbs son el Maserati Bora(columna hp, p-value 0.111) y Merc 230 (columna qsec, p-value = 0.08)

# Outliers Multivariantes

## Métodos estadísticos basados en la distancia de Mahalanobis

Para encontrar outliers multivariantes con técnicas estadísticas, vamos a aplicar las que se basan en la distancia de Mahalanobis. Es importante destacar que la finalidad de estas técnicas es ofrecer una garantía estadística de que si un valor se etiqueta como outlier, realmente lo es. Por lo tanto, la hipótesis nula establece que no lo es, de forma que si se rechaza, estaremos seguros de que sí es un outlier:

```
H0:El valor más alejado del centro de la distribución no es un outlier
```

### Hipótesis de Normalidad
Los métodos basados en la distancia de Mahalanobis asumen que la distribución conjunta es una distribución Normal multivariante. Por lo tanto, la hipótesis nula es realmente la siguiente:

```
H0:El valor con mayor distancia de Mahalanobis al centro de la distribución vienede la misma distribución Normal multivariante que el resto de datos
```

Una condición necesaria para que un conjunto de variables siga una distribución Normal multivariante es que cada una de ellas siga una distribución normal 1-variante. Por lo tanto, lo primero que vamos a hacer es trabajar únicamente con aquellas variables que siguen una Normal. Para ello, usamos la función test_Grubbs que ya habíamos construido previamente. Recuerde que esta función devuelve una lista que incluye la propiedad es.distrib.norm que es un bool que nos dice si la variable en cuestión puede considerarse que sigue una distribución Normal. Utilícela con sapply para obtener un vector de bools son.col.normales. Utilice dicho vector para construir el dataset datos.num.distrib.norm que contendrá aquellas variables Normales del conjunto de datos datos.num. En nuestro ejemplo, recuerde que disp y drat (índices de variables 2 y 4) no eran variables Normales. Debe salir lo siguiente:
```{r}
test <- sapply(1:ncol(datos.num), test_Grubbs, datos=datos.num)
son.col.normales <- apply(test, 2, function(x) {
  x$es.distrib.norm
})
datos.num.distrib.norm = datos.num[,son.col.normales]

son.col.normales
head(datos.num.distrib.norm)
```

Ahora bien, el que las variables sigan una distribución Normal 1-variante no garantiza que el conjunto de ellas siga una distribución Normal multivariante. Es una condición necesaria pero no suficiente. Por lo tanto, tenemos que lanzar un test de Normalidad multivariante. Para ello, lanzamos la función mvn de la librería MVN. Lo hacemos sobre el conjunto de datos datos.num.distrib.norm:
```{r}
test.MVN = mvn(datos.num.distrib.norm, mvnTest = "energy")
test.MVN$multivariateNormality["MVN"]
test.MVN$multivariateNormality["p value"]
```


El test nos dice que la distribución conjunta de las variables mpg, hp, wt, qsec no es una Normal multivariante. Por lo tanto, no deberíamos aplicar el método basado en la distancia de Mahalanobis. De todas formas, vamos a lanzarlo para ver si detectamos algún valor que, aunque no pueda considerarse un outlier con garantía estadística, al menos proporcione alguna información interesante.

### Tests de hipótesis para detectar outliers
Vamos a usar la función cerioli2010.fsrmcd.test del paquete CerioliOutlierDetection (el paquete ofrece otra función cerioli2010.irmcd.test que, por simplicidad, no la veremos) La función cerioli2010.fsrmcd.test obtiene los outliers calculando las distancias de Mahalanobis usando una estimación de la matriz de covarianzas, según el método robusto MCD -minimum covariance determinant (la distribución del estadístico es la obtenida en Hardin-Rocke o Green and Martin) A título informativo, estos métodos robustos no incluyen los valores alejados del centro de la distribución en la estimación de la matriz de covarianzas. Para verlo visualmente, lancemos la función corr.plot (del paquete mvoutlier) sobre las dos primeras variables (es sólo un ejemplo):

```{r}
corr.plot(datos.num[,1], datos.num[,2])
```

Observe cómo cambia la forma de las elipses determinadas por la distancia de Mahalanobis. En rojo se muestran los puntos de la derecha que están más alejados del centro y que, por tanto, no se han usado en la estimación de la matriz de covarianzas.

Tenemos dos formas de llamar a la función cerioli2010.fsrmcd.test dependiendo del tipo de test que queramos realizar:

1. Si queremos lanzar el test siguiente:

```
H0:El valor con mayor distancia de Mahalanobis viene de la misma distribución Normal multivariante que el resto de datos
```

  llamaremos a la función con un valor de significación de 0.05 (parámetro signif.alpha). Éste sería el equivalente al test  de Grubbs en el que sólo se establece como posible outlier el valor más alejado del centro de la distribución. Lo llamaremos test individual

2. Si queremos lanzar el conjunto de tests siguientes:

```
∀i=1⋯n,  H0i:El i-ésimo valor viene de la misma distribuciónNormal multivariante que el resto de datos
```

  llamaremos a la función con un valor de significación penalizado, por ejemplo usando la corrección de Sidak: 1−(1−α)1/n. Ésta sería la forma de comprobar si cada uno de los valores es un outlier o no. Al penalizar el error de significación, controlamos el error FWER (consulte las transparencias) pero el test será muy conservador. Lo llamaremos test de intersección

La función cerioli2010.fsrmcd.test devuelve una lista y podremos acceder a las siguientes propiedades:

- outliers: Es un vector de bools en la que indica si el dato i-ésimo es un outlier. En el caso de que hayamos aplicado el test individual , sólo tenemos garantía estadística de que es un outlier el valor con mayor distancia de Mahalanobis.

- mahdist.rw: Es un vector con las distancias de Mahalanobis de cada valor. Realmente, son las distancias de Mahalanobis modificadas por los autores del paquete. Si necesita conocer las distancias de Mahalanobis no modificadas, debe acceder a la propiedad mahdist.

Aplique el test individual con un valor de significación de 0.05 y el test de intersección con un valor de 1−(1−0.05)1/n (n es el número de registros del conjunto de datos). Obtenga las claves de los outliers encontrados por ambos métodos. Para ello tendrá que acceder al vector outliers devuelto por la función cerioli2010.fsrmcd.test. Obtenga también los nombres de las filas correspondientes usando la función nombres_filas disponible en OutliersFunciones_byCubero.R. Como este tipo de métodos robustos usan un método aleatorio para iniciar el proceso de elección de los datos que participarán en el cómputo final, es necesario que establezcamos el valor de semilla para que los resultados que veamos en esta ejecución sean siempre los mismos. Así pues pondremos, por ejemplo, set.seed(2). Debe salir lo siguiente:

EL ALPHA DEL DE INTERSECCIÓN ES MUY PEQUEÑO. SI QUISIÉRAMOS BUSCAR K OUTLIERS NO SERÍA MEJOR PONER 0.05/K ??

```{r}
set.seed(2)

cerioli.individual <- cerioli2010.fsrmcd.test(datos.num, signif.alpha = 0.05)
claves.test.individual <- which(cerioli.individual$outliers)
nombres.test.individual <- nombres_filas(datos.num, claves.test.individual)

n <- nrow(datos.num)
alpha <- 0.05
cerioli.interseccion <- cerioli2010.fsrmcd.test(datos.num, signif.alpha = 1 - (1 - alpha)^(1/n))
claves.test.interseccion <- which(cerioli.interseccion$outliers)
nombres.test.interseccion <- nombres_filas(datos.num, claves.test.individual)

claves.test.individual
## [1]  9 17 29 31
nombres.test.individual
## [1] "Merc 230"          "Chrysler Imperial" "Ford Pantera L"    "Maserati Bora"
claves.test.interseccion
## integer(0)
nombres.test.interseccion
```

Observe que el test de intersección no devuelve ningún outlier, mientras que el test individual devuelve 4 outliers. Ya hemos explicado que sólo tenemos garantía estadística de que sea un outlier el que tiene mayor valor de distancia de Mahalanobis. Para ver cuál es ese valor, basta ordenar decrecientemente el vector mahdist.rw (use la función order con el parámetro decreasing = TRUE ) y seleccionar el primero. Muestre también un gráfico de todas las distancias de Mahlanobis obtenidas para que aprecie cuál es el mayor valor. Le debe salir lo siguiente:
```{r}
cerioli.individual$mahdist.rw %>% sort() %>% plot()
```


```{r}
clave.mayor.dist.Mah <- order(cerioli.individual$mahdist.rw , decreasing = TRUE)[1]
nombre.mayor.dist.Mah <- nombres_filas(datos.num, clave.mayor.dist.Mah)

cerioli.individual$mahdist
clave.mayor.dist.Mah
nombre.mayor.dist.Mah
```

Por lo tanto, podemos concluir que el test individual rechazaría la hipótesis de que el registro con clave 31 (Maserati Bora) no es un outlier. Así pues, lo aceptamos como outlier. Algunas consideraciones:

1. Recuerde que no hemos podido determinar que la distribución subyacente fuese una Normal, por lo que no tenemos garantía estadística de que, efectivamente, dicho registro sea un outlier (de que provenga de una distribución distinta del resto de los datos).

2. Bajo la premisa de lo dicho anteriormente, el test individual ha etiquetado al Maserati Bora como un outlier multivariante. Recuerde que dicho registro no fue etiquetado como outlier 1-variante en niguna variable por el test de Grubbs ya que tenía un valor muy alto en dos variables (hp y qsec), aunque no lo suficiente para que fuese un outlier. Sin embargo, al tener el mismo coche dos variables con valores muy altos, el test multivariante sí lo puede considerar como un outlier, ya que se suman las contribuciones de ambas variables.

## Visualización de datos con un Biplot
El BiPlot es una herramienta gráfica que nos permite tener una idea aproximada de los valores de los registros con respecto a todas las variables, así como las correlaciones entre dichas variables.

El Biplot muestra los registros (las filas del dataset) como puntos en un plano 2D (también podría usarse un gráfico tridimensional) En el mismo gráfico se representan las variables como flechas, indicando la dirección de crecimiento en dicha variable de los datos. Al pasar de n dimensiones a sólo 2, es obvio que se pierde información por lo que siempre debemos tener en cuenta que es una representación aproximada. La aproximación será mejor cuanto mayor sea la suma de los porcentajes explicados por cada eje del plano (componente principal).

Llamamos a la función biplot_2_colores disponible en OutliersFunciones_byCubero.R pasándole como primer parámetro el conjunto de datos y como segundo las claves de aquellos registros cuyos nombres queremos mostrar en el gráfico. En nuestro caso, le pasamos las claves de los outliers IQR.
```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num, 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

La suma de los porcentajes explicados es muy alta (19.1 + 69.8 = 88.9), por lo que la representación obtenida es una buena aproximación. Puede apreciarse en el gráfico que, efectivamente, Toyota Corolla se sitúa en la zona más alta de la variable mpg, al igual que Maserati Bora lo hace en la parte de valores muy altos de hp y en la de valores muy bajos de qsec (recuerde que las flechas indican la dirección de crecimiento) En la sección siguiente usaremos el biplot para mostrar los resultados de otros métodos de detección de outliers.

## Métodos basados en distancias: LOF

Los métodos estadísticos tienen como finalidad proporcionar garantía estadística de que los valores etiquetados como outliers efectivamente lo son. Para ello, presuponen que los datos siguen una distribución estadística concreta. Sin embargo, en las situaciones en las que este requisito no se cumple, no podemos aplicar dichos métodos. En estos casos, vamos a aplicar otros métodos que no ofrecen garantía estadística, pero son capaces de determinar cómo de alejado está cada punto al resto de los datos. Para ello, se usa una medida de distancia que, mientras no digamos lo contrario, será la distancia euclídea. Para que unas variables no dominen sobre otras tendremos que, obligatoriamente, normalizar los datos. Nosotros usaremos la normalización por z-score. De los métodos basados en distancia, aplicaremos uno de los más conocidos: LOF

En primer lugar, debemos determinar el número de vecinos más cercanos que usaremos en el cómputo del método LOF (consulte las transparencias de clase) A falta de más información, elegimos arbitrariamente el valor de 5. Llamamos a la función lofactor de la librería DMwR pasándole como parámetro el conjunto de datos numéricos, una vez normalizados (recuerde que era datos.num.norm):
```{r}
num.vecinos.lof = 5
lof.scores = lofactor(datos.num.norm, k = num.vecinos.lof)
claves.lof.ordenados <- order(lof.scores, decreasing = T)
```

La función lofactor asigna un score a cada dato, indicando hasta qué punto es un outlier. Ordenamos dicho vector de forma decreciente y mostramos en un gráfico los scores correspondientes. Nos debe salir lo siguiente:
```{r}
plot(sort(lof.scores, decreasing = T))
```

Podemos apreciar que hay un grupo de tres valores con scores más altos que el resto de datos. Vamos a analizar dichos valores. Establecemos la variable num.outliers en 3 y obtenemos sus claves junto con sus nombres (use la función nombres_filas). Nos debe salir lo siguiente:
```{r}
num.outliers <- 3

claves.outliers.lof <- order(lof.scores, decreasing = T) %>% head(num.outliers)
nombres.outliers.lof <- nombres_filas(datos.num.norm, claves.outliers.lof)

claves.outliers.lof
nombres.outliers.lof
```

Mostramos también los valores normalizados de dichos registros:
```{r}
datos.num.norm[claves.outliers.lof, ]
```

Viendo estos datos, es posible que el Lincoln Continental y el Cadillac Fletwood hayan obtenido un score alto debido simplemente a que tenían valores extremos en una única variable (y por tanto eran puntos alejados del resto) Posteriormente analizaremos con más detalle esta cuestión. Por ahora, vamos a analizar el registro que tiene el mayor score en LOF. Corresponde al Valiant. Podemos apreciar que no tiene un valor extremo en ninguna variable por separado (el más extremo es -1.564608 en drat)

Vamos a empezar viendo las posibles interacciones de dos variables. Recuerde que el método LOF tiene en cuenta todas las variables a la hora de calcular el valor del score, por lo que no podemos extraer conclusiones definitivas analizando únicamente las interacciones de dos variables. En cualquier caso, obtendremos una idea aproximada de la situación del outlier.

Para ello, mostramos los diagramas de dispersión corespondientes a los cruces 2 a 2 de las variables. Para ello, ejecutamos el siguiente código, que muestra en rojo el registro correspondiente al Valiant (en general, el que ha obtenido un mayor score):
```{r}
clave.max.outlier.lof = claves.outliers.lof[1]

colores = rep("black", times = nrow(datos.num.norm))
colores[clave.max.outlier.lof] = "red"
pairs(datos.num.norm, pch = 19,  cex = 0.5, col = colores, lower.panel = NULL)
```

Podemos apreciar que, por ejemplo, hay una correlación inversa entre mpg y disp. En este sentido, el valor de Valiant no contradice esta correlación ya que se sitúa en mitad de la nube de puntos. Por lo tanto, si sólo tuviésemos en cuenta estas dos variables, un método estadístico no lo hubiera marcado como outlier. Sin embargo, no hay apenas vehículos en ese rango de valores: está él y otro más. Así pues, Valiant está aislado en una zona de puntos, cercano a otras dos zonas de puntos de alta densidad, por lo que es posible que este hecho haya influido en el score asignado por el método LOF.

Si nos fijamos en la combinación drat y qsec, vemos que no hay ninguna correlación entre ambas variables, pero el registro Valiant se sitúa de nuevo en una zona aislada cerca de una nube de puntos de alta densidad a su izquierda, por lo que también es posible que ésto haya influido en el método LOF.

Una vez que tenemos una idea aproximada, vamos a ver de un forma gráfica la interacción de todas las variables (no sólo 2 a 2) Para ello, usamos un biplot. A diferencia de los diagramas de dispersión, el biplot muestra el comportamiento de los datos con respecto a todas las variables. Sin embargo, la información obtenida no es exacta y es proporcional al porcentaje de variación explicado por las componentes principales. En nuestro ejemplo, la suma de la variabilidad explicada por las dos componentes principales es muy alta (casi un 90%) y por tanto la aproximación es muy buena.

Para mostrar el biplot ejecutamos el siguiente código:
```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.norm, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```

Valiant está en una zona con poca densidad, pero bastante cerca a otra zona de alta densidad (la de los vehículos que están en la parte central-izquierda del gráfico) Este grupo de vehículos correspondería a los que tienen un valor algo superior al normal en las variables hp, disp, wt bastante por debajo de lo normal en drat y algo por debajo de lo normal en mpg. Son vehículos con bastante potencia, cilindrada y peso, pero parece que al tener una relación con el eje trasero baja, les permite tener una aceleración media y un consumo algo menor de la media. Sin embargo el Valiant, es un vehículo con valores no demasiado distintos en disp, wt, drat, mpg, algo menor en hp pero un valor de qsec mucho mayor. Así pues, es un coche de consumo normal pero con una aceleración muy baja en relación a otros vehículos con valores similares de drat, disp, wt pero con algo más de potencia (hp). Así pues, a primera vista, el Valiant no es un vehículo cuya compra estaría recomendada, aunque obviamente habría que tener en cuenta otros factores no incluidos en el conjunto de datos como por ejemplo el diseño, precio, fiabilidad, etc.

En cualquier caso, no olvidemos que el método LOF depende del número de vecinos elegido. De hecho, en este ejemplo, si hubiésemos cambiado la variable num.vecinos.lof habríamos obtenido otro resultado.

## Métodos basados en Clustering

En este apartado vamos a ver otro método basado en distancias. Detectaremos outliers según la distancia de cada dato al centroide de su cluster. El centroide podrá ser cualquiera (podrá provenir de un k-means o ser un medoide, por ejemplo). Recordemos que al ser un método basado en distancias, debemos trabajar con los datos normalizados. Empezamos con k-means

### Clustering usando k-means

A falta de más información, fijamos el número de outliers en 5 y el de clusters en 3. Además, como los resultados del método de clustering k-means dependen de la elección inicial de los centroides, fijamos un valor de la semilla con la función set.seed para que, de esta forma, no varíen los resultados de una ejecución a otra.
```{r}
num.outliers = 5
num.clusters = 3
set.seed(2)
```

Construimos el modelo kmeans (modelo.kmeans) llamando a la función kmeans. Nos devuelve una lista con las siguientes propiedades:

- cluster: contiene los índices de asignación de cada dato al cluster correspondiente. El resultado lo guardamos en la variable asignac.clust.

  Por ejemplo, si el dato con índice 69 está asignado al tercer cluster, en el vector asignac.clust habrá un 3 en la componente número 69.

- centers: contiene los datos de los centroides. Los datos están normalizados por lo que los centroides también lo están. El resultado lo guardamos en la variable centroides.normalizados

Nos debe salir lo siguiente:
```{r}
modelo.kmeans <- kmeans(datos.num.norm, num.clusters)
asignaciones.clustering.kmeans <- modelo.kmeans$cluster
centroides.normalizados <- modelo.kmeans$centers

head(asignaciones.clustering.kmeans)
centroides.normalizados
```

En el caso de que necesite conocer los valores sin normalizar que le corresponderían a los datos de los centroides, puede usar la función desnormaliza disponible en OutliersFunciones_byCubero.R:
```{r}
centroides.desnormalizados = desnormaliza(datos.num, centroides.normalizados)
centroides.desnormalizados
```

Ya podemos calcular los outliers como aquellos datos que más se alejan del centroide del cluster al que ha sido asignado. Esto lo vamos a hacer construyendo una función top_clustering_outliers que debe tener la siguiente cabecera:
```{r}
#######################################################################
# Calcula las distancias de los datos a los centroides
# y se queda con los primeros (tantos como indica num.outliers)
# Devuelve una lista con las claves de dichos registros y las
# correspondientes distancias a sus centroides

 
top_clustering_outliers = function(datos.normalizados, 
                                   asignaciones.clustering, 
                                   datos.centroides.normalizados, 
                                   num.outliers){
  distancias <- distancias_a_centroides(datos.normalizados, 
                                        asignaciones.clustering,
                                        datos.centroides.normalizados)
  claves <- distancias %>% order(decreasing = T) %>% head(num.outliers)
  
  res <- list()
  res$distancias <- distancias %>% sort(decreasing = T) %>% head(num.outliers)
  res$claves <- claves
  res
}
```

Implemente esta función de la siguiente forma: si un dato di ha sido asignado a un cluster Ck, calculamos la distancia euclídea disti de di al centroide de Ck. Los outliers serán los datos con mayores valores de disti. Para ello, debe calcular el vector de todas las distancias distancias, ordenarlo y quedarse con los primeros (tantos como indique la variable num.outliers) De esa forma obtendrá el vector claves. La función top_clustering_outliers devolverá una lista con ambos valores, a saber, distancias y claves. Para calcular las distancias use la siguiente función disponible en OutliersFunciones_byCubero.R.

Le debe salir lo siguiente:
```{r}
top.outliers.kmeans = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.kmeans, 
                                              centroides.normalizados, 
                                              num.outliers)
claves.outliers.kmeans = top.outliers.kmeans$claves 
nombres.outliers.kmeans = nombres_filas(datos.num, claves.outliers.kmeans)
distancias.outliers.centroides = top.outliers.kmeans$distancias

claves.outliers.kmeans
nombres.outliers.kmeans
distancias.outliers.centroides
```

Vamos a mostrar un biplot con la información de los outliers y de los clusters. Para ello, usamos la función biplot_outliers_clustering disponible en OutliersFunciones_byCubero.R:

Esta función llama a otra función más genérica biplot_colores_formas que muestra un biplot de un conjunto de datos diferenciados por color y por forma. En nuestro caso, los colores corresponden a las asignaciones de los clusters y las formas a si es o no un outlier. Esta segunda función no la necesita para estas prácticas pero le puede ser útil en otras aplicaciones.
```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers k-means",
                           asignaciones.clustering = asignaciones.clustering.kmeans,
                           claves.outliers = claves.outliers.kmeans)
```

Podemos apreciar que cuatro de los cinco outliers detectados por este método están en el exterior de la nube de puntos, por lo que es muy posible que se hayan etiquetado como outliers porque tienen un valor muy alto en una o varias variables. Estos coches son Ford Pantera L, Honda Civic, Cadillac Fletwood y Merc 230. Los registros Merc 230 y Cadillac Fletwood ya los conocíamos porque eran outliers con respecto a 1 columna. Ahora han aparecido dos nuevos: Ford Pantera L y Honda Civic. Veamos el diagrama de cajas conjunto para hacernos una idea de los valores que toman:
```{r}
diag_caja_juntos(datos.num, "Outliers k-means", claves.outliers.kmeans)
```

Parece ser que, efectivamente, Ford Pantera L y Honda Civic han sido etiquetados como outliers porque tienen valores algo extremos (sin llegar a ser muy extremos) en varias variables, por lo que la suma de los efectos de dichas variables han podido determinar que tengan scores altos. Por ejemplo, Honda Civic tiene valores algo extremos en todas las variables salvo en qsec.

El único registro que no está en el exterior del biplot y ha sido etiquetado como outlier es Ferrari Dino. El diagrama de cajas también muestra que, efectivamente, no tiene un valor extremo en una o varias variables. Posteriormente analizaremos con más detalle este caso.

### Clustering usando medoides

En este apartado vamos a usar el método de clustering PAM (Partition around medoids) Previamente tenemos que calcular la matriz de distancias de todos con todos usando la función dist. A continuación, usamos la función pam del paquete cluster, pasándole como parámetros la matriz de distancias y k = número de clusters. Guardamos el resultado en modelo.pam
```{r}
set.seed(2)
matriz.distancias = dist(datos.num.norm)
modelo.pam        = pam(matriz.distancias , k = num.clusters)
```

Para obtener las asignaciones de cada dato a su cluster accedemos a modelo.pam$clustering. Aunque no sea de interés para el cálculo de los outliers, podemos ver la información de los medoides. Para ver los nombres de los medoides accedemos a modelo.pam$medoids. Mostramos también los valores que toman los medoides en todas las variables para hacernos una idea de cómo son los representantes de los clusters encontrados por pam (ahora no hace falta desnormalizar como hicimos en kmeans ya que los medoides son valores reales del conjunto de datos). También mostramos los valores normalizados.
```{r}
asignaciones.clustering.pam = modelo.pam$clustering   
nombres.medoides = modelo.pam$medoids    
medoides = datos.num[nombres.medoides, ]
medoides.normalizados = datos.num.norm[nombres.medoides, ]

nombres.medoides
medoides
medoides.normalizados
```
Calculamos ahora los top outliers. Para ello llamamos a la función top_clustering_outliers. Mostramos también el biplot correspondiente llamando a la función biplot_outliers_clustering. Nos debe salir lo siguiente:
```{r}
top.outliers.pam = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.pam, 
                                              medoides.normalizados, 
                                              num.outliers)
claves.outliers.pam = top.outliers.pam$claves 
nombres.outliers.pam = nombres_filas(datos.num, claves.outliers.pam)

claves.outliers.pam
nombres.outliers.pam
```


```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers PAM",
                           asignaciones.clustering = asignaciones.clustering.pam,
                           claves.outliers = claves.outliers.pam)
```

Podemos apreciar que, en este ejemplo, al usar la partición creada por pam, los outliers encontrados corresponden a registros que tienen un valor extremo en alguna de las variables.

### Análisis de los outliers multivariantes puros

Un outlier multivariante puede ser etiquetado como tal por varias razones:

- El outlier tiene un valor extremo en alguna variable. Es decir, en una única variable, dicho registro presenta un valor extremo y por tanto esa variable contribuye de forma decisiva en el cómputo global. Puede ser el caso del Merc 230 que era un outlier en qsec.

  También sería el caso del grupo de vehículos Lincoln Continental, Chrysler Imperial, Cadillac Fletwood que eran outliers no sólo en una variable sino en varias (coches muy pesados, con mucha cilindrada y que consumen mucho)

  Estos outliers se sitúan en la periferia del biplot.

- El outlier tiene valores relativamente extremos en más de una variable, aunque no llega a ser un outlier en ninguna de ellas. El efecto sumado de dichas variables hace que el registro sea etiquetado como outlier. Era el caso del Ford Pantera L y Honda Civic.

  Al igual que los outliers anteriores, éstos también suelen situarse en la periferia del biplot.

- El outlier no tiene valores extremos en ninguna variable pero, sin embargo, presenta una combinación inusual de valores de dos o más variables.

  Estos outliers suelen estar en la zona interior del biplot.

- El outlier tiene alguna otra característica que depende del método de detección aplicado. Por ejemplo, los métodos basados en distancia detectan aquellos valores que están aislados del resto de valores. Concretamente, el método LOF tiene en cuenta la densidad relativa de los vecinos próximos. Realmente, podría considerarse que este tipo de outliers aislados son un tipo particular de los anteriores (registros con combinaciones inusuales de variables)

Informalmente, diremos que los tres últimos tipos de outliers son outliers multivariantes puros, es decir, aquellos que no son outliers con respecto a una única variable. De hecho, ya hemos detectado algunos en los apartados anteriores. Por ejemplo, el método k-means había identificado como outliers a Ford Pantera L y Honda Civic ya que presentaban valores algo extremos en varias variables. También identificó a Ferrari Dino como outlier, aunque éste no parecía que tuviese valores extremos en varias variables. Este vehículo será analizado con más detalle posteriormente.

Lo que vamos a hacer ahora es automatizar el proceso. Para ello, vamos a identificar a los outliers en una única variable usando el método IQR: recuerde que habíamos obtenido los outliers en alguna columna (claves.outliers.IQR.en.alguna.columna) Bastará por tanto ver los outliers que son multivariantes pero no son 1-variantes (con respecto a ninguna variable). Este proceso lo podemos aplicar sobre cualquiera de los métodos vistos anteriormente (estadísticos, basados en distancia o basados en clustering). Nosotros vamos a usar el método LOF, que es uno de los que mejores resultados dan en una gran variedad de situaciones.

Así pues, se le pide que calcule los registros que están en claves.outliers.lof pero no están en claves.outliers.IQR.en.alguna.columna (deberá aplicar la función setdiff). Debe salir lo siguiente:
```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.outliers.lof, claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.outliers.lof
claves.outliers.lof.no.IQR
nombres.outliers.lof.no.IQR
```

Así pues, el único outlier LOF multivariante puro es el correspondiente al Valiant. En la sección LOF vimos que era un outlier porque, si bien no tenía valores extremos en cada variable, no había otros registros (salvo uno) que tuviesen valores similares (era un registro aislado del resto) Analizando los valores del Valiant vimos que era un coche poco recomendable (en comparación a los otros coches) atendiendo a los parámetros de aceleración y consumo.

Sería interesante intentar extraer alguna otra información adicional del conjunto de datos, aumentando el número de outliers a estudiar. Si recuerda la gráfica de los scores LOF, había un grupo de 3 registros con mayores valores de score, pero también había otro grupo de 8 registros con scores notablemente superiores al resto. Vamos por tanto a analizar de nuevo los resultados del método LOF aumentando el número total de outliers a 11. Para ello, basta con que seleccione los 11 primeros registros del vector claves.lof.ordenados y calcular de nuevo los que son outliers LOF pero no 1-variantes. Le debe salir lo siguiente (el vector claves.outliers.IQR.en.alguna.columna no cambia):
```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.lof.ordenados %>% head(11), claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.lof.ordenados %>% head(11)
claves.outliers.lof.no.IQR
nombres.outliers.lof.no.IQR
```

Mostramos el biplot de los outliers puros:
```{r}
biplot <- biplot_2_colores(datos.num, 
                           titulo = "Outliers LOF (excluídos los que son IQR)",
                           claves.a.mostrar = claves.outliers.lof.no.IQR)
biplot
```

Puede apreciar que el Hornet 4 Drive es un outlier muy similar a Valiant, aunque este último tenía un mayor score. El resto de outliers, exceptuando a Ferrari Dino presentan valores extremos en varias variables. Por ejemplo, Camaro Z28 y Duster 360 tienen un valor muy alto en hp y muy bajo en qsec por lo que posiblemente el efecto sumado de ambas variables haya contribuido a que tengan un alto score. Nos fijamos por tanto en el Ferrari Dino que no parece que tenga un valor extremo en ninguna variable (se sitúa en la zona central del biplot). Veamos los datos normalizados:
```{r}
datos.num.norm[claves.outliers.lof.no.IQR, ]
```

Ferrari Dino es un dato con valores interesantes porque tiene:

- Sólo algo más de potencia que los otros coches (hp = 0.4129) El Maserati Bora tiene mucha más potencia (2.7)
- Una aceleración muy notable (qsec = -1.3143) El Maserati Bora tiene algo más, pero tampoco mucho (-1.8)
- Un consumo excelente para los coches de su categoría ya que consume lo mismo que la media (mpg= -0.0648)
- Menos centímetros cúbicos (disp = -0.6916) El Maserati Bora tiene más que la media (0.567)

Los ingenieros del Ferrari Dino han sido capaces de conseguir un coche con una aceleración muy notable y un consumo excelente sin tener que recurrir a aumentar la potencia y los centímetros cúbicos. Posiblemente también habrá contribuido a alcanzar estos buenos resultados el tener un peso bastante bajo (wt = -0.4570) algo inusual en coches de esas características (El Maserati tiene 0.3)

# Análisis de resultados

Conjunto de datos

Hemos trabajado con la base de datos mtcars disponible en R. Hemos suprimido las variables que tenían pocos valores distintos (cyl, vs, am, gear, carb) y nos hemos quedado con las variables siguientes: mpg, disp, hp, drat, wt, qsec

Hemos aplicado la normalización por z-score en aquellos métodos que así lo han requerido.

Outliers en una variable

Método IQR

No hemos encontrado outliers extremos (alejados de la media más de tres veces la distancia intercuartil) pero sí hay varios outliers no extremos (alejados de la media más de 1.5 veces la distancia intercuartil).

El Toyota Corolla se dispara (por arriba) en mpg pero no tanto en el resto de columnas. Parece por tanto un coche bastante equilibrado que consume muy poco.

Por otra parte, el Maserati Bora se dispara en hp (por arriba) y algo menos en qsec (por abajo): es un coche muy potente lo que le permite obtener una aceleración muy alta. Además, tiene un consumo (mpg) bastante moderado para ser un coche de esas características.

Es llamativo el caso del Merc 230 que tenga una aceleración tan baja, la menor de todos los coches. Habría que determinar si se trata de un error en la toma de datos o simplemente los ingenieros diseñaron el vehículo con esas características.

También es llamativo el bloque de coches Lincoln Continental, Chrysler Imperial, Cadillac Fletwood. Son coches muy pesados, con mucha cilindrada y que consumen mucho. Los típicos coches americanos.

Test de Hipótesis

El test de Shapiro-Wilks rechaza la Normalidad en las variables disp y drat. En cuanto al resto de variables, el test no puede rechazar por lo que concluimos que dichas variables puede considerarse que siguen una Normal. Recuerde que no tenemos ninguna garantía estadística ya que el test no ha rechazado y realmente lo único que podemos afirmar es que los datos no contradicen la hipótesis de Normalidad.

Por otra parte, vemos que ninguno de los outliers IQR pueden considerarse realmente outliers con garantía estadística. Los candidatos que han estado más cerca de considerarse outliers según el test de Grubbs son el Maserati Bora(columna hp, p-value 0.111) y Merc 230 (columna qsec, p-value = 0.08)

Outliers multivariantes

Visualización con biplot

La suma de los porcentajes explicados es muy alta (19.1 + 69.8 = 88.9), por lo que la representación obtenida es una buena aproximación.

Métodos estadísticos usando la distancia de Mahalanobis

La distribución conjunta de las variables mpg, hp, wt, qsec no es una Normal multivariante. Por lo tanto, no deberíamos aplicar el método basado en la distancia de Mahalanobis. De todas formas, lo hemos lanzado para ver si detectamos algún valor que, aunque no pueda considerarse un outlier con garantía estadística, al menos proporcione alguna información interesante.

El test de intersección no devuelve ningún outlier, mientras que el test individual devuelve 4 outliers. Recordemos que sólo tenemos garantía estadística de que sea un outlier el que tiene mayor valor de distancia de Mahalanobis. Dicho valor es el Maserati Bora. Dicho registro no fue etiquetado como outlier 1-variante en niguna variable por el test de Grubbs ya que tenía un valor muy alto en dos variables (hp y qsec), aunque no lo suficiente para que fuese un outlier. Sin embargo, al tener el mismo coche dos variables con valores muy altos, el test multivariante sí lo puede considerar como un outlier, ya que se suman las contribuciones de ambas variables. En cualquier caso, insistimos en que al no haber podido establecer que la distribución subyacente sea una Normal multivariante, no podemos concluir con garantía estadística que, efectivamente, sea un outlier.

LOF

El gráfico de scores mostró un grupo destacado de 3 registros: Lincoln Continental, Cadillac Fletwood y Valiant. Tanto Lincoln Continental como Cadillac Fletwood obtuvieron un score alto debido a que tienen valores extremos en una única variable, por lo que no resultan de interés como outliers multivariantes.

De entre los tres anteriores, el registro con mayor score es el Valiant. Este vehículo no tiene un valor inusual en ninguna variable (por separado), pero se sitúa en una zona con poca densidad, bastante cerca a otra zona de alta densidad que consiste en vehículos con bastante potencia, cilindrada y peso, pero parece que al tener una relación con el eje trasero baja, les permite tener una aceleración media y un consumo algo menor de la media. Sin embargo el Valiant, es un vehículo con valores no demasiado distintos en disp, wt, drat, mpg, algo menor en hp pero un valor de qsec mucho mayor. Así pues, es un coche de consumo normal pero con una aceleración muy baja en relación a otros vehículos con valores similares de drat, disp, wt pero con algo más de potencia (hp). Así pues, a primera vista, el Valiant no es un vehículo cuya compra estaría recomendada, aunque obviamente habría que tener en cuenta otros factores no incluidos en el conjunto de datos como por ejemplo el diseño, precio, fiabilidad, etc.

En una segunda ejecución de LOF se incluyó un grupo de otros 8 registros con altos scores. De entre los nuevos outliers detectados cabe destacar el Camaro Z28 y el Duster 360. Ambos tienen un valor muy alto en hp y muy bajo en qsec por lo que posiblemente el efecto sumado de ambas variables haya contribuido a que tengan un alto score.

Destaca también el Hornet 4 Drive que es muy similar a Valiant (que ya hemos analizado anteriormente), aunque este último tenía un mayor score.

Otro outlier a destacar es Ferrari Dino. Es un dato con valores interesantes porque tiene:

Sólo algo más de potencia que los otros coches (hp = 0.4129) El Maserati Bora tiene mucha más potencia (2.7)
Una aceleración muy notable (qsec = -1.3143) El Maserati Bora tiene algo más, pero tampoco mucho (-1.8) - Un consumo excelente para los coches de su categoría ya que consume lo mismo que la media (mpg-= 0.0648)
Menos centímetros cúbicos (disp = -0.6916) El Maserati Bora tiene más que la media (0.567)
Los ingenieros del Ferrari Dino han sido capaces de conseguir un coche con una aceleración muy notable y un consumo excelente sin tener que recurrir a aumentar la potencia y los centímetros cúbicos. Posiblemente también habrá contribuido a alcanzar estos buenos resultados el tener un peso bastante bajo (wt = -0.4570) algo inusual en coches de esas características (El Maserati tiene 0.3)

En resumen, el método LOF ha detectado outliers interesantes como por ejemplo el Valiant que es un coche con unos resultados inusuales por malos, así como el Ferrari Dino que destaca justo por lo contrario.

Métodos basados en clustering

Con el método de k-means (ordenando outliers según la distancia euclídea a los centroides) hemos detectado como outliers vehículos como Ford Pantera L y Honda Civic al tener valores bastante extremos (sin llegar a ser muy extremos) en varias variables, por lo que la suma de los efectos de dichas variables han podido determinar que tengan scores altos. Por ejemplo, Honda Civic tiene valores algo extremos en todas las variables salvo en qsec.

Este método también etiquetó a Ferrari Dino como outlier (registro que ya hemos analizado anteriormente)



















