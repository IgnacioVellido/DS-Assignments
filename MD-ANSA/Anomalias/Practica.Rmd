---
title: "Practica"
author: "Ignacio Vellido"
date: "12/9/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    toc: true
    highlight: github
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results="hold", fig.align="center", 
                      comment=NA, messages=FALSE)

library(ggplot2)   # Gr?ficos
library(fitdistrplus)  # Ajuste de una distribuciÃ³n -> denscomp 
library(reshape)   # melt
library(ggbiplot)  # biplot
library(tidyverse)   
library(outliers)  # Grubbs
library(MVN)       # mvn: Test de normalidad multivariante  
library(CerioliOutlierDetection)  #MCD Hardin Rocke
library(mvoutlier) # corr.plot 
library(DMwR)      # lof
library(cluster)   # PAM
```

```{r include=FALSE}
# M?ster -> Detecci?n de anomal?as
# Juan Carlos Cubero. Universidad de Granada

###########################################################################
# Funciones utilizadas a lo largo del curso
###########################################################################

# rm(list=ls()) 


###########################################################################
# Realiza un plot de todos los registros
# Permite cambiar el color con el que se visualiza un conjunto de registros. 
# Los registros que se muestran con otro color se especifican en el par?metro
# claves.a.mostrar 

plot_2_colores = function (datos, 
                           claves.a.mostrar, 
                           titulo = "",
                           colores = c("black", "red")){
  
  num.datos = nrow(as.matrix(datos))
  seleccionados =  rep(FALSE, num.datos)
  seleccionados[claves.a.mostrar] = TRUE
  colores.a.mostrar = rep(colores[1], num.datos)
  colores.a.mostrar [seleccionados] = colores[2]
  
  plot(datos, col=colores.a.mostrar, main = titulo)
}



###########################################################################
# Funci?n an?loga a son_outliers_IQR, salvo que devuelve un vector
# de claves en vez de un vector de bools

claves_outliers_IQR = function(datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  son.outliers.IQR = son_outliers_IQR(datos, ind.columna, coef)
  return (which(son.outliers.IQR  == TRUE))
}



###########################################################################
# Calcula los outliers IQR con respecto a una columna 
# Devuelve un vector de bools indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a la columna ind.columna
# coef es 1.5 para los outliers normales y hay que pasarle 3 para los outliers extremos

son_outliers_IQR = function (datos, ind.columna, coef = 1.5){
  columna.datos = datos[,ind.columna]
  cuartil.primero = quantile(columna.datos)[2]  
  #quantile[1] es el m?nimo y quantile[5] el m?ximo.
  cuartil.tercero = quantile(columna.datos)[4] 
  iqr = cuartil.tercero - cuartil.primero
  extremo.superior.outlier = (iqr * coef) + cuartil.tercero
  extremo.inferior.outlier = cuartil.primero - (iqr * coef)
  son.outliers.IQR  = columna.datos > extremo.superior.outlier |
    columna.datos < extremo.inferior.outlier
  return (son.outliers.IQR)
}


###########################################################################
# Calcula los outliers IQR con respecto a ALGUNA columna
# Devuelve un vector de claves indicando si el registro i-?simo 
# de datos es o no un outlier IQR con respecto a ALGUNA columna
# coef es 1.5 para los outliers normales y  3 para los outliers extremos

claves_outliers_IQR_en_alguna_columna = function(datos, coef = 1.5){
  df.clave.columnas = data.frame()
  claves.outliers =  sapply(1:ncol(datos), 
                               function(x) claves_outliers_IQR(datos, x, coef)
  )
  claves.outliers.en.alguna.columna = unlist(claves.outliers)
  return (claves.outliers.en.alguna.columna)
}




#######################################################################
# Devuelve los nombres de aquellas filas especificadas en el par?metro claves
# filas es un vector de bools 

nombres_filas = function (datos, claves) {
  num.claves = length(claves)
  nombres.filas = row.names(as.data.frame(datos))[claves]
  
  return (nombres.filas)
}




#######################################################################
# funci?n base para diag_caja_outliers_IQR y diag_caja

diag_caja_grafico_base = function(datos, indice.columna){
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  nombre.columna = colnames(datos)[indice.columna]
  ggboxplot = ggplot(data = as.data.frame(datos), 
                     aes(x=factor(""), 
                         y = datos[,indice.columna]) , 
                     environment = environment()) + 
              xlab(nombre.columna) + ylab("") 
  return (ggboxplot)
}

#######################################################################
# Muestra un diagrama de caja
# Calcula los outliers IQR y los muestra como puntos en rojo en un BoxPlot

diag_caja_outliers_IQR = function (datos, ind.columna, coef.IQR = 1.5){
  # Si quisi?semos l?neas horizontales en los l?mites de las cajas
  # habr?a que a?adir 
  # + stat_boxplot(geom = 'errorbar')   
  
   outliers.IQR = son_outliers_IQR(datos, ind.columna, coef = coef.IQR)
   ggboxplot =  diag_caja_grafico_base(datos, ind.columna) + 
                stat_boxplot(coef = coef.IQR) +
                geom_boxplot(coef = coef.IQR, outlier.colour = "red") 
                # Importante: geom_boxplot debe ir despu?s de stat_boxplot
   
   return (ggboxplot)
}



#######################################################################
# Muestra un diagrama de caja
# Tambi?n muestra las etiquetas de los registros indicados en 
# el par?metro claves.a.mostrar 

diag_caja = function (datos, ind.columna, claves.a.mostrar = c()){
  num.filas = nrow(datos)
  num.claves = length(claves.a.mostrar)
  nombres.filas = vector (mode = "character", length = num.filas)
  nombres.filas = rep("", num.filas)
  nombres.claves = nombres_filas(datos, claves.a.mostrar)

  for (i in num.claves)
    nombres.filas[claves.a.mostrar[i]]  = nombres.claves[i]
  

  ggboxplot = diag_caja_grafico_base(datos, ind.columna) + 
    geom_boxplot(outlier.shape = NA) + # Para que no imprima los outliers IQR calculados dentro del mismo geom_boxplot
    geom_text(aes(label = nombres.filas)) 
  
  return (ggboxplot)
}






#######################################################################
# Muestra de forma conjunta todos los diagramas de caja de las variables de datos
# Para ello, normaliza previamente los datos.
# Tambi?n muestra las etiquetas de los registros indicados en claves.a.mostrar
# Requiere reshape

diag_caja_juntos = function (datos, titulo = "", claves.a.mostrar = c()){  
  # Importante: Para que aes busque los par?metros en el ?mbito local, 
  # debe incluirse  environment = environment()
  
  # Para hacerlo con ggplot, lamentablemente hay que construir antes una tabla 
  # que contenga en cada fila el valor que a cada tupla le da cada variable 
  # -> paquete reshape->melt
  
  # Por ejemplo, si tenemos el siguiente data frame
  
  # datos = data.frame(
  #   A = c(1, 2),
  #   B = c(3, 4)
  # )
  # datos =
  #     A  B
  #     1  3
  #     2  4
  
  # melt(datos) construye esta tabla:
  
  #      variable value
  # 1        A     1
  # 2        A     2
  # 3        B     3
  # 4        B     4
  
  
  nombres.de.filas = nombres_filas (datos, claves.a.mostrar)
  
  datos = scale(datos)
  datos.melted = melt(datos)
  colnames(datos.melted)[2]="Variables"
  colnames(datos.melted)[3]="zscore"
  factor.melted = colnames(datos.melted)[1]
  columna.factor = as.factor(datos.melted[,factor.melted])
  levels(columna.factor)[!levels(columna.factor) %in% nombres.de.filas] = ""  
  
  ggplot(data = datos.melted, 
         aes(x=Variables, y=zscore), 
         environment = environment()) + 
    ggtitle(titulo) + 
    geom_boxplot(outlier.shape = NA) + 
    geom_text(aes(label = columna.factor), size = 3) 
}






#######################################################################
# Muestra un biplot del conjunto de datos
# Se muestran los nombres de los registros indicados en claves.a.mostrar
# El color usado para dichos registros es el segundo del par?metro colores
# El t?tulo para el grupo de dichos registros es el especificado en titulo.grupo.a.mostrar
# El par?metro titulo especifica el t?tulo principal del gr?fico

biplot_2_colores = function (datos, 
                             claves.a.mostrar = c(), 
                             titulo = "",
                             titulo.grupo.a.mostrar = "Outliers",
                             colores = c("black","red")){
  nombres = rownames(datos)
  claves.datos = c(1:nrow(datos))
  son.a.mostrar = claves.datos %in% claves.a.mostrar
  nombres[!son.a.mostrar] = ''

  PCA.model = princomp(scale(datos))
  outlier.shapes = c(".","x") 
  biplot = ggbiplot(PCA.model,
                    obs.scale = 1,
                    var.scale = 1 ,
                    varname.size = 5,
                    groups =  son.a.mostrar,
                    alpha = 1/2) #alpha = 1/10
  biplot = biplot + labs(color = titulo.grupo.a.mostrar)
  biplot = biplot + scale_color_manual(values = colores)
  biplot = biplot + geom_text(label = nombres,
                              stat = "identity",
                              size = 3,
                              hjust=0,
                              vjust=0)
  biplot = biplot + ggtitle(titulo)
}



#######################################################################
# Muestra un biplot de un conjunto de datos diferenciados por color
# El color lo determina la asignaci?n de cada dato a un cluster 
# Las asignaciones de datos a cluster se indican en asignaciones.clustering
# Tambi?n se muestran los outliers cuyas claves vienen indicadas en claves.outliers
 
biplot_outliers_clustering = function(datos, 
                                      titulo = "Outliers por el m?todo de Clustering", 
                                      titulo.color = "Asignaciones Clustering",
                                      titulo.outlier = "Outliers",
                                      asignaciones.clustering,
                                      claves.outliers){
  son.outliers = rep(FALSE, nrow(datos))
  son.outliers[claves.outliers] = TRUE
  
  bip = biplot_colores_formas(datos, 
                              titulo, titulo.color, titulo.outlier,
                              asignaciones.clustering,
                              son.outliers,
                              claves.outliers)
  bip 
}

#######################################################################
# Muestra un biplot del conjunto de datos
# Los datos se muestran diferenciados por color y por forma
# Las asignaciones de cada dato a su color y forma vienen dadas por los vectores
# asignaciones.colores y asignaciones.formas 
# Tambi?n se muestran las etiquetas de los registros indicados
# en el par?metro opcional claves.a.mostrar 

biplot_colores_formas = function (datos, 
                                  titulo, titulo.color = '', titulo.forma = '', 
                                  asignaciones.colores, asignaciones.formas,
                                  claves.a.mostrar = c()){
  PCA.model = princomp(scale(datos))
  
  son.a.mostrar = rep(FALSE, nrow(datos))
  son.a.mostrar[claves.a.mostrar] = TRUE
  nombres.a.mostrar = rownames(datos)
  nombres.a.mostrar[!son.a.mostrar] = ''

  asignaciones.colores = factor(asignaciones.colores)
  asignaciones.formas  = factor(asignaciones.formas)

  
  bip = ggbiplot(PCA.model, obs.scale = 1, var.scale=1 , varname.size = 3, alpha = 0) +              
    geom_point(aes(shape = asignaciones.formas, colour = asignaciones.colores))  +
    labs(shape = titulo.forma) +
    labs(colour = titulo.color) +
    ggtitle(titulo) +
    geom_text(label = nombres.a.mostrar, stat = "identity", size = 3, hjust=0, vjust=0)      
  
  bip
}

#######################################################################
# Calcula las distancias de cada dato al centroide de su cluster
# Las asignaciones de cada dato a su cluster se indican en asignaciones.clustering
# Cada centroide es una fila del data frame datos.centroides.normalizados

distancias_a_centroides = function (datos.normalizados, 
                                    asignaciones.clustering, 
                                    datos.centroides.normalizados){
  
  sqrt(rowSums(   (datos.normalizados 
                   - 
                   datos.centroides.normalizados[asignaciones.clustering,])^2  ))
}


#######################################################################
# Revierte la funci?n de normalizaci?n (z-score)

desnormaliza = function(datos, filas.normalizadas){
  medias        = colMeans(datos)
  desviaciones  = apply(datos, 2, sd , na.rm = TRUE)
  
  filas.desnormalizadas  = sweep(filas.normalizadas, 2, desviaciones, "*")
  filas.desnormalizadas  = sweep(filas.desnormalizadas, 2, medias, "+")
  
  filas.desnormalizadas 
}




top_clustering_outliers = function(datos.norm, 
                                   asignaciones.clustering, 
                                   datos.centroides.norm, 
                                   num.outliers){
  
  dist_centroides = distancias_a_centroides (datos.norm, 
                                             asignaciones.clustering, 
                                             datos.centroides.norm)
  
  claves = order(dist_centroides, decreasing=T)[1:num.outliers]
  
  list(distancias = dist_centroides[claves]  , claves = claves)
}
```

# Dataset y SelecciÃ³n de Variables

En este guion usaremos el conjunto de datos __mtcars__ disponible directamente en R. Contiene los datos de un serie de vehÃ­culos. Puede encontrar en Internet una descripciÃ³n completa de dicho dataset. Nosotros describimos aquÃ­ las columnas que serÃ¡n el objetivo de nuestro estudio.

- Variables relativas a las caracterÃ­sticas fÃ­sicas

  - disp (displacement) Nos indica la cilindrada en pulgadas cÃºbicas. En EspaÃ±a, lo normal es referirnos a la cilindrada en centÃ­metros cÃºbicos.

  - hp (horse power) Es la potencia del motor

  - drat (Rear axle ratio) Es la relaciÃ³n del eje trasero. Un valor bajo nos permite unos desarrollos mayores con bajo consumo: es lo habitual en turismos. Un valor alto hace que el coche consuma mÃ¡s, pero permite enviar mÃ¡s fuerza, como por ejemplo en un todo terreno.

  - wt (weight) Peso del vehÃ­culo

- Variables relativas al rendimiento

  - mpg (miles per gallon) Nos indica el consumo del coche. Cuanto mayor sea, mÃ¡s combustible consume.

  - qsec (1/4 mile time) Mide el tiempo necesario para recorrer un cuarto de milla. Es una medida inversa a la aceleraciÃ³n: cuanto mÃ¡s acelere un coche, menor serÃ¡ el valor de qsec .

Para trabajar con dicho conjunto, vamos a construir los siguientes objetos:

  - datos: frame de datos que contendrÃ¡ mtcars
  - datos.num: frame obtenido a partir de datos utilizando sÃ³lo las columnas de tipo numÃ©rico.
  - indice.columna: Ãndice de la columna de datos con la que se quiera trabajar.
  - columna: ContendrÃ¡ la columna de datos correspondiente a indice.columna.
  - nombre.columna: Nombre de la columna correspondiente a indice.columna.

Trabajaremos Ãºnicamente sobre las variables numÃ©ricas. Por lo tanto procedemos de la siguiente forma:

1. Cargamos el conjunto de datos. En nuestro caso usaremos el conjunto de datos mtcars
2. Seleccionamos sÃ³lo las variables numÃ©ricas. Para ello usamos la funciÃ³n is.numeric. Aplicada sobre una columna, nos dice si 3. todos sus valores son numÃ©ricos. Por ejemplo, para ver si la tercera columna es numÃ©rica, pondrÃ­amos is.numeric(datos[, 3])
4. Vemos los valores que toman dichas variables y eliminamos aquellas que sean ordinales o con pocos valores distintos
5. Eliminamos tambiÃ©n aquellos registros que tienen algÃºn valor nulo en alguna columna. En aquellos casos en los que tenga sentido hacerlo, se puede aplicar un procedimiento de imputaciÃ³n de valores en vez de eliminar dichos registros.

Cargamos el conjunto de datos
```{r}
datos = mtcars
head(datos)
```

ConstruÃ­mos un dataframe con las columnas numÃ©ricas
```{r}
columnas.num = sapply(c(1:ncol(datos)) , function(x) is.numeric(datos[, x]))
columnas.num

datos.num = datos[, columnas.num]
```

Vemos informaciÃ³n sobre cada variable
```{r}
head(datos)

# Medidas estadÃ­sticas
summary(datos)

# Ocurrencias
apply(mpg, 2, table)
```

Puede apreciar que las variables cyl, vs, am, gear, carb tienen muy pocos valores distintos por lo que las eliminamos del estudio.
```{r}
datos.num  = datos.num[,-c(2 , 8:11)]  
head(datos.num)
```

Finalmente, eliminamos todas aquellas filas que tengan algÃºn valor nulo:
```{r}
datos.num = na.omit(datos.num)
```

# DetecciÃ³n de outliers en una dimensiÃ³n
## Outliers IQR

Los mÃ©todos IQR teÃ³ricamente solo se deben aplicar a distribuciones normales, pero tambiÃ©n pueden funcionar si la forma de la distribuciÃ³n no es rara (multimodal, uniforme...).

Mostramos histograma de cada variable
```{r}
par(mfrow = c(2,3))
c(1:ncol(datos.num)) %>% sapply(function(x) hist(datos.num[,x], 
                                                 main="", 
                                                 xlab=names(datos.num)[x]))
```

Ninguna variable sigue una distribuciÃ³n __rara__ (quizÃ¡s la variable _disp_ que parece uniforme), asÃ­ que mantenemos todas las columnas.

A falta de mÃ¡s informaciÃ³n, seleccionamos cualquiera de ellas (posteriormente trabajaremos con todas) Por ejemplo, seleccionamos _mpg_ (ya que junto a qsec, drat y hp son las que mÃ¡s se asemejan a una Normal) Establecemos las siguientes variables para reutilizarlas a lo largo de este apartado.

```{r}
indice.columna = 1
columna        = datos.num[, indice.columna]
nombre.columna = names(datos.num) [indice.columna]
```

### ObtenciÃ³n de los outliers IQR

1. En primer lugar debe calcular las siguiente variables:

  - cuartil.primero: Es el primer cuartil
  - cuartil.tercero: Es el tercer cuartil
  - iqr : Distancia intercuartil IQR
  
  Para ello, usamos las siguientes funciones:

  - quantile(columna, x) para obtener los cuartiles: x=0.25 para el primer cuartil, 0.5 para la mediana y 0.75 para el tercero.
  - IQR para obtener la distancia intercuartil (o bien reste directamente el cuartil tercero y el primero)
  
```{r}
cuartil.primero <- quantile(columna, .25, names = F)
cuartil.tercero <- quantile(columna, .75, names = F)
iqr <- IQR(columna)
```

```{r}
cat("Q1: ")
cuartil.primero
cat("\nQ3: ")
cuartil.tercero
cat("\nIQR: ")
iqr
```
  
2. A continuaciÃ³n debe calcular los extremos que delimitan los outliers:

  - extremo.superior.outlier.IQR se calcula como el cuartil tercero mÃ¡s 1.5 veces la distancia intercuartil.
  - extremo.inferior.outlier.IQR se calcula como el cuartil primero menos 1.5 veces 1.5 la distancia intercuartil.
  - extremo.superior.outlier.IQR.extremo se calcula como el cuartil tercero mÃ¡s 3 veces la distancia intercuartil.
  - extremo.inferior.outlier.IQR.extremo se calcula como el cuartil primero menos 3 veces la distancia intercuartil.

```{r}
extremo.superior.outlier.IQR <- cuartil.tercero + 1.5 * iqr
extremo.inferior.outlier.IQR <- cuartil.primero - 1.5 * iqr
extremo.superior.outlier.IQR.extremo <- cuartil.tercero + 3 * iqr
extremo.inferior.outlier.IQR.extremo <- cuartil.primero - 3 * iqr
```

```{r}
extremo.superior.outlier.IQR
extremo.inferior.outlier.IQR
extremo.superior.outlier.IQR.extremo
extremo.inferior.outlier.IQR.extremo
```


3. Finalmente, construya sendos vectores de valores lÃ³gicos TRUE/FALSE que nos dicen si cada registro es o no un outlier con respecto a la columna fijada:

  - son.outliers.IQR
  - son.outliers.IQR.extremos

Para ello, basta comparar con el operador relacional > o el operador relacional < la columna con alguno de los valores extremos anteriores (El operador lÃ³gico que debe usar es |)

```{r}
son.outliers.IQR <- columna < extremo.inferior.outlier.IQR | columna > extremo.superior.outlier.IQR
son.outliers.IQR.extremos <- columna < extremo.inferior.outlier.IQR.extremo | columna > extremo.superior.outlier.IQR.extremo
```

```{r}
head(son.outliers.IQR)
head(son.outliers.IQR.extremos)
sum(son.outliers.IQR)
sum(son.outliers.IQR.extremos)
```

### Ãndices y valores de los outliers IQR
```{r}
claves.outliers.IQR <- which(son.outliers.IQR)
df.outliers.IQR <- datos.num[claves.outliers.IQR,]
nombres.outliers.IQR <- row.names(df.outliers.IQR) 
valores.outliers.IQR <- columna[claves.outliers.IQR]

claves.outliers.IQR.extremos <- which(son.outliers.IQR.extremos)
df.outliers.IQR.extremos <- datos.num[claves.outliers.IQR.extremos,]
nombres.outliers.IQR.extremos <- row.names(df.outliers.IQR.extremos) 
valores.outliers.IQR.extremos <- columna[claves.outliers.IQR.extremos]
```

```{r}
claves.outliers.IQR
df.outliers.IQR
nombres.outliers.IQR
valores.outliers.IQR
```


```{r}
claves.outliers.IQR.extremos
df.outliers.IQR.extremos
nombres.outliers.IQR.extremos
valores.outliers.IQR.extremos
```

### CÃ³mputo de los outliers IQR con funciones

ELIMINAR LO DE ARRIBA PARA LA MEMORIA (o incluÃ­rlo como subapartado)

```{r}
son.outliers.IQR     = son_outliers_IQR(datos.num, indice.columna)
head(son.outliers.IQR)

claves.outliers.IQR  = claves_outliers_IQR(datos.num, indice.columna)
claves.outliers.IQR

son.outliers.IQR.extremos    = son_outliers_IQR(datos.num, indice.columna, 3)
head(son.outliers.IQR.extremos)

claves.outliers.IQR.extremos = claves_outliers_IQR(datos.num, indice.columna, 3)
claves.outliers.IQR.extremos
```

### DesviaciÃ³n de los outliers con respecto a la media de la columna

Si partimos de una variable X cuya distribuciÃ³n no es normal, el mÃ©todo de z-score no obtiene una N(0,1), pero si la distribuciÃ³n de X no es demasiado rara, los datos que asÃ­ obtengamos nos darÃ¡n informaciÃ³n Ãºtil sobre si los registros son usuales o no. Para ilustrarlo, apliquemos el mÃ©todo z-score a la variable mpg. Para ello, usamos la funciÃ³n scale:

```{r}
datos.num.norm = scale(datos.num)
head(datos.num.norm)

columna.norm   = datos.num.norm[, indice.columna]
```
Para ver los valores normalizados de los outliers, construya la la variable valores.outliers.IQR.norm. Para ello, debe usar la variable columna.norm junto con son.outliers.IQR (o bien claves.outliers.IQR). Le debe salir lo siguiente:
```{r}
valores.outliers.IQR.norm <- columna.norm[claves.outliers.IQR]

valores.outliers.IQR.norm
```

Vamos a ver ahora el comportamiento de los outliers en la columna seleccionada con respecto al resto de columnas. Para ello, basta con seleccionar los datos correspondientes del conjunto de datos normalizado. En nuestro caso, sÃ³lo tenemos un outlier IQR en la columna seleccionada. Nos debe salir lo siguiente:

```{r}
datos.num.norm.outliers.IQR <- datos.num.norm[claves.outliers.IQR,]

datos.num.norm.outliers.IQR
```
Podemos apreciar que el Toyota Corolla no tiene valores excesivamente grandes o pequeÃ±os en el resto de columnas (distintas de mpg)

### GrÃ¡fico

Mostramos en un grÃ¡fico los valores de los registros. Usaremos el color rojo para mostrar lo outliers. Para ello, llame a la siguiente funciÃ³n:
```{r}
plot_2_colores(datos.num.norm, claves.outliers.IQR)
```

```{r}
plot_2_colores(datos.num.norm, claves.outliers.IQR.extremos)
```

### Diagrama de cajas

Otro anÃ¡lisis exploratorio de los datos nos lo da los diagramas de cajas. Vamos a usar la funciÃ³n geom_boxplot definida en el paquete ggplot. En vez de usarla directamente, llamamos a la siguiente funciÃ³n (que llama internamente a geom_boxplot), disponible en el fichero

```{r}
diag_caja_outliers_IQR(datos.num.norm, 1)
```

Esta funciÃ³n se ha construido para mostrar un diagrama de cajas genÃ©rico. El diagrama tambiÃ©n muestra las etiquetas de los registros cuyos Ã­ndices se indican en el parÃ¡metro claves.a.mostrar. En nuestro caso, le pasamos como parÃ¡metro el vector que ya habÃ­a construido anteriormente con los Ã­ndices de los outliers IQR, es decir, el vector claves.outliers.IQR ( pero podrÃ­a pasarle cualquier otro vector de Ã­ndices). Nos debe salir lo siguiente:

```{r}
diag_caja(datos.num.norm, 1, claves.outliers.IQR)
```

Al igual que hicimos en el apartado anterior, vamos a analizar los valores que un outlier (con respecto a una columna seleccionada) toma en el resto de columnas. Para ello, vamos a mostrar de forma conjunta los diagramas de cajas de varias variables. Llamamos a la funciÃ³n diag_caja_juntos, disponible en el fichero OutliersFunciones_byCubero.R
```{r}
diag_caja_juntos(datos.num, "Outliers", claves.outliers.IQR)
```

Tal y como habÃ­amos analizado anteriormente, el Toyota Corolla (que es un outlier IQR con respecto a mpg) no tiene valores anormales en el resto de columnas (aunque tal vez con la excepciÃ³n de la variable disp).

## Test de hipÃ³tesis

En este apartado vamos a determinar con un test de hipÃ³tesis si el valor mÃ¡s alejado de la media puede considerarse como un outlier.
AsÃ­ pues, la hipÃ³tesis nula es la siguiente:


H0:El valor mÃ¡s alejado de la media no es un outlier

O siendo mÃ¡s correctos:

H0:El valor mÃ¡s alejado de la media proviene de la misma distribuciÃ³n que el resto de datos

El mÃ©todo IQR que hemos visto anteriormente es un mÃ©todo que suele aplicarse con la Ãºnica restricciÃ³n de que el histograma de la variable no sea demasiado raro. Sin embargo, un test de hipÃ³tesis es un mÃ©todo de decisiÃ³n cuya finalidad es rechazar una hipÃ³tesis con suficientes garantÃ­as, desde un punto de vista estadÃ­stico. Por tanto, debemos ser mÃ¡s cautelosos con las restricciones exigidas para aplicar el mÃ©todo. En nuestro caso, vamos a aplicar el test de Grubbs.

### ComprobaciÃ³n de la hipÃ³tesis de Normalidad

El test de Grubbs establece como hipÃ³tesis nula que el valor mÃ¡s alejado de la media (llamÃ©mosle O) no es un outlier. Por tanto, si el test rechaza, tendremos garantÃ­a estadÃ­stica de que es un outlier. Ahora bien:

El test asume que los datos deben seguir una distribuciÃ³n Normal. Esta hipÃ³tesis se refiere al conjunto de datos sin tener en cuenta O. Por tanto, si el test de Grubbs decide rechazar y se acepta que O es un outlier, el siguiente paso que debemos dar es comprobar que los datos que quedan siguen una distribuciÃ³n Normal. Esto lo haremos aplicando un test especÃ­fico de ajuste de distribuciones.

Si no se rechaza, sÃ³lo podremos decir que no hay evidencia de que O provenga de otra distribuciÃ³n distinta al resto de los datos.

En primer lugar pasamos a comprobar de una forma informal que los datos siguen una distribuciÃ³n Normal. Lo vamos a hacer visualmente analizando el histograma. Por simplicidad incluimos el posible outlier en el grÃ¡fico. Posteriormente, aplicaremos un test de hipÃ³tesis especÃ­fico de ajuste de distribuciones (sin tener en cuenta el outlier), tal y como hemos indicado anteriormente.

Para ver la curva Normal que mejor se ajusta al histograma, usamos la funciÃ³n denscompdel paquete fitdistrplus y observamos que, efectivamente, podemos suponer que la distribuciÃ³n subyacente es una Normal.

```{r}
ajusteNormal = fitdist(columna , "norm")
denscomp (ajusteNormal,  xlab = nombre.columna)
```

### Test de Grubs

Una vez que hemos visto que los datos siguen una distribuciÃ³n no demasiado alejada de la Normal, procedemos a aplicar el test de Grubbs.
```{r}
test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)
test.de.Grubbs$p.value
```

El p-value es > 0.05, por lo que el test no puede rechazar. AsÃ­ pues, aunque Toyota Corolla tiene un valor alto en mpg, no podemos deducir que realmente sea un outlier desde el punto de vista estadÃ­stico.

En el caso de que el estudio de los outliers lo hubiÃ©semos empezado directamente con el test de Grubbs, la anterior funciÃ³n sÃ³lo nos dice si el valor mÃ¡s alejado de la media puede considerarse un outlier. Â¿Pero a quÃ© valor corresponde? Para responder esta pregunta, usamos la funciÃ³n outlier del paquete outliers. Es importante enfatizar que la funciÃ³n outlier no realiza ningÃºn test. Simplemente nos da informaciÃ³n referente a las diferencias de cada valor con respecto a la media. 

Para conocer el valor que toma el registro que mÃ¡s se aleja de la media, basta pasar como parÃ¡metro la columna de datos a la funciÃ³n outlier:
```{r}
valor.posible.outlier = outlier(columna)
valor.posible.outlier
```

Efectivamente, el valor del Toyota Corolla en la columna mpg es 33.9. Para obtener el identificador de dicho registro, pasamos como parÃ¡metro adicional a la funciÃ³n outlier el valor logical = TRUE: Ã©sto nos devuelve un vector de bools en el que todos son FALSE excepto el valor que estÃ¡ mÃ¡s alejado de la media. Basta pues, usar which para ver su identificador:

```{r}
es.posible.outlier = outlier(columna, logical = TRUE)
clave.posible.outlier = which( es.posible.outlier == TRUE)
clave.posible.outlier
```

Lo que nos devuelve la clave 20 (la clave de Toyota Corolla).

### Test de Normalidad

Vamos a comprobar que los datos que quedan despuÃ©s de eliminar el outlier detectado por el test de Grubss siguen una distribuciÃ³n Normal. En el conjunto mtcars no hay ningÃºn registro de ninguna variable que el test de Grubbs etiquete como outlier. Por lo tanto, para ilustrar el proceso que vamos a seguir, vamos a usar un conjunto de datos sintÃ©tico. En el trabajo final que usted debe desarrollar (en su caso) use la misma variable que hubiese seleccionado al principio. HÃ¡galo aunque el test de Grubbs no haya detectado ningÃºn outlier, para asÃ­ comprobar si la variable se distribuye segÃºn una distribuciÃ³n Normal.

El test de hipÃ³tesis que se plantea es el siguiente:

```
H0:La distribuciÃ³n subyacente de la variable es una Normal
```

Observe que el tipo de hipÃ³tesis nula es diferente que el del test de outliers. En este caso, la hipÃ³tesis nula es una afirmaciÃ³n (en el caso del test de Grubbs era una negaciÃ³n). Por lo tanto, si se rechaza, podemos afirmar que los datos no vienen de una Normal. En el caso de que no se pueda rechazar, sÃ³lo podremos afirmar que los datos no contradicen la hipÃ³tesis nula y por tanto, podremos asumir (pero sin garantÃ­a estadÃ­stica) que se satisface el requisito de Normalidad de los datos.

Hay varios tests de hipÃ³tesis para comprobar el ajuste de una distribuciÃ³n (por orden de importancia):

- El test de Shapiro-Wilks (shapiro.test) es un test especÃ­fico para la distribuciÃ³n Normal. Es el preferible cuando hay pocos datos (menos de 50)

- El test de Anderson-Darling es un test para cualquier distribuciÃ³n. Requiere que se conozcan los parÃ¡metros de la distribuciÃ³n, aunque suele utilizarse con las estimaciones de Ã©stos. EstÃ¡ disponible a travÃ©s de gofstat$adtest del paquete fitdistrplus

- El test de Kolomogorov-Smirnov (shapiro.test) es otro test genÃ©rico aplicable a cualquier distribuciÃ³n (sÃ³lo compara la mayor diferencia observada entre los datos y la media). Requiere conocer los parÃ¡metros de la distribuciÃ³n. En el caso de la Normal, se usa la variante de Lilliefors (lillie.test del paquete nortest) que no requiere el conocimiento de Ã©stos.

Vamos a trabajar con los dos primeros tests. Construimos un dataset artificial

```{r}
datos.artificiales = c(45,56,54,34,32,45,67,45,67,65,140)
```

y lanzamos el mismo proceso anterior para detectar el posible outlier O:
```{r}
test.de.Grubbs = grubbs.test(datos.artificiales, two.sided = TRUE)
test.de.Grubbs$p.value

valor.posible.outlier = outlier(datos.artificiales)
valor.posible.outlier

es.posible.outlier = outlier(datos.artificiales, logical = TRUE)
es.posible.outlier

clave.posible.outlier = which(es.posible.outlier == TRUE)
clave.posible.outlier
```

Pasamos los tests de Normalidad al conjunto de datos eliminando previamente el outlier O:
```{r}
datos.artificiales.sin.outlier = datos.artificiales[-clave.posible.outlier]
datos.artificiales.sin.outlier

shapiro.test(datos.artificiales)

goodness_fit = gofstat(ajusteNormal)
goodness_fit$adtest
```

El test de Anderson-Darling no se ha podido aplicar porque hay pocos datos. El test de Shapiro no puede rechazar la hipÃ³tesis nula de Normalidad (p-value > 0.05) AsÃ­ pues, podemos asumir que los datos no contradicen que la distribuciÃ³n subyacente sea una Normal.

En resumen, podemos concluir que los valores presentes en datos.artificiales son compatibles con una distribuciÃ³n Normal y que el registro 11 con un valor de 140 es el que mÃ¡s se aleja de la media y puede considerarse un outlier con garantÃ­a estadÃ­stica segÃºn el test de Grubbs.

Construya una funciÃ³n con el nombre test_Grubbs que devuelva una lista con los cÃ³mputos anteriores. TambiÃ©n debe lanzar el test de Normalidad sobre la columna elegida (una vez eliminado el posible outlier). Concretamente, la funciÃ³n pedida debe tener la siguiente cabecera:

```{r}
#######################################################################
# Aplica el test de Grubbs sobre la columna ind.col de datos y devuelve una lista con:

# nombre.columna: Nombre de la columna datos[, ind.col]
# clave.mas.alejado.media: Clave del valor O que estÃ¡ mÃ¡s alejado de la media
# valor.mas.alejado.media: Valor de O en datos[, ind.col]
# nombre.mas.alejado.media: Nombre de O en datos
# es.outlier: TRUE/FALSE dependiendo del resultado del test de Grubbs sobre O
# p.value:  p-value calculado por el test de Grubbs
# es.distrib.norm: Resultado de aplicar el test de Normalidad 
#    de Shapiro-Wilks sobre datos[, ind.col]
#    El test de normalidad se aplica sin tener en cuenta el 
#    valor mÃ¡s alejado de la media (el posible outlier O)
#    TRUE si el test no ha podido rechazar
#       -> SÃ³lo podemos concluir que los datos no contradicen una Normal
#    FALSE si el test rechaza 
#       -> Los datos no siguen una Normal

# Requiere el paquete outliers

test_Grubbs = function(datos, ind.col, alpha = 0.05) {
  columna <- datos[,ind.col]
  res <- list()
  
  # Nombre columna
  res$nombre.columna <- colnames(datos)[ind.col]
    
  # BÃºsqueda del outlier
  es.posible.outlier <- outlier(columna, logical = TRUE)
  
  res$clave.mas.alejado.media <- which(es.posible.outlier == TRUE)
  res$valor.mas.alejado.media <- outlier(columna)
  res$nombre.mas.alejado.media <- rownames(datos)[res$clave.mas.alejado.media]
  
  # Test de Grubbs
  test.de.Grubbs = grubbs.test(columna, two.sided = TRUE)

  res$es.outlier <- ifelse(test.de.Grubbs$p.value <= alpha, TRUE, FALSE)
  res$p.value <- test.de.Grubbs$p.value

  # Test de normalidad
  test.Normalidad <- shapiro.test(columna[-res$clave.mas.alejado.media])
  res$es.distrib.norm <- ifelse(test.Normalidad$p.value > alpha, TRUE, FALSE)
  
  res
}
  
df.datos.artificiales = as.data.frame(datos.artificiales)

test.Grubbs.datos.artificiales = test_Grubbs(df.datos.artificiales, 1)

test.Grubbs.datos.artificiales
```

```{r}
test.Grubbs.datos.num = test_Grubbs(datos.num, indice.columna)

test.Grubbs.datos.num
```

## Trabajando con varias columnas

### Outliers IQR

Empezamos con los outliers IQR: vamos a calcular los outliers IQR con respecto a cada una de las columnas. El conjunto de ellos nos darÃ¡ aquellos registros que son outliers con respecto a alguna columna.
Para ello, llamamos a la siguiente funciÃ³n (disponible en OutliersFunciones_byCubero.R) y guardamos el resultado en la variable claves.outliers.IQR.en.alguna.columna (mire el cÃ³digo de la funciÃ³n para ver cÃ³mo utiliza sapply)

```{r}
claves.outliers.IQR.en.alguna.columna =
  claves_outliers_IQR_en_alguna_columna(datos.num, 1.5)

claves.outliers.IQR.en.alguna.columna
```

En este ejemplo, no hay registros duplicados pero podrÃ­a haberlos. En ese caso, construimos sendas variables claves.outliers.IQR.en.mas.de.una.columna con todos aquellos registros que aparecen mÃ¡s de una vez y modificamos la variable claves.outliers.IQR.en.alguna.columna para que no aparezcan registros repetidos:

```{r}
claves.outliers.IQR.en.mas.de.una.columna = 
  unique(
    claves.outliers.IQR.en.alguna.columna[
      duplicated(claves.outliers.IQR.en.alguna.columna)])
claves.outliers.IQR.en.alguna.columna = 
  unique (claves.outliers.IQR.en.alguna.columna)


claves.outliers.IQR.en.mas.de.una.columna
claves.outliers.IQR.en.alguna.columna 
nombres_filas(datos.num, claves.outliers.IQR.en.mas.de.una.columna)
nombres_filas(datos.num, claves.outliers.IQR.en.alguna.columna)
```

Vamos a ver los valores normalizados de estos outliers. Nos debe salir lo siguiente:
```{r}
datos.num.norm[claves.outliers.IQR.en.alguna.columna,]
```

Vamos a ver esta misma informaciÃ³n de forma grÃ¡fica. Para ello, utilice el vector claves.outliers.IQR.en.alguna.columna para pasarlo como parÃ¡metro a la funciÃ³n diag_caja_juntos. De esta forma, obtendremos los diagramas de cajas de todas las variables y se mostrarÃ¡n los valores que toman los outliers con respecto a alguna columna. HÃ¡galo con los outliers normales y con los extremos. En nuestro ejemplo, como no hay outliers extremos, mostraremos los resultados con los outliers normales. Debe salir lo siguiente:
```{r}
diag_caja_juntos(datos.num.norm, "Outliers en alguna columna", claves.outliers.IQR.en.alguna.columna)
```

Vemos, por ejemplo, que el Toyota Corolla se dispara (por arriba) en mpg pero no tanto en el resto de columnas. Parece por tanto un coche bastante equilibrado que consume muy poco.

Por otra parte, el Maserati Bora se dispara en hp (por arriba) y algo menos en qsec (por abajo): es un coche muy potente lo que le permite obtener una aceleraciÃ³n muy alta. AdemÃ¡s, tiene un consumo (mpg) bastante moderado para ser un coche de esas caracterÃ­sticas.

Es llamativo el caso del Merc 230 que tenga una aceleraciÃ³n tan baja, la menor de todos los coches. HabrÃ­a que determinar si se trata de un error en la toma de datos o simplemente los ingenieros diseÃ±aron el vehÃ­culo con esas caracterÃ­sticas.

TambiÃ©n es llamativo el bloque de coches Lincoln Continental, Chrysler Imperial, Cadillac Fletwood. Son coches muy pesados, con mucha cilindrada y que consumen mucho. Los tÃ­picos coches americanos.

### Test de HipÃ³tesis

Vamos a ejecutar el test de Grubbs sobre las columnas de datos.num. En primer lugar, analizamos los histogramas de las variables para ver aquellas que se ajustan a una distribuciÃ³n Normal. Podemos usar los grÃ¡ficos que generamos en el apartado Datasets y SelecciÃ³n de Variables o bien generarlos con las funciones fitdist y denscomp tal y como hicimos en el apartado ComprobaciÃ³n de la HipÃ³tesis de Normalidad (tendrÃ¡ que recorrer todas las columnas con sapply). Si lo hace de esta segunda forma, le debe salir lo siguiente:
```{r}
par(mfrow = c(2,3))
datos.num %>% apply(2, function(columna) {
  ajusteNormal = fitdist(columna , "norm")
  denscomp (ajusteNormal,  xlab = nombre.columna)
})
```

Tal y como vimos en el apartado Datasets y SelecciÃ³n de Variables, la variable disp es la que mÃ¡s se aleja de una Normal. En cualquier caso, la mantenemos por ahora. Pasamos el test de Grubbs a todas las columnas. Para ello, utilice sapply (tambiÃ©n podrÃ­a haber usado apply, pero los resultados no se muestran de una forma tan compacta). Debe obtener lo siguiente:
```{r}
sapply(1:ncol(datos.num), test_Grubbs, datos=datos.num)
```

En primer lugar, analizamos el test de Normalidad de Shapiro-Wilks. Recordemos que la funciÃ³n test_Grubbs la habÃ­amos construido de forma que aplicaba el test despuÃ©s de haber eliminado el posible outlier de la columna correspondiente. El test rechaza en las variables disp (como ya habÃ­amos supuesto) y drat. AsÃ­ pues, podemos afirmar que dichas variables no siguen una distribuciÃ³n Normal. En cuanto al resto de variables, el test no puede rechazar por lo que concluimos que dichas variables puede considerarse que siguen una Normal. Recuerde que no tenemos ninguna garantÃ­a estadÃ­stica ya que el test no ha rechazado y realmente lo Ãºnico que podemos afirmar es que los datos no contradicen la hipÃ³tesis de Normalidad.

Por otra parte, vemos que ninguno de los outliers IQR pueden considerarse realmente outliers con garantÃ­a estadÃ­stica. Los candidatos que han estado mÃ¡s cerca de considerarse outliers segÃºn el test de Grubbs son el Maserati Bora(columna hp, p-value 0.111) y Merc 230 (columna qsec, p-value = 0.08)

# Outliers Multivariantes

## MÃ©todos estadÃ­sticos basados en la distancia de Mahalanobis

Para encontrar outliers multivariantes con tÃ©cnicas estadÃ­sticas, vamos a aplicar las que se basan en la distancia de Mahalanobis. Es importante destacar que la finalidad de estas tÃ©cnicas es ofrecer una garantÃ­a estadÃ­stica de que si un valor se etiqueta como outlier, realmente lo es. Por lo tanto, la hipÃ³tesis nula establece que no lo es, de forma que si se rechaza, estaremos seguros de que sÃ­ es un outlier:

```
H0:El valor mÃ¡s alejado del centro de la distribuciÃ³n no es un outlier
```

### HipÃ³tesis de Normalidad
Los mÃ©todos basados en la distancia de Mahalanobis asumen que la distribuciÃ³n conjunta es una distribuciÃ³n Normal multivariante. Por lo tanto, la hipÃ³tesis nula es realmente la siguiente:

```
H0:El valor con mayor distancia de Mahalanobis al centro de la distribuciÃ³n vienede la misma distribuciÃ³n Normal multivariante que el resto de datos
```

Una condiciÃ³n necesaria para que un conjunto de variables siga una distribuciÃ³n Normal multivariante es que cada una de ellas siga una distribuciÃ³n normal 1-variante. Por lo tanto, lo primero que vamos a hacer es trabajar Ãºnicamente con aquellas variables que siguen una Normal. Para ello, usamos la funciÃ³n test_Grubbs que ya habÃ­amos construido previamente. Recuerde que esta funciÃ³n devuelve una lista que incluye la propiedad es.distrib.norm que es un bool que nos dice si la variable en cuestiÃ³n puede considerarse que sigue una distribuciÃ³n Normal. UtilÃ­cela con sapply para obtener un vector de bools son.col.normales. Utilice dicho vector para construir el dataset datos.num.distrib.norm que contendrÃ¡ aquellas variables Normales del conjunto de datos datos.num. En nuestro ejemplo, recuerde que disp y drat (Ã­ndices de variables 2 y 4) no eran variables Normales. Debe salir lo siguiente:
```{r}
test <- sapply(1:ncol(datos.num), test_Grubbs, datos=datos.num)
son.col.normales <- apply(test, 2, function(x) {
  x$es.distrib.norm
})
datos.num.distrib.norm = datos.num[,son.col.normales]

son.col.normales
head(datos.num.distrib.norm)
```

Ahora bien, el que las variables sigan una distribuciÃ³n Normal 1-variante no garantiza que el conjunto de ellas siga una distribuciÃ³n Normal multivariante. Es una condiciÃ³n necesaria pero no suficiente. Por lo tanto, tenemos que lanzar un test de Normalidad multivariante. Para ello, lanzamos la funciÃ³n mvn de la librerÃ­a MVN. Lo hacemos sobre el conjunto de datos datos.num.distrib.norm:
```{r}
test.MVN = mvn(datos.num.distrib.norm, mvnTest = "energy")
test.MVN$multivariateNormality["MVN"]
test.MVN$multivariateNormality["p value"]
```


El test nos dice que la distribuciÃ³n conjunta de las variables mpg, hp, wt, qsec no es una Normal multivariante. Por lo tanto, no deberÃ­amos aplicar el mÃ©todo basado en la distancia de Mahalanobis. De todas formas, vamos a lanzarlo para ver si detectamos algÃºn valor que, aunque no pueda considerarse un outlier con garantÃ­a estadÃ­stica, al menos proporcione alguna informaciÃ³n interesante.

### Tests de hipÃ³tesis para detectar outliers
Vamos a usar la funciÃ³n cerioli2010.fsrmcd.test del paquete CerioliOutlierDetection (el paquete ofrece otra funciÃ³n cerioli2010.irmcd.test que, por simplicidad, no la veremos) La funciÃ³n cerioli2010.fsrmcd.test obtiene los outliers calculando las distancias de Mahalanobis usando una estimaciÃ³n de la matriz de covarianzas, segÃºn el mÃ©todo robusto MCD -minimum covariance determinant (la distribuciÃ³n del estadÃ­stico es la obtenida en Hardin-Rocke o Green and Martin) A tÃ­tulo informativo, estos mÃ©todos robustos no incluyen los valores alejados del centro de la distribuciÃ³n en la estimaciÃ³n de la matriz de covarianzas. Para verlo visualmente, lancemos la funciÃ³n corr.plot (del paquete mvoutlier) sobre las dos primeras variables (es sÃ³lo un ejemplo):

```{r}
corr.plot(datos.num[,1], datos.num[,2])
```

Observe cÃ³mo cambia la forma de las elipses determinadas por la distancia de Mahalanobis. En rojo se muestran los puntos de la derecha que estÃ¡n mÃ¡s alejados del centro y que, por tanto, no se han usado en la estimaciÃ³n de la matriz de covarianzas.

Tenemos dos formas de llamar a la funciÃ³n cerioli2010.fsrmcd.test dependiendo del tipo de test que queramos realizar:

1. Si queremos lanzar el test siguiente:

```
H0:El valor con mayor distancia de Mahalanobis viene de la misma distribuciÃ³n Normal multivariante que el resto de datos
```

  llamaremos a la funciÃ³n con un valor de significaciÃ³n de 0.05 (parÃ¡metro signif.alpha). Ãste serÃ­a el equivalente al test  de Grubbs en el que sÃ³lo se establece como posible outlier el valor mÃ¡s alejado del centro de la distribuciÃ³n. Lo llamaremos test individual

2. Si queremos lanzar el conjunto de tests siguientes:

```
âi=1â¯n,  H0i:El i-Ã©simo valor viene de la misma distribuciÃ³nNormal multivariante que el resto de datos
```

  llamaremos a la funciÃ³n con un valor de significaciÃ³n penalizado, por ejemplo usando la correcciÃ³n de Sidak: 1â(1âÎ±)1/n. Ãsta serÃ­a la forma de comprobar si cada uno de los valores es un outlier o no. Al penalizar el error de significaciÃ³n, controlamos el error FWER (consulte las transparencias) pero el test serÃ¡ muy conservador. Lo llamaremos test de intersecciÃ³n

La funciÃ³n cerioli2010.fsrmcd.test devuelve una lista y podremos acceder a las siguientes propiedades:

- outliers: Es un vector de bools en la que indica si el dato i-Ã©simo es un outlier. En el caso de que hayamos aplicado el test individual , sÃ³lo tenemos garantÃ­a estadÃ­stica de que es un outlier el valor con mayor distancia de Mahalanobis.

- mahdist.rw: Es un vector con las distancias de Mahalanobis de cada valor. Realmente, son las distancias de Mahalanobis modificadas por los autores del paquete. Si necesita conocer las distancias de Mahalanobis no modificadas, debe acceder a la propiedad mahdist.

Aplique el test individual con un valor de significaciÃ³n de 0.05 y el test de intersecciÃ³n con un valor de 1â(1â0.05)1/n (n es el nÃºmero de registros del conjunto de datos). Obtenga las claves de los outliers encontrados por ambos mÃ©todos. Para ello tendrÃ¡ que acceder al vector outliers devuelto por la funciÃ³n cerioli2010.fsrmcd.test. Obtenga tambiÃ©n los nombres de las filas correspondientes usando la funciÃ³n nombres_filas disponible en OutliersFunciones_byCubero.R. Como este tipo de mÃ©todos robustos usan un mÃ©todo aleatorio para iniciar el proceso de elecciÃ³n de los datos que participarÃ¡n en el cÃ³mputo final, es necesario que establezcamos el valor de semilla para que los resultados que veamos en esta ejecuciÃ³n sean siempre los mismos. AsÃ­ pues pondremos, por ejemplo, set.seed(2). Debe salir lo siguiente:

EL ALPHA DEL DE INTERSECCIÃN ES MUY PEQUEÃO. SI QUISIÃRAMOS BUSCAR K OUTLIERS NO SERÃA MEJOR PONER 0.05/K ??

```{r}
set.seed(2)

cerioli.individual <- cerioli2010.fsrmcd.test(datos.num, signif.alpha = 0.05)
claves.test.individual <- which(cerioli.individual$outliers)
nombres.test.individual <- nombres_filas(datos.num, claves.test.individual)

n <- nrow(datos.num)
alpha <- 0.05
cerioli.interseccion <- cerioli2010.fsrmcd.test(datos.num, signif.alpha = 1 - (1 - alpha)^(1/n))
claves.test.interseccion <- which(cerioli.interseccion$outliers)
nombres.test.interseccion <- nombres_filas(datos.num, claves.test.individual)

claves.test.individual
## [1]  9 17 29 31
nombres.test.individual
## [1] "Merc 230"          "Chrysler Imperial" "Ford Pantera L"    "Maserati Bora"
claves.test.interseccion
## integer(0)
nombres.test.interseccion
```

Observe que el test de intersecciÃ³n no devuelve ningÃºn outlier, mientras que el test individual devuelve 4 outliers. Ya hemos explicado que sÃ³lo tenemos garantÃ­a estadÃ­stica de que sea un outlier el que tiene mayor valor de distancia de Mahalanobis. Para ver cuÃ¡l es ese valor, basta ordenar decrecientemente el vector mahdist.rw (use la funciÃ³n order con el parÃ¡metro decreasing = TRUE ) y seleccionar el primero. Muestre tambiÃ©n un grÃ¡fico de todas las distancias de Mahlanobis obtenidas para que aprecie cuÃ¡l es el mayor valor. Le debe salir lo siguiente:
```{r}
cerioli.individual$mahdist.rw %>% sort() %>% plot()
```


```{r}
clave.mayor.dist.Mah <- order(cerioli.individual$mahdist.rw , decreasing = TRUE)[1]
nombre.mayor.dist.Mah <- nombres_filas(datos.num, clave.mayor.dist.Mah)

cerioli.individual$mahdist
clave.mayor.dist.Mah
nombre.mayor.dist.Mah
```

Por lo tanto, podemos concluir que el test individual rechazarÃ­a la hipÃ³tesis de que el registro con clave 31 (Maserati Bora) no es un outlier. AsÃ­ pues, lo aceptamos como outlier. Algunas consideraciones:

1. Recuerde que no hemos podido determinar que la distribuciÃ³n subyacente fuese una Normal, por lo que no tenemos garantÃ­a estadÃ­stica de que, efectivamente, dicho registro sea un outlier (de que provenga de una distribuciÃ³n distinta del resto de los datos).

2. Bajo la premisa de lo dicho anteriormente, el test individual ha etiquetado al Maserati Bora como un outlier multivariante. Recuerde que dicho registro no fue etiquetado como outlier 1-variante en niguna variable por el test de Grubbs ya que tenÃ­a un valor muy alto en dos variables (hp y qsec), aunque no lo suficiente para que fuese un outlier. Sin embargo, al tener el mismo coche dos variables con valores muy altos, el test multivariante sÃ­ lo puede considerar como un outlier, ya que se suman las contribuciones de ambas variables.

## VisualizaciÃ³n de datos con un Biplot
El BiPlot es una herramienta grÃ¡fica que nos permite tener una idea aproximada de los valores de los registros con respecto a todas las variables, asÃ­ como las correlaciones entre dichas variables.

El Biplot muestra los registros (las filas del dataset) como puntos en un plano 2D (tambiÃ©n podrÃ­a usarse un grÃ¡fico tridimensional) En el mismo grÃ¡fico se representan las variables como flechas, indicando la direcciÃ³n de crecimiento en dicha variable de los datos. Al pasar de n dimensiones a sÃ³lo 2, es obvio que se pierde informaciÃ³n por lo que siempre debemos tener en cuenta que es una representaciÃ³n aproximada. La aproximaciÃ³n serÃ¡ mejor cuanto mayor sea la suma de los porcentajes explicados por cada eje del plano (componente principal).

Llamamos a la funciÃ³n biplot_2_colores disponible en OutliersFunciones_byCubero.R pasÃ¡ndole como primer parÃ¡metro el conjunto de datos y como segundo las claves de aquellos registros cuyos nombres queremos mostrar en el grÃ¡fico. En nuestro caso, le pasamos las claves de los outliers IQR.
```{r}
biplot.outliers.IQR = biplot_2_colores(datos.num, 
                                       claves.outliers.IQR.en.alguna.columna, 
                                       titulo.grupo.a.mostrar = "Outliers IQR",
                                       titulo ="Biplot Outliers IQR")
biplot.outliers.IQR
```

La suma de los porcentajes explicados es muy alta (19.1 + 69.8 = 88.9), por lo que la representaciÃ³n obtenida es una buena aproximaciÃ³n. Puede apreciarse en el grÃ¡fico que, efectivamente, Toyota Corolla se sitÃºa en la zona mÃ¡s alta de la variable mpg, al igual que Maserati Bora lo hace en la parte de valores muy altos de hp y en la de valores muy bajos de qsec (recuerde que las flechas indican la direcciÃ³n de crecimiento) En la secciÃ³n siguiente usaremos el biplot para mostrar los resultados de otros mÃ©todos de detecciÃ³n de outliers.

## MÃ©todos basados en distancias: LOF

Los mÃ©todos estadÃ­sticos tienen como finalidad proporcionar garantÃ­a estadÃ­stica de que los valores etiquetados como outliers efectivamente lo son. Para ello, presuponen que los datos siguen una distribuciÃ³n estadÃ­stica concreta. Sin embargo, en las situaciones en las que este requisito no se cumple, no podemos aplicar dichos mÃ©todos. En estos casos, vamos a aplicar otros mÃ©todos que no ofrecen garantÃ­a estadÃ­stica, pero son capaces de determinar cÃ³mo de alejado estÃ¡ cada punto al resto de los datos. Para ello, se usa una medida de distancia que, mientras no digamos lo contrario, serÃ¡ la distancia euclÃ­dea. Para que unas variables no dominen sobre otras tendremos que, obligatoriamente, normalizar los datos. Nosotros usaremos la normalizaciÃ³n por z-score. De los mÃ©todos basados en distancia, aplicaremos uno de los mÃ¡s conocidos: LOF

En primer lugar, debemos determinar el nÃºmero de vecinos mÃ¡s cercanos que usaremos en el cÃ³mputo del mÃ©todo LOF (consulte las transparencias de clase) A falta de mÃ¡s informaciÃ³n, elegimos arbitrariamente el valor de 5. Llamamos a la funciÃ³n lofactor de la librerÃ­a DMwR pasÃ¡ndole como parÃ¡metro el conjunto de datos numÃ©ricos, una vez normalizados (recuerde que era datos.num.norm):
```{r}
num.vecinos.lof = 5
lof.scores = lofactor(datos.num.norm, k = num.vecinos.lof)
claves.lof.ordenados <- order(lof.scores, decreasing = T)
```

La funciÃ³n lofactor asigna un score a cada dato, indicando hasta quÃ© punto es un outlier. Ordenamos dicho vector de forma decreciente y mostramos en un grÃ¡fico los scores correspondientes. Nos debe salir lo siguiente:
```{r}
plot(sort(lof.scores, decreasing = T))
```

Podemos apreciar que hay un grupo de tres valores con scores mÃ¡s altos que el resto de datos. Vamos a analizar dichos valores. Establecemos la variable num.outliers en 3 y obtenemos sus claves junto con sus nombres (use la funciÃ³n nombres_filas). Nos debe salir lo siguiente:
```{r}
num.outliers <- 3

claves.outliers.lof <- order(lof.scores, decreasing = T) %>% head(num.outliers)
nombres.outliers.lof <- nombres_filas(datos.num.norm, claves.outliers.lof)

claves.outliers.lof
nombres.outliers.lof
```

Mostramos tambiÃ©n los valores normalizados de dichos registros:
```{r}
datos.num.norm[claves.outliers.lof, ]
```

Viendo estos datos, es posible que el Lincoln Continental y el Cadillac Fletwood hayan obtenido un score alto debido simplemente a que tenÃ­an valores extremos en una Ãºnica variable (y por tanto eran puntos alejados del resto) Posteriormente analizaremos con mÃ¡s detalle esta cuestiÃ³n. Por ahora, vamos a analizar el registro que tiene el mayor score en LOF. Corresponde al Valiant. Podemos apreciar que no tiene un valor extremo en ninguna variable por separado (el mÃ¡s extremo es -1.564608 en drat)

Vamos a empezar viendo las posibles interacciones de dos variables. Recuerde que el mÃ©todo LOF tiene en cuenta todas las variables a la hora de calcular el valor del score, por lo que no podemos extraer conclusiones definitivas analizando Ãºnicamente las interacciones de dos variables. En cualquier caso, obtendremos una idea aproximada de la situaciÃ³n del outlier.

Para ello, mostramos los diagramas de dispersiÃ³n corespondientes a los cruces 2 a 2 de las variables. Para ello, ejecutamos el siguiente cÃ³digo, que muestra en rojo el registro correspondiente al Valiant (en general, el que ha obtenido un mayor score):
```{r}
clave.max.outlier.lof = claves.outliers.lof[1]

colores = rep("black", times = nrow(datos.num.norm))
colores[clave.max.outlier.lof] = "red"
pairs(datos.num.norm, pch = 19,  cex = 0.5, col = colores, lower.panel = NULL)
```

Podemos apreciar que, por ejemplo, hay una correlaciÃ³n inversa entre mpg y disp. En este sentido, el valor de Valiant no contradice esta correlaciÃ³n ya que se sitÃºa en mitad de la nube de puntos. Por lo tanto, si sÃ³lo tuviÃ©semos en cuenta estas dos variables, un mÃ©todo estadÃ­stico no lo hubiera marcado como outlier. Sin embargo, no hay apenas vehÃ­culos en ese rango de valores: estÃ¡ Ã©l y otro mÃ¡s. AsÃ­ pues, Valiant estÃ¡ aislado en una zona de puntos, cercano a otras dos zonas de puntos de alta densidad, por lo que es posible que este hecho haya influido en el score asignado por el mÃ©todo LOF.

Si nos fijamos en la combinaciÃ³n drat y qsec, vemos que no hay ninguna correlaciÃ³n entre ambas variables, pero el registro Valiant se sitÃºa de nuevo en una zona aislada cerca de una nube de puntos de alta densidad a su izquierda, por lo que tambiÃ©n es posible que Ã©sto haya influido en el mÃ©todo LOF.

Una vez que tenemos una idea aproximada, vamos a ver de un forma grÃ¡fica la interacciÃ³n de todas las variables (no sÃ³lo 2 a 2) Para ello, usamos un biplot. A diferencia de los diagramas de dispersiÃ³n, el biplot muestra el comportamiento de los datos con respecto a todas las variables. Sin embargo, la informaciÃ³n obtenida no es exacta y es proporcional al porcentaje de variaciÃ³n explicado por las componentes principales. En nuestro ejemplo, la suma de la variabilidad explicada por las dos componentes principales es muy alta (casi un 90%) y por tanto la aproximaciÃ³n es muy buena.

Para mostrar el biplot ejecutamos el siguiente cÃ³digo:
```{r}
biplot.max.outlier.lof = biplot_2_colores(datos.num.norm, clave.max.outlier.lof, titulo = "Mayor outlier LOF")
biplot.max.outlier.lof
```

Valiant estÃ¡ en una zona con poca densidad, pero bastante cerca a otra zona de alta densidad (la de los vehÃ­culos que estÃ¡n en la parte central-izquierda del grÃ¡fico) Este grupo de vehÃ­culos corresponderÃ­a a los que tienen un valor algo superior al normal en las variables hp, disp, wt bastante por debajo de lo normal en drat y algo por debajo de lo normal en mpg. Son vehÃ­culos con bastante potencia, cilindrada y peso, pero parece que al tener una relaciÃ³n con el eje trasero baja, les permite tener una aceleraciÃ³n media y un consumo algo menor de la media. Sin embargo el Valiant, es un vehÃ­culo con valores no demasiado distintos en disp, wt, drat, mpg, algo menor en hp pero un valor de qsec mucho mayor. AsÃ­ pues, es un coche de consumo normal pero con una aceleraciÃ³n muy baja en relaciÃ³n a otros vehÃ­culos con valores similares de drat, disp, wt pero con algo mÃ¡s de potencia (hp). AsÃ­ pues, a primera vista, el Valiant no es un vehÃ­culo cuya compra estarÃ­a recomendada, aunque obviamente habrÃ­a que tener en cuenta otros factores no incluidos en el conjunto de datos como por ejemplo el diseÃ±o, precio, fiabilidad, etc.

En cualquier caso, no olvidemos que el mÃ©todo LOF depende del nÃºmero de vecinos elegido. De hecho, en este ejemplo, si hubiÃ©semos cambiado la variable num.vecinos.lof habrÃ­amos obtenido otro resultado.

## MÃ©todos basados en Clustering

En este apartado vamos a ver otro mÃ©todo basado en distancias. Detectaremos outliers segÃºn la distancia de cada dato al centroide de su cluster. El centroide podrÃ¡ ser cualquiera (podrÃ¡ provenir de un k-means o ser un medoide, por ejemplo). Recordemos que al ser un mÃ©todo basado en distancias, debemos trabajar con los datos normalizados. Empezamos con k-means

### Clustering usando k-means

A falta de mÃ¡s informaciÃ³n, fijamos el nÃºmero de outliers en 5 y el de clusters en 3. AdemÃ¡s, como los resultados del mÃ©todo de clustering k-means dependen de la elecciÃ³n inicial de los centroides, fijamos un valor de la semilla con la funciÃ³n set.seed para que, de esta forma, no varÃ­en los resultados de una ejecuciÃ³n a otra.
```{r}
num.outliers = 5
num.clusters = 3
set.seed(2)
```

Construimos el modelo kmeans (modelo.kmeans) llamando a la funciÃ³n kmeans. Nos devuelve una lista con las siguientes propiedades:

- cluster: contiene los Ã­ndices de asignaciÃ³n de cada dato al cluster correspondiente. El resultado lo guardamos en la variable asignac.clust.

  Por ejemplo, si el dato con Ã­ndice 69 estÃ¡ asignado al tercer cluster, en el vector asignac.clust habrÃ¡ un 3 en la componente nÃºmero 69.

- centers: contiene los datos de los centroides. Los datos estÃ¡n normalizados por lo que los centroides tambiÃ©n lo estÃ¡n. El resultado lo guardamos en la variable centroides.normalizados

Nos debe salir lo siguiente:
```{r}
modelo.kmeans <- kmeans(datos.num.norm, num.clusters)
asignaciones.clustering.kmeans <- modelo.kmeans$cluster
centroides.normalizados <- modelo.kmeans$centers

head(asignaciones.clustering.kmeans)
centroides.normalizados
```

En el caso de que necesite conocer los valores sin normalizar que le corresponderÃ­an a los datos de los centroides, puede usar la funciÃ³n desnormaliza disponible en OutliersFunciones_byCubero.R:
```{r}
centroides.desnormalizados = desnormaliza(datos.num, centroides.normalizados)
centroides.desnormalizados
```

Ya podemos calcular los outliers como aquellos datos que mÃ¡s se alejan del centroide del cluster al que ha sido asignado. Esto lo vamos a hacer construyendo una funciÃ³n top_clustering_outliers que debe tener la siguiente cabecera:
```{r}
#######################################################################
# Calcula las distancias de los datos a los centroides
# y se queda con los primeros (tantos como indica num.outliers)
# Devuelve una lista con las claves de dichos registros y las
# correspondientes distancias a sus centroides

 
top_clustering_outliers = function(datos.normalizados, 
                                   asignaciones.clustering, 
                                   datos.centroides.normalizados, 
                                   num.outliers){
  distancias <- distancias_a_centroides(datos.normalizados, 
                                        asignaciones.clustering,
                                        datos.centroides.normalizados)
  claves <- distancias %>% order(decreasing = T) %>% head(num.outliers)
  
  res <- list()
  res$distancias <- distancias %>% sort(decreasing = T) %>% head(num.outliers)
  res$claves <- claves
  res
}
```

Implemente esta funciÃ³n de la siguiente forma: si un dato di ha sido asignado a un cluster Ck, calculamos la distancia euclÃ­dea disti de di al centroide de Ck. Los outliers serÃ¡n los datos con mayores valores de disti. Para ello, debe calcular el vector de todas las distancias distancias, ordenarlo y quedarse con los primeros (tantos como indique la variable num.outliers) De esa forma obtendrÃ¡ el vector claves. La funciÃ³n top_clustering_outliers devolverÃ¡ una lista con ambos valores, a saber, distancias y claves. Para calcular las distancias use la siguiente funciÃ³n disponible en OutliersFunciones_byCubero.R.

Le debe salir lo siguiente:
```{r}
top.outliers.kmeans = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.kmeans, 
                                              centroides.normalizados, 
                                              num.outliers)
claves.outliers.kmeans = top.outliers.kmeans$claves 
nombres.outliers.kmeans = nombres_filas(datos.num, claves.outliers.kmeans)
distancias.outliers.centroides = top.outliers.kmeans$distancias

claves.outliers.kmeans
nombres.outliers.kmeans
distancias.outliers.centroides
```

Vamos a mostrar un biplot con la informaciÃ³n de los outliers y de los clusters. Para ello, usamos la funciÃ³n biplot_outliers_clustering disponible en OutliersFunciones_byCubero.R:

Esta funciÃ³n llama a otra funciÃ³n mÃ¡s genÃ©rica biplot_colores_formas que muestra un biplot de un conjunto de datos diferenciados por color y por forma. En nuestro caso, los colores corresponden a las asignaciones de los clusters y las formas a si es o no un outlier. Esta segunda funciÃ³n no la necesita para estas prÃ¡cticas pero le puede ser Ãºtil en otras aplicaciones.
```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers k-means",
                           asignaciones.clustering = asignaciones.clustering.kmeans,
                           claves.outliers = claves.outliers.kmeans)
```

Podemos apreciar que cuatro de los cinco outliers detectados por este mÃ©todo estÃ¡n en el exterior de la nube de puntos, por lo que es muy posible que se hayan etiquetado como outliers porque tienen un valor muy alto en una o varias variables. Estos coches son Ford Pantera L, Honda Civic, Cadillac Fletwood y Merc 230. Los registros Merc 230 y Cadillac Fletwood ya los conocÃ­amos porque eran outliers con respecto a 1 columna. Ahora han aparecido dos nuevos: Ford Pantera L y Honda Civic. Veamos el diagrama de cajas conjunto para hacernos una idea de los valores que toman:
```{r}
diag_caja_juntos(datos.num, "Outliers k-means", claves.outliers.kmeans)
```

Parece ser que, efectivamente, Ford Pantera L y Honda Civic han sido etiquetados como outliers porque tienen valores algo extremos (sin llegar a ser muy extremos) en varias variables, por lo que la suma de los efectos de dichas variables han podido determinar que tengan scores altos. Por ejemplo, Honda Civic tiene valores algo extremos en todas las variables salvo en qsec.

El Ãºnico registro que no estÃ¡ en el exterior del biplot y ha sido etiquetado como outlier es Ferrari Dino. El diagrama de cajas tambiÃ©n muestra que, efectivamente, no tiene un valor extremo en una o varias variables. Posteriormente analizaremos con mÃ¡s detalle este caso.

### Clustering usando medoides

En este apartado vamos a usar el mÃ©todo de clustering PAM (Partition around medoids) Previamente tenemos que calcular la matriz de distancias de todos con todos usando la funciÃ³n dist. A continuaciÃ³n, usamos la funciÃ³n pam del paquete cluster, pasÃ¡ndole como parÃ¡metros la matriz de distancias y k = nÃºmero de clusters. Guardamos el resultado en modelo.pam
```{r}
set.seed(2)
matriz.distancias = dist(datos.num.norm)
modelo.pam        = pam(matriz.distancias , k = num.clusters)
```

Para obtener las asignaciones de cada dato a su cluster accedemos a modelo.pam$clustering. Aunque no sea de interÃ©s para el cÃ¡lculo de los outliers, podemos ver la informaciÃ³n de los medoides. Para ver los nombres de los medoides accedemos a modelo.pam$medoids. Mostramos tambiÃ©n los valores que toman los medoides en todas las variables para hacernos una idea de cÃ³mo son los representantes de los clusters encontrados por pam (ahora no hace falta desnormalizar como hicimos en kmeans ya que los medoides son valores reales del conjunto de datos). TambiÃ©n mostramos los valores normalizados.
```{r}
asignaciones.clustering.pam = modelo.pam$clustering   
nombres.medoides = modelo.pam$medoids    
medoides = datos.num[nombres.medoides, ]
medoides.normalizados = datos.num.norm[nombres.medoides, ]

nombres.medoides
medoides
medoides.normalizados
```
Calculamos ahora los top outliers. Para ello llamamos a la funciÃ³n top_clustering_outliers. Mostramos tambiÃ©n el biplot correspondiente llamando a la funciÃ³n biplot_outliers_clustering. Nos debe salir lo siguiente:
```{r}
top.outliers.pam = top_clustering_outliers(datos.num.norm , 
                                              asignaciones.clustering.pam, 
                                              medoides.normalizados, 
                                              num.outliers)
claves.outliers.pam = top.outliers.pam$claves 
nombres.outliers.pam = nombres_filas(datos.num, claves.outliers.pam)

claves.outliers.pam
nombres.outliers.pam
```


```{r}
biplot_outliers_clustering(datos.num, 
                           titulo = "Outliers PAM",
                           asignaciones.clustering = asignaciones.clustering.pam,
                           claves.outliers = claves.outliers.pam)
```

Podemos apreciar que, en este ejemplo, al usar la particiÃ³n creada por pam, los outliers encontrados corresponden a registros que tienen un valor extremo en alguna de las variables.

### AnÃ¡lisis de los outliers multivariantes puros

Un outlier multivariante puede ser etiquetado como tal por varias razones:

- El outlier tiene un valor extremo en alguna variable. Es decir, en una Ãºnica variable, dicho registro presenta un valor extremo y por tanto esa variable contribuye de forma decisiva en el cÃ³mputo global. Puede ser el caso del Merc 230 que era un outlier en qsec.

  TambiÃ©n serÃ­a el caso del grupo de vehÃ­culos Lincoln Continental, Chrysler Imperial, Cadillac Fletwood que eran outliers no sÃ³lo en una variable sino en varias (coches muy pesados, con mucha cilindrada y que consumen mucho)

  Estos outliers se sitÃºan en la periferia del biplot.

- El outlier tiene valores relativamente extremos en mÃ¡s de una variable, aunque no llega a ser un outlier en ninguna de ellas. El efecto sumado de dichas variables hace que el registro sea etiquetado como outlier. Era el caso del Ford Pantera L y Honda Civic.

  Al igual que los outliers anteriores, Ã©stos tambiÃ©n suelen situarse en la periferia del biplot.

- El outlier no tiene valores extremos en ninguna variable pero, sin embargo, presenta una combinaciÃ³n inusual de valores de dos o mÃ¡s variables.

  Estos outliers suelen estar en la zona interior del biplot.

- El outlier tiene alguna otra caracterÃ­stica que depende del mÃ©todo de detecciÃ³n aplicado. Por ejemplo, los mÃ©todos basados en distancia detectan aquellos valores que estÃ¡n aislados del resto de valores. Concretamente, el mÃ©todo LOF tiene en cuenta la densidad relativa de los vecinos prÃ³ximos. Realmente, podrÃ­a considerarse que este tipo de outliers aislados son un tipo particular de los anteriores (registros con combinaciones inusuales de variables)

Informalmente, diremos que los tres Ãºltimos tipos de outliers son outliers multivariantes puros, es decir, aquellos que no son outliers con respecto a una Ãºnica variable. De hecho, ya hemos detectado algunos en los apartados anteriores. Por ejemplo, el mÃ©todo k-means habÃ­a identificado como outliers a Ford Pantera L y Honda Civic ya que presentaban valores algo extremos en varias variables. TambiÃ©n identificÃ³ a Ferrari Dino como outlier, aunque Ã©ste no parecÃ­a que tuviese valores extremos en varias variables. Este vehÃ­culo serÃ¡ analizado con mÃ¡s detalle posteriormente.

Lo que vamos a hacer ahora es automatizar el proceso. Para ello, vamos a identificar a los outliers en una Ãºnica variable usando el mÃ©todo IQR: recuerde que habÃ­amos obtenido los outliers en alguna columna (claves.outliers.IQR.en.alguna.columna) BastarÃ¡ por tanto ver los outliers que son multivariantes pero no son 1-variantes (con respecto a ninguna variable). Este proceso lo podemos aplicar sobre cualquiera de los mÃ©todos vistos anteriormente (estadÃ­sticos, basados en distancia o basados en clustering). Nosotros vamos a usar el mÃ©todo LOF, que es uno de los que mejores resultados dan en una gran variedad de situaciones.

AsÃ­ pues, se le pide que calcule los registros que estÃ¡n en claves.outliers.lof pero no estÃ¡n en claves.outliers.IQR.en.alguna.columna (deberÃ¡ aplicar la funciÃ³n setdiff). Debe salir lo siguiente:
```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.outliers.lof, claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.outliers.lof
claves.outliers.lof.no.IQR
nombres.outliers.lof.no.IQR
```

AsÃ­ pues, el Ãºnico outlier LOF multivariante puro es el correspondiente al Valiant. En la secciÃ³n LOF vimos que era un outlier porque, si bien no tenÃ­a valores extremos en cada variable, no habÃ­a otros registros (salvo uno) que tuviesen valores similares (era un registro aislado del resto) Analizando los valores del Valiant vimos que era un coche poco recomendable (en comparaciÃ³n a los otros coches) atendiendo a los parÃ¡metros de aceleraciÃ³n y consumo.

SerÃ­a interesante intentar extraer alguna otra informaciÃ³n adicional del conjunto de datos, aumentando el nÃºmero de outliers a estudiar. Si recuerda la grÃ¡fica de los scores LOF, habÃ­a un grupo de 3 registros con mayores valores de score, pero tambiÃ©n habÃ­a otro grupo de 8 registros con scores notablemente superiores al resto. Vamos por tanto a analizar de nuevo los resultados del mÃ©todo LOF aumentando el nÃºmero total de outliers a 11. Para ello, basta con que seleccione los 11 primeros registros del vector claves.lof.ordenados y calcular de nuevo los que son outliers LOF pero no 1-variantes. Le debe salir lo siguiente (el vector claves.outliers.IQR.en.alguna.columna no cambia):
```{r}
claves.outliers.lof.no.IQR <- setdiff(claves.lof.ordenados %>% head(11), claves.outliers.IQR.en.alguna.columna)
nombres.outliers.lof.no.IQR <- nombres_filas(datos.num, claves.outliers.lof.no.IQR)

claves.outliers.IQR.en.alguna.columna
claves.lof.ordenados %>% head(11)
claves.outliers.lof.no.IQR
nombres.outliers.lof.no.IQR
```

Mostramos el biplot de los outliers puros:
```{r}
biplot <- biplot_2_colores(datos.num, 
                           titulo = "Outliers LOF (excluÃ­dos los que son IQR)",
                           claves.a.mostrar = claves.outliers.lof.no.IQR)
biplot
```

Puede apreciar que el Hornet 4 Drive es un outlier muy similar a Valiant, aunque este Ãºltimo tenÃ­a un mayor score. El resto de outliers, exceptuando a Ferrari Dino presentan valores extremos en varias variables. Por ejemplo, Camaro Z28 y Duster 360 tienen un valor muy alto en hp y muy bajo en qsec por lo que posiblemente el efecto sumado de ambas variables haya contribuido a que tengan un alto score. Nos fijamos por tanto en el Ferrari Dino que no parece que tenga un valor extremo en ninguna variable (se sitÃºa en la zona central del biplot). Veamos los datos normalizados:
```{r}
datos.num.norm[claves.outliers.lof.no.IQR, ]
```

Ferrari Dino es un dato con valores interesantes porque tiene:

- SÃ³lo algo mÃ¡s de potencia que los otros coches (hp = 0.4129) El Maserati Bora tiene mucha mÃ¡s potencia (2.7)
- Una aceleraciÃ³n muy notable (qsec = -1.3143) El Maserati Bora tiene algo mÃ¡s, pero tampoco mucho (-1.8)
- Un consumo excelente para los coches de su categorÃ­a ya que consume lo mismo que la media (mpg= -0.0648)
- Menos centÃ­metros cÃºbicos (disp = -0.6916) El Maserati Bora tiene mÃ¡s que la media (0.567)

Los ingenieros del Ferrari Dino han sido capaces de conseguir un coche con una aceleraciÃ³n muy notable y un consumo excelente sin tener que recurrir a aumentar la potencia y los centÃ­metros cÃºbicos. Posiblemente tambiÃ©n habrÃ¡ contribuido a alcanzar estos buenos resultados el tener un peso bastante bajo (wt = -0.4570) algo inusual en coches de esas caracterÃ­sticas (El Maserati tiene 0.3)

# AnÃ¡lisis de resultados

Conjunto de datos

Hemos trabajado con la base de datos mtcars disponible en R. Hemos suprimido las variables que tenÃ­an pocos valores distintos (cyl, vs, am, gear, carb) y nos hemos quedado con las variables siguientes: mpg, disp, hp, drat, wt, qsec

Hemos aplicado la normalizaciÃ³n por z-score en aquellos mÃ©todos que asÃ­ lo han requerido.

Outliers en una variable

MÃ©todo IQR

No hemos encontrado outliers extremos (alejados de la media mÃ¡s de tres veces la distancia intercuartil) pero sÃ­ hay varios outliers no extremos (alejados de la media mÃ¡s de 1.5 veces la distancia intercuartil).

El Toyota Corolla se dispara (por arriba) en mpg pero no tanto en el resto de columnas. Parece por tanto un coche bastante equilibrado que consume muy poco.

Por otra parte, el Maserati Bora se dispara en hp (por arriba) y algo menos en qsec (por abajo): es un coche muy potente lo que le permite obtener una aceleraciÃ³n muy alta. AdemÃ¡s, tiene un consumo (mpg) bastante moderado para ser un coche de esas caracterÃ­sticas.

Es llamativo el caso del Merc 230 que tenga una aceleraciÃ³n tan baja, la menor de todos los coches. HabrÃ­a que determinar si se trata de un error en la toma de datos o simplemente los ingenieros diseÃ±aron el vehÃ­culo con esas caracterÃ­sticas.

TambiÃ©n es llamativo el bloque de coches Lincoln Continental, Chrysler Imperial, Cadillac Fletwood. Son coches muy pesados, con mucha cilindrada y que consumen mucho. Los tÃ­picos coches americanos.

Test de HipÃ³tesis

El test de Shapiro-Wilks rechaza la Normalidad en las variables disp y drat. En cuanto al resto de variables, el test no puede rechazar por lo que concluimos que dichas variables puede considerarse que siguen una Normal. Recuerde que no tenemos ninguna garantÃ­a estadÃ­stica ya que el test no ha rechazado y realmente lo Ãºnico que podemos afirmar es que los datos no contradicen la hipÃ³tesis de Normalidad.

Por otra parte, vemos que ninguno de los outliers IQR pueden considerarse realmente outliers con garantÃ­a estadÃ­stica. Los candidatos que han estado mÃ¡s cerca de considerarse outliers segÃºn el test de Grubbs son el Maserati Bora(columna hp, p-value 0.111) y Merc 230 (columna qsec, p-value = 0.08)

Outliers multivariantes

VisualizaciÃ³n con biplot

La suma de los porcentajes explicados es muy alta (19.1 + 69.8 = 88.9), por lo que la representaciÃ³n obtenida es una buena aproximaciÃ³n.

MÃ©todos estadÃ­sticos usando la distancia de Mahalanobis

La distribuciÃ³n conjunta de las variables mpg, hp, wt, qsec no es una Normal multivariante. Por lo tanto, no deberÃ­amos aplicar el mÃ©todo basado en la distancia de Mahalanobis. De todas formas, lo hemos lanzado para ver si detectamos algÃºn valor que, aunque no pueda considerarse un outlier con garantÃ­a estadÃ­stica, al menos proporcione alguna informaciÃ³n interesante.

El test de intersecciÃ³n no devuelve ningÃºn outlier, mientras que el test individual devuelve 4 outliers. Recordemos que sÃ³lo tenemos garantÃ­a estadÃ­stica de que sea un outlier el que tiene mayor valor de distancia de Mahalanobis. Dicho valor es el Maserati Bora. Dicho registro no fue etiquetado como outlier 1-variante en niguna variable por el test de Grubbs ya que tenÃ­a un valor muy alto en dos variables (hp y qsec), aunque no lo suficiente para que fuese un outlier. Sin embargo, al tener el mismo coche dos variables con valores muy altos, el test multivariante sÃ­ lo puede considerar como un outlier, ya que se suman las contribuciones de ambas variables. En cualquier caso, insistimos en que al no haber podido establecer que la distribuciÃ³n subyacente sea una Normal multivariante, no podemos concluir con garantÃ­a estadÃ­stica que, efectivamente, sea un outlier.

LOF

El grÃ¡fico de scores mostrÃ³ un grupo destacado de 3 registros: Lincoln Continental, Cadillac Fletwood y Valiant. Tanto Lincoln Continental como Cadillac Fletwood obtuvieron un score alto debido a que tienen valores extremos en una Ãºnica variable, por lo que no resultan de interÃ©s como outliers multivariantes.

De entre los tres anteriores, el registro con mayor score es el Valiant. Este vehÃ­culo no tiene un valor inusual en ninguna variable (por separado), pero se sitÃºa en una zona con poca densidad, bastante cerca a otra zona de alta densidad que consiste en vehÃ­culos con bastante potencia, cilindrada y peso, pero parece que al tener una relaciÃ³n con el eje trasero baja, les permite tener una aceleraciÃ³n media y un consumo algo menor de la media. Sin embargo el Valiant, es un vehÃ­culo con valores no demasiado distintos en disp, wt, drat, mpg, algo menor en hp pero un valor de qsec mucho mayor. AsÃ­ pues, es un coche de consumo normal pero con una aceleraciÃ³n muy baja en relaciÃ³n a otros vehÃ­culos con valores similares de drat, disp, wt pero con algo mÃ¡s de potencia (hp). AsÃ­ pues, a primera vista, el Valiant no es un vehÃ­culo cuya compra estarÃ­a recomendada, aunque obviamente habrÃ­a que tener en cuenta otros factores no incluidos en el conjunto de datos como por ejemplo el diseÃ±o, precio, fiabilidad, etc.

En una segunda ejecuciÃ³n de LOF se incluyÃ³ un grupo de otros 8 registros con altos scores. De entre los nuevos outliers detectados cabe destacar el Camaro Z28 y el Duster 360. Ambos tienen un valor muy alto en hp y muy bajo en qsec por lo que posiblemente el efecto sumado de ambas variables haya contribuido a que tengan un alto score.

Destaca tambiÃ©n el Hornet 4 Drive que es muy similar a Valiant (que ya hemos analizado anteriormente), aunque este Ãºltimo tenÃ­a un mayor score.

Otro outlier a destacar es Ferrari Dino. Es un dato con valores interesantes porque tiene:

SÃ³lo algo mÃ¡s de potencia que los otros coches (hp = 0.4129) El Maserati Bora tiene mucha mÃ¡s potencia (2.7)
Una aceleraciÃ³n muy notable (qsec = -1.3143) El Maserati Bora tiene algo mÃ¡s, pero tampoco mucho (-1.8) - Un consumo excelente para los coches de su categorÃ­a ya que consume lo mismo que la media (mpg-= 0.0648)
Menos centÃ­metros cÃºbicos (disp = -0.6916) El Maserati Bora tiene mÃ¡s que la media (0.567)
Los ingenieros del Ferrari Dino han sido capaces de conseguir un coche con una aceleraciÃ³n muy notable y un consumo excelente sin tener que recurrir a aumentar la potencia y los centÃ­metros cÃºbicos. Posiblemente tambiÃ©n habrÃ¡ contribuido a alcanzar estos buenos resultados el tener un peso bastante bajo (wt = -0.4570) algo inusual en coches de esas caracterÃ­sticas (El Maserati tiene 0.3)

En resumen, el mÃ©todo LOF ha detectado outliers interesantes como por ejemplo el Valiant que es un coche con unos resultados inusuales por malos, asÃ­ como el Ferrari Dino que destaca justo por lo contrario.

MÃ©todos basados en clustering

Con el mÃ©todo de k-means (ordenando outliers segÃºn la distancia euclÃ­dea a los centroides) hemos detectado como outliers vehÃ­culos como Ford Pantera L y Honda Civic al tener valores bastante extremos (sin llegar a ser muy extremos) en varias variables, por lo que la suma de los efectos de dichas variables han podido determinar que tengan scores altos. Por ejemplo, Honda Civic tiene valores algo extremos en todas las variables salvo en qsec.

Este mÃ©todo tambiÃ©n etiquetÃ³ a Ferrari Dino como outlier (registro que ya hemos analizado anteriormente)



















